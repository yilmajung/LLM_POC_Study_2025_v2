{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae0d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239e424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-sectional GSS data processing\n",
    "# Load GSS overall data\n",
    "gss = pd.read_excel(\"/Users/wooyongjung/WJ_Projects/LLM_POC_Study_2025_v2/data/GSS/GSS.xlsx\")\n",
    "\n",
    "# Create yearid by combining year and id\n",
    "gss['yearid'] = gss['year'].astype(str) + \"_\" + gss['id_'].astype(str)\n",
    "\n",
    "# Expect one row per respondent per wave, columns listed below:\n",
    "BINARY_ITEMS = [\"abdefect\",\"abnomore\",\"abany\",\"abhlth\",\"abpoor\",\"abrape\",\"absingle\"]\n",
    "\n",
    "# Map raw responses â†’ 1/0 (edit as needed to match your GSS coding)\n",
    "YES_VALUES = {\"yes\",\"Yes\",\"YES\",1, \"1\", True}\n",
    "NO_VALUES  = {\"no\",\"No\",\"NO\",2, \"2\", False}\n",
    "\n",
    "def to_binary(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    return 1 if s in YES_VALUES else (0 if s in NO_VALUES else np.nan)\n",
    "\n",
    "def prepare_binary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for col in BINARY_ITEMS:\n",
    "        if col in out.columns:\n",
    "            out[col+\"_bin\"] = out[col].apply(to_binary).astype(\"float\")\n",
    "    return out\n",
    "\n",
    "# Following Rossi Scale\n",
    "\n",
    "def collapse_abortion_attitudes(df: pd.DataFrame, id_col=\"yearid\", binary_items=None):\n",
    "    \"\"\"\n",
    "    Collapse 7 binary GSS abortion items (Rossi Scale) into a single 4-level categorical variable.\n",
    "    \n",
    "    - Input: df with columns abdefect ... absingle coded 0/1 (NaN allowed).\n",
    "    - Output: df with new columns:\n",
    "        'n_yes'  (count of yes across 7 items)\n",
    "        'att4'   (collapsed attitude category)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Count number of \"yes\" responses (ignore NaN)\n",
    "    out[\"n_yes\"] = out[binary_items].sum(axis=1, skipna=True)\n",
    "\n",
    "    # Count number of non-missing responses\n",
    "    out[\"n_nonmiss\"] = out[binary_items].notna().sum(axis=1)\n",
    "\n",
    "    # Map counts into categories\n",
    "    def map_to_cat(n_yes):\n",
    "        if pd.isna(n_yes):\n",
    "            return np.nan\n",
    "        n_yes = int(n_yes)\n",
    "        if n_yes <= 1:\n",
    "            return \"strong_anti\"\n",
    "        elif n_yes <= 3:\n",
    "            return \"anti\"\n",
    "        elif n_yes <= 6:\n",
    "            return \"pro\"\n",
    "        else:  # n_yes == 7\n",
    "            return \"strong_pro\"\n",
    "\n",
    "    out[\"abortion_att4\"] = out[\"n_yes\"].apply(map_to_cat)\n",
    "\n",
    "    return out[[id_col, \"year\", \"n_yes\", \"n_nonmiss\", \"abortion_att4\"]]\n",
    "\n",
    "\n",
    "# Construct binary items and collapse into attitude categories\n",
    "gss_abt_bin = prepare_binary(gss)\n",
    "gss_abt_bin = collapse_abortion_attitudes(gss_abt_bin, binary_items=[col+\"_bin\" for col in BINARY_ITEMS])\n",
    "\n",
    "# Required columns (demographic) from gss_2020\n",
    "required_columns = [\"cohort\", \"degree\", \"race\", \"sex\", \"polviews\", \"natenvir\", \"trust\", \"homosex\", \"wtssps\"] # exclude relig, marital (no data for GSS 2024)\n",
    "\n",
    "# Merge demographic columns\n",
    "gss_abt_cs = gss_abt_bin.merge(gss[required_columns + [\"yearid\"]], on=\"yearid\", how=\"left\")\n",
    "\n",
    "# Create generation column based on birth year\n",
    "def determine_generation(year):\n",
    "    if pd.isna(year):\n",
    "        return None\n",
    "    elif year.startswith('.'):\n",
    "        return None\n",
    "    year = int(year)\n",
    "    if year <= 1945:\n",
    "        return \"Silent Generation\"\n",
    "    elif 1946 <= year <= 1964:\n",
    "        return \"Baby Boomer\"\n",
    "    elif 1965 <= year <= 1980:\n",
    "        return \"Generation X\"\n",
    "    elif 1981 <= year <= 1996:\n",
    "        return \"Millennial\"\n",
    "    elif 1997 <= year <= 2012:\n",
    "        return \"Generation Z\"\n",
    "    else:\n",
    "        return \"Generation Alpha\"\n",
    "\n",
    "gss_abt_cs['generation'] = gss_abt_cs['cohort'].apply(determine_generation)\n",
    "\n",
    "\n",
    "# Create education level column\n",
    "def categorize_education(edu):\n",
    "    if pd.isna(edu):\n",
    "        return None\n",
    "    elif edu in [\"Less than high school\", \"High school\"]:\n",
    "        return \"Less or equal to high school\"\n",
    "    elif edu in [\"Associate/junior college\", \"Bachelor's\"]:\n",
    "        return \"Associate or Bachelor's Degree\"\n",
    "    elif edu in [\"Graduate\"]:\n",
    "        return \"Graduate Degree\"\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "gss_abt_cs['edu_level'] = gss_abt_cs['degree'].apply(categorize_education)\n",
    "\n",
    "# Create gender column\n",
    "def categorize_gender(gender):\n",
    "    if pd.isna(gender):\n",
    "        return None\n",
    "    elif gender == \"MALE\":\n",
    "        return \"Male\"\n",
    "    elif gender == \"FEMALE\":\n",
    "        return \"Female\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "gss_abt_cs['gender'] = gss_abt_cs['sex'].apply(categorize_gender)\n",
    "\n",
    "# Create race column\n",
    "def categorize_race(race):\n",
    "    if pd.isna(race):\n",
    "        return None\n",
    "    elif race == \"White\":\n",
    "        return \"White\"\n",
    "    elif race == \"Black\":\n",
    "        return \"Black\"\n",
    "    elif race == \"Other\":\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_abt_cs['race'] = gss_abt_cs['race'].apply(categorize_race)\n",
    "\n",
    "# # Create religion column\n",
    "# def categorize_religion(relig):\n",
    "#     if relig in [\"Protestant\", \"Catholic\", \"None\"]:\n",
    "#         return relig\n",
    "#     elif pd.isna(relig):\n",
    "#         return None\n",
    "#     else:\n",
    "#         return \"Other\"\n",
    "\n",
    "# gss_abt_cs['religion'] = gss_abt_cs['relig'].apply(categorize_religion)\n",
    "\n",
    "# Create political views column\n",
    "def categorize_political_views(pv):\n",
    "    if pd.isna(pv):\n",
    "        return None\n",
    "    elif pv in [\"Extremely liberal\", \"Liberal\", \"Slightly liberal\"]:\n",
    "        return \"Liberal\"\n",
    "    elif pv in [\"Moderate, middle of the road\"]:\n",
    "        return \"Moderate\"\n",
    "    elif pv in [\"Slightly conservative\", \"Conservative\", \"Extremely conservative\"]:\n",
    "        return \"Conservative\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_abt_cs['political_views'] = gss_abt_cs['polviews'].apply(categorize_political_views)\n",
    "\n",
    "\n",
    "# Create environmental attitude column\n",
    "def categorize_environmental_attitude(env):\n",
    "    if pd.isna(env):\n",
    "        return None\n",
    "    if env in [\"TOO LITTLE\"]:\n",
    "        return \"too_little\"\n",
    "    elif env in [\"ABOUT RIGHT\"]:\n",
    "        return \"about_right\"\n",
    "    elif env in [\"TOO MUCH\"]:\n",
    "        return \"too_much\"\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "gss_abt_cs['natenvir'] = gss_abt_cs['natenvir'].apply(categorize_environmental_attitude)\n",
    "\n",
    "# Create trust other people column\n",
    "def categorize_trust(trust):\n",
    "    if pd.isna(trust):\n",
    "        return None\n",
    "    if trust in [\"Most people can be trusted\"]:\n",
    "        return \"trust\"\n",
    "    elif trust in [\"Can't be too careful\"]:\n",
    "        return \"distrust\"\n",
    "    elif trust in [\"Depends\"]:\n",
    "        return \"depends\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_abt_cs['trust'] = gss_abt_cs['trust'].apply(categorize_trust)\n",
    "\n",
    "# Create homosexuality attitude column\n",
    "def categorize_homosexuality_attitude(homosex):\n",
    "    if pd.isna(homosex):\n",
    "        return None\n",
    "    if homosex in [\"ALWAYS WRONG\"]:\n",
    "        return \"always_wrong\"\n",
    "    elif homosex in [\"ALMST ALWAYS WRG\"]:\n",
    "        return \"almost_always_wrong\"\n",
    "    elif homosex in [\"SOMETIMES WRONG\"]:\n",
    "        return \"sometimes_wrong\"\n",
    "    elif homosex in [\"NOT WRONG AT ALL\"]:\n",
    "        return \"not_wrong_at_all\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_abt_cs['homosex'] = gss_abt_cs['homosex'].apply(categorize_homosexuality_attitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae526428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/z9h695vx2w3gbjh7z32h4j9m0000gn/T/ipykernel_44683/2658170453.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gss_2010['yearid'] = '2010_' + gss_2010.index.astype(str)\n",
      "/var/folders/y6/z9h695vx2w3gbjh7z32h4j9m0000gn/T/ipykernel_44683/2658170453.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gss_2008['yearid'] = '2008_' + gss_2008.index.astype(str)\n",
      "/var/folders/y6/z9h695vx2w3gbjh7z32h4j9m0000gn/T/ipykernel_44683/2658170453.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gss_2006['yearid'] = '2006_' + gss_2006.index.astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Panel data processing\n",
    "# Load panel data (three-wave rollings)\n",
    "gss_2010 = pd.read_stata('data/GSS/GSS_panel2010w123_R6.dta', convert_categoricals=False)\n",
    "gss_2008 = pd.read_stata('data/GSS/GSS_panel2008w123_r6 .dta', convert_categoricals=False)\n",
    "gss_2006 = pd.read_stata('data/GSS/GSS_panel2006w123_r6a.dta', convert_categoricals=False)\n",
    "\n",
    "# Create a \"yearid\" column by combining \"2010\" and index\n",
    "gss_2010['yearid'] = '2010_' + gss_2010.index.astype(str)\n",
    "gss_2008['yearid'] = '2008_' + gss_2008.index.astype(str)\n",
    "gss_2006['yearid'] = '2006_' + gss_2006.index.astype(str)\n",
    "\n",
    "# Select relevant columns for analysis\n",
    "required_columns = [\"cohort_1\", \"degree_1\", \"degree_2\", \"degree_3\", \"race_1\", \"sex_1\", \"polviews_1\", \"polviews_2\", \"polviews_3\"]\n",
    "abortion_items = [\"abdefect_1\", \"abnomore_1\", \"abany_1\", \"abhlth_1\", \"abpoor_1\", \"abrape_1\", \"absingle_1\",\n",
    "                 \"abdefect_2\", \"abnomore_2\", \"abany_2\", \"abhlth_2\", \"abpoor_2\", \"abrape_2\", \"absingle_2\",\n",
    "                 \"abdefect_3\", \"abnomore_3\", \"abany_3\", \"abhlth_3\", \"abpoor_3\", \"abrape_3\", \"absingle_3\"]\n",
    "trust_columns = [\"trust_1\", \"trust_2\", \"trust_3\"]\n",
    "natenvir_columns = [\"natenvir_1\", \"natenvir_2\", \"natenvir_3\"]\n",
    "homosex_columns = [\"homosex_1\", \"homosex_2\", \"homosex_3\"]\n",
    "\n",
    "gss_2010 = gss_2010[['yearid'] +required_columns + abortion_items + trust_columns + natenvir_columns + homosex_columns]\n",
    "gss_2008 = gss_2008[['yearid'] + required_columns + abortion_items + trust_columns + natenvir_columns + homosex_columns]\n",
    "gss_2006 = gss_2006[['yearid'] + required_columns + abortion_items + trust_columns + natenvir_columns + homosex_columns]\n",
    "\n",
    "# Rename columns for consistency\n",
    "gss_2010.rename(columns={\"cohort_1\": \"cohort\", \"race_1\": \"race\", \"sex_1\": \"gender\"}, inplace=True)\n",
    "gss_2008.rename(columns={\"cohort_1\": \"cohort\", \"race_1\": \"race\", \"sex_1\": \"gender\"}, inplace=True)\n",
    "gss_2006.rename(columns={\"cohort_1\": \"cohort\", \"race_1\": \"race\", \"sex_1\": \"gender\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f0bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dfs to long format: _1 â†’ 2010, _2 â†’ 2012, _3 â†’ 2014\n",
    "gss_2010_long = pd.wide_to_long(gss_2010, stubnames=['abdefect', 'abnomore', 'abany', 'abhlth', 'abpoor', 'abrape', 'absingle','degree', 'polviews', 'natenvir', 'trust', 'homosex'],\n",
    "                                i='yearid', j='wave', suffix='_\\\\d+').reset_index()\n",
    "gss_2008_long = pd.wide_to_long(gss_2008, stubnames=['abdefect', 'abnomore', 'abany', 'abhlth', 'abpoor', 'abrape', 'absingle','degree', 'polviews', 'natenvir', 'trust', 'homosex'], \n",
    "                                i='yearid', j='wave', suffix='_\\\\d+').reset_index()\n",
    "gss_2006_long = pd.wide_to_long(gss_2006, stubnames=['abdefect', 'abnomore', 'abany', 'abhlth', 'abpoor', 'abrape', 'absingle','degree', 'polviews', 'natenvir', 'trust', 'homosex'],\n",
    "                                i='yearid', j='wave', suffix='_\\\\d+').reset_index()\n",
    "\n",
    "# Create year column based on wave\n",
    "def map_wave_to_year(wave, base_year):\n",
    "    if wave == \"_1\":\n",
    "        return base_year\n",
    "    elif wave == \"_2\":\n",
    "        return base_year + 2\n",
    "    elif wave == \"_3\":\n",
    "        return base_year + 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_2010_long['year'] = gss_2010_long['wave'].apply(lambda w: map_wave_to_year(w, 2010))\n",
    "gss_2008_long['year'] = gss_2008_long['wave'].apply(lambda w: map_wave_to_year(w, 2008))\n",
    "gss_2006_long['year'] = gss_2006_long['wave'].apply(lambda w: map_wave_to_year(w, 2006))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8410f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18201, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine long dfs\n",
    "gss_abt_panel = pd.concat([gss_2010_long, gss_2008_long, gss_2006_long], ignore_index=True)\n",
    "gss_abt_panel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b46d3d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y6/z9h695vx2w3gbjh7z32h4j9m0000gn/T/ipykernel_44683/1958530074.py:2: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  gss_2020 = pd.read_stata('data/GSS/GSS_panel2020_r1a.dta', convert_categoricals=False)\n"
     ]
    }
   ],
   "source": [
    "# Load 2020 panel data\n",
    "gss_2020 = pd.read_stata('data/GSS/GSS_panel2020_r1a.dta', convert_categoricals=False)\n",
    "\n",
    "# Expect one row per respondent per wave, columns listed below:\n",
    "ITEMS_2016 = [\"abdefect_1a\",\"abnomore_1a\",\"abany_1a\",\"abhlth_1a\",\"abpoor_1a\",\"abrape_1a\",\"absingle_1a\",\"natenvir_1a\",\"trust_1a\",\"homosex_1a\"]\n",
    "ITEMS_2018 = [\"abdefect_1b\",\"abnomore_1b\",\"abany_1b\",\"abhlth_1b\",\"abpoor_1b\",\"abrape_1b\",\"absingle_1b\",\"natenvir_1b\",\"trust_1b\",\"homosex_1b\"]\n",
    "ITEMS_2020 = [\"abdefect_2\",\"abnomore_2\",\"abany_2\",\"abhlth_2\",\"abpoor_2\",\"abrape_2\",\"absingle_2\",\"natenvir_2\",\"trust_2\",\"homosex_2\"]\n",
    "\n",
    "# Split gss_2020 into 2016->2020 and 2018->2020 waves\n",
    "gss_2016 = gss_2020[gss_2020['samptype'] == 2016]\n",
    "gss_2018 = gss_2020[gss_2020['samptype'] == 2018]\n",
    "\n",
    "# Required columns (demographic) from gss_2020\n",
    "required_columns_2016 = [\"cohort_1a\", \"degree_1a\", \"race_1a\", \"gender1_1a\", \"polviews_1a\", \"degree_2\", \"polviews_2\"]\n",
    "required_columns_2018 = [\"cohort_1b\", \"degree_1b\", \"race_1b\", \"gender1_1b\", \"polviews_1b\", \"degree_2\", \"polviews_2\"]\n",
    "\n",
    "gss_2016 = gss_2016[['yearid'] + required_columns_2016 + ITEMS_2016 + ITEMS_2020]\n",
    "gss_2018 = gss_2018[['yearid'] + required_columns_2018 + ITEMS_2018 + ITEMS_2020]\n",
    "\n",
    "# Rename 1a and 1b\n",
    "gss_2016.rename(columns={\"gender1_1a\": \"gender\", \"race_1a\": \"race\", \"degree_1a\": \"degree_1\", \"cohort_1a\": \"cohort\", \"polviews_1a\": \"polviews_1\", \"natenvir_1a\": \"natenvir_1\", \"trust_1a\": \"trust_1\",\"homosex_1a\": \"homosex_1\",\n",
    "\"abdefect_1a\": \"abdefect_1\", \"abnomore_1a\": \"abnomore_1\", \"abany_1a\": \"abany_1\", \"abhlth_1a\": \"abhlth_1\", \"abpoor_1a\": \"abpoor_1\", \"abrape_1a\": \"abrape_1\", \"absingle_1a\": \"absingle_1\"}, inplace=True)\n",
    "gss_2018.rename(columns={\"gender1_1b\": \"gender\", \"race_1b\": \"race\", \"degree_1b\": \"degree_1\", \"cohort_1b\": \"cohort1\", \"polviews_1b\": \"polviews_1\", \"natenvir_1b\": \"natenvir_1\", \"trust_1b\": \"trust_1\",\"homosex_1b\": \"homosex_1\",\n",
    "\"abdefect_1b\": \"abdefect_1\", \"abnomore_1b\": \"abnomore_1\", \"abany_1b\": \"abany_1\", \"abhlth_1b\": \"abhlth_1\", \"abpoor_1b\": \"abpoor_1\", \"abrape_1b\": \"abrape_1\", \"absingle_1b\": \"absingle_1\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c884b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to long format for analysis\n",
    "gss_2016_long = pd.wide_to_long(gss_2016, stubnames=['abdefect', 'abnomore', 'abany', 'abhlth', 'abpoor', 'abrape', 'absingle','degree', 'polviews', 'natenvir', 'trust', 'homosex'], \n",
    "                                i='yearid', j='wave', suffix='_\\\\d+').reset_index()\n",
    "gss_2018_long = pd.wide_to_long(gss_2018, stubnames=['abdefect', 'abnomore', 'abany', 'abhlth', 'abpoor', 'abrape', 'absingle','degree', 'polviews', 'natenvir', 'trust', 'homosex'],\n",
    "                                i='yearid', j='wave', suffix='_\\\\d+').reset_index()\n",
    "\n",
    "# Create year column based on wave\n",
    "def map_wave_to_year(wave, last_year, gap):\n",
    "    if wave == \"_1\":\n",
    "        return last_year - gap\n",
    "    elif wave == \"_2\":\n",
    "        return last_year\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_2016_long['year'] = gss_2016_long['wave'].apply(lambda w: map_wave_to_year(w, 2020, 4))\n",
    "gss_2018_long['year'] = gss_2018_long['wave'].apply(lambda w: map_wave_to_year(w, 2020, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1950e319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28631, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine long dfs\n",
    "gss_abt_panel = pd.concat([gss_2016_long, gss_2018_long, gss_abt_panel], ignore_index=True)\n",
    "gss_abt_panel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9386bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect one row per respondent per wave, columns listed below:\n",
    "BINARY_ITEMS = [\"abdefect\",\"abnomore\",\"abany\",\"abhlth\",\"abpoor\",\"abrape\",\"absingle\"]\n",
    "\n",
    "# Map raw responses â†’ 1/0 (edit as needed to match your GSS coding)\n",
    "YES_VALUES = {\"yes\",\"Yes\",\"YES\",1, \"1\", True}\n",
    "NO_VALUES  = {\"no\",\"No\",\"NO\",2, \"2\", False}\n",
    "\n",
    "def to_binary(s):\n",
    "    if pd.isna(s): return np.nan\n",
    "    return 1 if s in YES_VALUES else (0 if s in NO_VALUES else np.nan)\n",
    "\n",
    "def prepare_binary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for col in BINARY_ITEMS:\n",
    "        if col in out.columns:\n",
    "            out[col+\"_bin\"] = out[col].apply(to_binary).astype(\"float\")\n",
    "    return out\n",
    "\n",
    "# Following Rossi Scale\n",
    "\n",
    "def collapse_abortion_attitudes(df: pd.DataFrame, id_col=\"yearid\", binary_items=None):\n",
    "    \"\"\"\n",
    "    Collapse 7 binary GSS abortion items (Rossi Scale) into a single 4-level categorical variable.\n",
    "    \n",
    "    - Input: df with columns abdefect ... absingle coded 0/1 (NaN allowed).\n",
    "    - Output: df with new columns:\n",
    "        'n_yes'  (count of yes across 7 items)\n",
    "        'att4'   (collapsed attitude category)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Count number of \"yes\" responses (ignore NaN)\n",
    "    out[\"n_yes\"] = out[binary_items].sum(axis=1, skipna=True)\n",
    "\n",
    "    # Count number of non-missing responses (for reference)\n",
    "    out[\"n_nonmiss\"] = out[binary_items].notna().sum(axis=1)\n",
    "\n",
    "    # Map counts into categories\n",
    "    def map_to_cat(n_yes):\n",
    "        if pd.isna(n_yes):\n",
    "            return np.nan\n",
    "        n_yes = int(n_yes)\n",
    "        if n_yes <= 1:\n",
    "            return \"strong_anti\"\n",
    "        elif n_yes <= 3:\n",
    "            return \"anti\"\n",
    "        elif n_yes <= 6:\n",
    "            return \"pro\"\n",
    "        else:  # n_yes == 7\n",
    "            return \"strong_pro\"\n",
    "\n",
    "    out[\"abortion_att4\"] = out[\"n_yes\"].apply(map_to_cat)\n",
    "\n",
    "    return out[[id_col, \"year\", \"n_yes\", \"n_nonmiss\", \"abortion_att4\", \"cohort\", \"degree\", \"race\", \"gender\", \"polviews\", \"natenvir\", \"trust\", \"homosex\"]]\n",
    "\n",
    "\n",
    "# Construct binary items and collapse into attitude categories\n",
    "gss_abt_panel = prepare_binary(gss_abt_panel)\n",
    "gss_abt_panel = collapse_abortion_attitudes(gss_abt_panel, binary_items=[col+\"_bin\" for col in BINARY_ITEMS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39765e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generation column based on birth year\n",
    "def determine_generation(year):\n",
    "    if pd.isna(year):\n",
    "        return None\n",
    "    year = int(year)\n",
    "    if year <= 1945:\n",
    "        return \"Silent Generation\"\n",
    "    elif 1946 <= year <= 1964:\n",
    "        return \"Baby Boomer\"\n",
    "    elif 1965 <= year <= 1980:\n",
    "        return \"Generation X\"\n",
    "    elif 1981 <= year <= 1996:\n",
    "        return \"Millennial\"\n",
    "    elif 1997 <= year <= 2012:\n",
    "        return \"Generation Z\"\n",
    "    else:\n",
    "        return \"Generation Alpha\"\n",
    "        \n",
    "gss_abt_panel['generation'] = gss_abt_panel['cohort'].apply(determine_generation)\n",
    "\n",
    "# Create education level column\n",
    "def categorize_education(edu):\n",
    "    if pd.isna(edu):\n",
    "        return None\n",
    "    elif edu <= 1:\n",
    "        return \"Less or equal to high school\"\n",
    "    elif edu == 2 or edu == 3:\n",
    "        return \"Associate or Bachelor's Degree\"\n",
    "    elif edu == 4:\n",
    "        return \"Graduate Degree\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_abt_panel['edu_level'] = gss_abt_panel['degree'].apply(categorize_education)\n",
    "\n",
    "# Create gender column\n",
    "def categorize_gender(gender):\n",
    "    if pd.isna(gender):\n",
    "        return None\n",
    "    elif gender == 1:\n",
    "        return \"Male\"\n",
    "    elif gender == 2:\n",
    "        return \"Female\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "gss_abt_panel['gender'] = gss_abt_panel['gender'].apply(categorize_gender)\n",
    "\n",
    "# Create race column\n",
    "def categorize_race(race):\n",
    "    if pd.isna(race):\n",
    "        return None\n",
    "    elif race == 1:\n",
    "        return \"White\"\n",
    "    elif race == 2:\n",
    "        return \"Black\"\n",
    "    elif race == 3:\n",
    "        return \"Other\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "gss_abt_panel['race'] = gss_abt_panel['race'].apply(categorize_race)\n",
    "\n",
    "# Create political views column\n",
    "def categorize_political_views(pv):\n",
    "    if pd.isna(pv):\n",
    "        return None\n",
    "    elif pv == 1 or pv == 2 or pv == 3:\n",
    "        return \"Liberal\"\n",
    "    elif pv == 4:\n",
    "        return \"Moderate\"\n",
    "    elif pv == 5 or pv == 6 or pv == 7:\n",
    "        return \"Conservative\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_abt_panel['political_views'] = gss_abt_panel['polviews'].apply(categorize_political_views)\n",
    "\n",
    "# Create environmental attitude column\n",
    "def categorize_environmental_attitude(env):\n",
    "    if pd.isna(env):\n",
    "        return None\n",
    "    if env == 1:\n",
    "        return \"too_little\"\n",
    "    elif env == 2:\n",
    "        return \"about_right\"\n",
    "    elif env == 3:\n",
    "        return \"too_much\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_abt_panel['natenvir'] = gss_abt_panel['natenvir'].apply(categorize_environmental_attitude)\n",
    "\n",
    "# Create trust other people column\n",
    "def categorize_trust(trust):\n",
    "    if pd.isna(trust):\n",
    "        return None\n",
    "    elif trust == 1:\n",
    "        return \"trust\"\n",
    "    elif trust == 2:\n",
    "        return \"distrust\"\n",
    "    elif trust == 3:\n",
    "        return \"depends\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "gss_abt_panel['trust'] = gss_abt_panel['trust'].apply(categorize_trust)\n",
    "\n",
    "# Create homosexuality attitude column\n",
    "def categorize_homosexuality_attitude(homosex):\n",
    "    if pd.isna(homosex):\n",
    "        return None\n",
    "    if homosex == 1:\n",
    "        return \"always_wrong\"\n",
    "    elif homosex == 2:\n",
    "        return \"almost_always_wrong\"\n",
    "    elif homosex == 3:\n",
    "        return \"sometimes_wrong\"\n",
    "    elif homosex == 4:\n",
    "        return \"not_wrong_at_all\"\n",
    "    else:\n",
    "        return None\n",
    "gss_abt_panel['homosex'] = gss_abt_panel['homosex'].apply(categorize_homosexuality_attitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56158f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop rows with n_nonmiss < 7\n",
    "gss_abt_cs_abortion = gss_abt_cs[gss_abt_cs['n_nonmiss'] == 7]\n",
    "gss_abt_panel_abortion = gss_abt_panel[gss_abt_panel['n_nonmiss'] == 7]\n",
    "\n",
    "# Drop rows with missing homosex\n",
    "gss_abt_cs_homosex = gss_abt_cs.dropna(subset=['homosex'])\n",
    "gss_abt_panel_homosex = gss_abt_panel.dropna(subset=['homosex'])\n",
    "\n",
    "# Finally add needed columns and save\n",
    "df_cs_abo = gss_abt_cs_abortion[['yearid', 'year', 'generation', 'edu_level', 'gender', 'race', 'political_views', 'natenvir', 'trust', 'abortion_att4', 'homosex', 'wtssps']].copy()\n",
    "df_pl_abo = gss_abt_panel_abortion[['yearid', 'year', 'generation', 'edu_level', 'gender', 'race', 'political_views', 'natenvir', 'trust', 'abortion_att4', 'homosex']].copy()\n",
    "\n",
    "df_cs_homosex = gss_abt_cs_homosex[['yearid', 'year', 'generation', 'edu_level', 'gender', 'race', 'political_views', 'natenvir', 'trust', 'abortion_att4', 'homosex', 'wtssps']].copy()\n",
    "df_pl_homosex = gss_abt_panel_homosex[['yearid', 'year', 'generation', 'edu_level', 'gender', 'race', 'political_views', 'natenvir', 'trust', 'abortion_att4', 'homosex']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8644174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13351, 12), (12610, 11), (17576, 12), (13647, 11))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cs_abo.shape, df_pl_abo.shape, df_cs_homosex.shape, df_pl_homosex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbc5298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert year 2021 to 2020\n",
    "df_cs_abo.loc[df_cs_abo['year'] == 2021, 'year'] = 2020\n",
    "df_cs_homosex.loc[df_cs_homosex['year'] == 2021, 'year'] = 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c3e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0215d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial logistic AR analysis\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# ALR helpers (reference = last category)\n",
    "def alr(p, eps=1e-8):\n",
    "    p = np.clip(np.asarray(p, dtype=float), eps, 1.0)\n",
    "    p = p / p.sum()\n",
    "    ref = p[-1]\n",
    "    return np.log(p[:-1] / ref)\n",
    "\n",
    "def inv_alr(u):\n",
    "    u = np.asarray(u, dtype=float)\n",
    "    z = np.r_[np.exp(u), 1.0]\n",
    "    return z / z.sum()\n",
    "\n",
    "# Fit AR(1) per subgroup with ridge, forecast next year\n",
    "def cs_ar_fit_and_forecast(p_cs, l2=1.0, min_years=3):\n",
    "    \"\"\"\n",
    "    p_cs: dict {((group_tuple), year) -> p (K,)}\n",
    "    Returns dict {((group_tuple), year_next) -> p_hat_AR}\n",
    "    \"\"\"\n",
    "    by_group = {}\n",
    "    for (g,y), p in p_cs.items():\n",
    "        by_group.setdefault(g, []).append(int(y))\n",
    "\n",
    "    forecasts = {}\n",
    "    for g, ys in by_group.items():\n",
    "        ys = sorted(ys)\n",
    "        if len(ys) < min_years:\n",
    "            # too few years: fallback to persistence p_{t+1} = p_t\n",
    "            for i in range(1, len(ys)):\n",
    "                y_prev, y = ys[i-1], ys[i]\n",
    "                forecasts[(g, y)] = p_cs[(g, y_prev)]\n",
    "            continue\n",
    "\n",
    "        # Build (X=u_{t-1}, Y=u_t) in ALR space\n",
    "        X_list, Y_list, targets = [], [], []\n",
    "        for i in range(1, len(ys)):\n",
    "            y_prev, y = ys[i-1], ys[i]\n",
    "            u_prev = alr(p_cs[(g, y_prev)])\n",
    "            u_curr = alr(p_cs[(g, y)])\n",
    "            X_list.append(np.r_[1.0, u_prev])   # intercept + u_{t-1}\n",
    "            Y_list.append(u_curr)\n",
    "            targets.append((g, y))              # weâ€™ll also produce in-sample one-step fits if needed\n",
    "        X = np.vstack(X_list)        # [T-1, 1+(K-1)]\n",
    "        Y = np.vstack(Y_list)        # [T-1, (K-1)]\n",
    "\n",
    "        # Fit separate Ridge for each ALR dimension (small T)\n",
    "        coefs = []\n",
    "        for k in range(Y.shape[1]):\n",
    "            reg = Ridge(alpha=l2, fit_intercept=False)  # intercept already in X\n",
    "            reg.fit(X, Y[:,k])\n",
    "            coefs.append(reg.coef_)\n",
    "        B = np.vstack(coefs)  # shape [(K-1) x (K)]\n",
    "\n",
    "        # Forecast each step from previous observed (one-step ahead backtest)\n",
    "        for i in range(1, len(ys)):\n",
    "            y_prev, y = ys[i-1], ys[i]\n",
    "            u_prev = alr(p_cs[(g, y_prev)])\n",
    "            x = np.r_[1.0, u_prev]             # [K]\n",
    "            u_hat = B @ x                      # [K-1]\n",
    "            p_hat = inv_alr(u_hat)\n",
    "            forecasts[(g, y)] = p_hat\n",
    "\n",
    "    return forecasts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd8f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2032789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b374a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b2849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b82af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453fa100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
