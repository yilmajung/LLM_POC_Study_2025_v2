{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "05409eed",
      "metadata": {
        "id": "05409eed"
      },
      "outputs": [],
      "source": [
        "# Inference with small classification head (Fix A)\n",
        "!pip -q install peft transformers accelerate sentencepiece\n",
        "\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uflq1K3B9Oe8",
        "outputId": "c343391f-c06d-452c-ccf6-b1a56eb7d636"
      },
      "id": "uflq1K3B9Oe8",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "30ebe9d8",
      "metadata": {
        "id": "30ebe9d8"
      },
      "outputs": [],
      "source": [
        "# Config (EDIT ME)\n",
        "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"   # or \"mistralai/Mistral-7B-v0.3\"\n",
        "INF_DIR         = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/tllm_fixA_head\"\n",
        "LORA_DIR        = os.path.join(INF_DIR, \"lora\")\n",
        "HEAD_PATH       = os.path.join(INF_DIR, \"dist_head.pt\") # saved small head weights\n",
        "GSS_PATH        = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_panel_2016_long.csv\"\n",
        "\n",
        "# Output CSVs\n",
        "PRED_OUT   = os.path.join(INF_DIR, \"gss_tllm_fixA_proj_2020_from_2016.csv\")\n",
        "EVAL_OUT   = os.path.join(INF_DIR, \"gss_tllm_fixA_eval_2020_vs_empirical.csv\")\n",
        "DELTA_OUT  = os.path.join(INF_DIR, \"gss_tllm_fixA_pred_deltas_2020_minus_2016.csv\")\n",
        "\n",
        "# If you need HF auth in Colab:\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "\n",
        "# Canonical bins (5-way; UNSURE excluded at inference)\n",
        "CANON5 = [\"strong_anti\", \"anti\", \"neutral\", \"pro\", \"strong_pro\"]\n",
        "IDX = {k:i for i,k in enumerate(CANON5)}\n",
        "\n",
        "# Map your GSS att5 -> 5 canonical bins (EDIT if your labels differ)\n",
        "GSS_ATT5_TO_5CANON = {\n",
        "    \"strong_anti\": \"strong_anti\",\n",
        "    \"anti\": \"anti\",\n",
        "    \"neutral\": \"neutral\",\n",
        "    \"pro\": \"pro\",\n",
        "    \"strong_pro\": \"strong_pro\",\n",
        "    # Any other responses (DK/NA/Refused/etc.) will be dropped for the 5-bin distribution\n",
        "}\n",
        "\n",
        "def fivebin_empirical(series):\n",
        "    \"\"\"Return 5-bin prob vector in CANON5 order; None if no mappable answers.\"\"\"\n",
        "    mapped = series.map(GSS_ATT5_TO_5CANON).dropna()\n",
        "    if mapped.empty:\n",
        "        return None\n",
        "    cnt = mapped.value_counts()\n",
        "    vec = np.array([cnt.get(k, 0.0) for k in CANON5], dtype=float)\n",
        "    s = vec.sum()\n",
        "    if s <= 0:\n",
        "        return None\n",
        "    return vec / s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0d20ddac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d2bf5ef8b67a49329fed5d36e2871545",
            "182e5a34e1444810b371dfd5ee5b8fa0",
            "c690d018f9c94d648a4f933ec098bf8c",
            "3071eece7c74410b8275b987c6d8e62e",
            "bfd9769e86904fe8bc2493809f26a7db",
            "20182c14a53c4d3e9a729e28810d02aa",
            "e3119156877045d58ab03bbb478484ef",
            "7205aea34cf1479fa1da8c8c1e4c3e80",
            "6ef15f106e924dcaa27cf465109c6bbf",
            "25ffd2fa116140f2a75ca182a7686cca",
            "498262ff71b34956a0169a7a8176f826"
          ]
        },
        "id": "0d20ddac",
        "outputId": "ce1b9422-091d-4ddf-c441-533ca73ac789"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2bf5ef8b67a49329fed5d36e2871545"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load model + LoRA + small head\n",
        "tokenizer = AutoTokenizer.from_pretrained(LORA_DIR, use_fast=True, token=hf_token)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token,\n",
        "    output_hidden_states=True\n",
        ")\n",
        "model = PeftModel.from_pretrained(base, LORA_DIR)\n",
        "model.eval()\n",
        "\n",
        "# Small 5-way classification head must match training definition\n",
        "class DistHead(nn.Module):\n",
        "    def __init__(self, hidden_size, K=5):\n",
        "        super().__init__()\n",
        "        self.out = nn.Linear(hidden_size, K)\n",
        "    def forward(self, x):  # x: [B, H]\n",
        "        return self.out(x) #   -> [B, K]\n",
        "\n",
        "hidden_size = base.config.hidden_size\n",
        "dist_head = DistHead(hidden_size, K=5).to(model.device)\n",
        "dist_head.load_state_dict(torch.load(HEAD_PATH, map_location=model.device))\n",
        "dist_head.eval()\n",
        "\n",
        "\n",
        "# GSS data prep (2016 & 2020)\n",
        "gss = pd.read_csv(GSS_PATH)\n",
        "gss = gss[gss[\"year\"].isin([2016, 2020])].copy()\n",
        "\n",
        "# Ignore education in prompts\n",
        "# Grouping keys\n",
        "GROUP_COLS = [\"generation\", \"gender\", \"race\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After loading base+LoRA and creating/loading dist_head:\n",
        "dist_head.load_state_dict(torch.load(HEAD_PATH, map_location=model.device))\n",
        "\n",
        "# Make the head match the model dtype (bf16) and device\n",
        "dist_head = dist_head.to(device=model.device, dtype=torch.bfloat16)\n",
        "dist_head.eval()\n"
      ],
      "metadata": {
        "id": "Aqvq9GyABKls",
        "outputId": "b6c2de00-b34c-4829-a09c-08edcbc9dacf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Aqvq9GyABKls",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistHead(\n",
              "  (out): Linear(in_features=4096, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cb459fa9",
      "metadata": {
        "id": "cb459fa9"
      },
      "outputs": [],
      "source": [
        "# Prompt builder\n",
        "QUESTION_TEXT = \"Attitudes toward abortion over the years\"\n",
        "\n",
        "def build_transition_prompt(group_meta, from_option):\n",
        "    \"\"\"\n",
        "    Training-style prompt you used, with edu_2016/edu_2020 present but set to NA.\n",
        "    Example:\n",
        "    [Task: Predict transition distribution]\n",
        "    Survey: UAS\n",
        "    From year: 2016  →  To year: 2020\n",
        "    Group: edu_2016=NA; edu_2020=NA; gender=Female; generation=Baby Boomer; race=Asian\n",
        "    Question: Attitudes toward abortion over the years\n",
        "    From option: anti\n",
        "    \"\"\"\n",
        "    return (\n",
        "        \"[Task: Predict transition distribution]\\n\"\n",
        "        \"Survey: UAS\\n\"\n",
        "        \"From year: 2016  \\u2192  To year: 2020\\n\"\n",
        "        f\"Group: edu_2016=NA; edu_2020=NA; \"\n",
        "        f\"gender={group_meta['gender']}; \"\n",
        "        f\"generation={group_meta['generation']}; \"\n",
        "        f\"race={group_meta['race']}\\n\"\n",
        "        f\"Question: {QUESTION_TEXT}\\n\"\n",
        "        f\"From option: {from_option}\\n\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1bbcdbc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bbcdbc6",
        "outputId": "d2cf29f3-bfc0-48ac-a56c-172b8e4ea745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total prompts to score: 170  (groups=34 × 5 from-options)\n"
          ]
        }
      ],
      "source": [
        "# Head-based predictor\n",
        "def predict_probs_with_head(texts, max_length=768, batch_size=32):\n",
        "    \"\"\"\n",
        "    Given a list of prompts (same style as training), return [N,5] probs over CANON5.\n",
        "    \"\"\"\n",
        "    out = np.zeros((len(texts), 5), dtype=np.float32)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            chunk = texts[i:i+batch_size]\n",
        "            enc = tokenizer(\n",
        "                chunk, return_tensors=\"pt\",\n",
        "                padding=True, truncation=True, max_length=max_length\n",
        "            )\n",
        "            ids = enc[\"input_ids\"].to(model.device)\n",
        "            attn = enc[\"attention_mask\"].to(model.device)\n",
        "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "                m_out = model(input_ids=ids, attention_mask=attn, output_hidden_states=True)\n",
        "                last_idx = attn.sum(dim=1) - 1\n",
        "                # Last hidden of last layer\n",
        "                last_vec = m_out.hidden_states[-1][torch.arange(m_out.hidden_states[-1].size(0)), last_idx]  # [B,H]\n",
        "                logits5 = dist_head(last_vec)  # [B,5]\n",
        "            p = F.softmax(logits5, dim=1).float().cpu().numpy()\n",
        "            out[i:i+batch_size] = p\n",
        "    return out\n",
        "\n",
        "\n",
        "# Build subgroup list (only those present in GSS)\n",
        "subgroups = gss[GROUP_COLS].drop_duplicates().reset_index(drop=True)\n",
        "subgroup_dicts = subgroups.to_dict(orient=\"records\")\n",
        "\n",
        "\n",
        "# Build prompts for each subgroup × from_option (5 prompts per subgroup)\n",
        "all_prompts = []\n",
        "index_map = []  # (group_idx, from_idx)\n",
        "for gi, g in enumerate(subgroup_dicts):\n",
        "    for k, from_opt in enumerate(CANON5):\n",
        "        all_prompts.append(build_transition_prompt(g, from_opt))\n",
        "        index_map.append((gi, k))\n",
        "\n",
        "print(f\"Total prompts to score: {len(all_prompts)}  (groups={len(subgroup_dicts)} \\u00d7 5 from-options)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict transitions (5x5 per subgroup)\n",
        "all_probs = predict_probs_with_head(all_prompts, max_length=768, batch_size=32)  # [G*5, 5]\n",
        "\n",
        "G = len(subgroup_dicts)\n",
        "T_mats = [np.zeros((5,5), dtype=np.float32) for _ in range(G)]\n",
        "for (gi, from_k), row_prob in zip(index_map, all_probs):\n",
        "    T_mats[gi][from_k, :] = row_prob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx7HlyV1AFJW",
        "outputId": "c81629eb-ddbf-421c-ca37-6da9f01f33ac"
      },
      "id": "cx7HlyV1AFJW",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-364935359.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7dc0e5bf",
      "metadata": {
        "id": "7dc0e5bf"
      },
      "outputs": [],
      "source": [
        "# Empirical 2016 for each subgroup; project to 2020: p2020 = p2016 @ T\n",
        "# Also (optional) empirical 2020 for evaluation\n",
        "def mask_group(df, g):\n",
        "    m = (df[\"generation\"]==g[\"generation\"]) & (df[\"gender\"]==g[\"gender\"]) & (df[\"race\"]==g[\"race\"])\n",
        "    return m\n",
        "\n",
        "rows_pred, rows_eval = [], []\n",
        "for gi, g in enumerate(subgroup_dicts):\n",
        "    m2016 = mask_group(gss, g) & (gss[\"year\"]==2016)\n",
        "    emp2016 = fivebin_empirical(gss.loc[m2016, \"att5\"])\n",
        "    if emp2016 is None:\n",
        "        # skip groups with no mappable 2016 answers\n",
        "        continue\n",
        "\n",
        "    T = T_mats[gi]  # [5,5]\n",
        "    pred2020 = emp2016 @ T\n",
        "\n",
        "    rec = {**g}\n",
        "    for j, cat in enumerate(CANON5):\n",
        "        rec[f\"emp2016_{cat}\"] = float(emp2016[j])\n",
        "        rec[f\"pred2020_{cat}\"] = float(pred2020[j])\n",
        "    rows_pred.append(rec)\n",
        "\n",
        "    # Optional evaluation vs empirical 2020\n",
        "    m2020 = mask_group(gss, g) & (gss[\"year\"]==2020)\n",
        "    emp2020 = fivebin_empirical(gss.loc[m2020, \"att5\"])\n",
        "    if emp2020 is not None:\n",
        "        def rmse(a,b): return float(np.sqrt(np.mean((a-b)**2)))\n",
        "        def jsd(p,q,eps=1e-9):\n",
        "            p = np.clip(p,eps,1); q = np.clip(q,eps,1)\n",
        "            p/=p.sum(); q/=q.sum()\n",
        "            m = 0.5*(p+q)\n",
        "            def kl(x,y): return float(np.sum(x*(np.log(x+eps)-np.log(y+eps))))\n",
        "            return 0.5*kl(p,m)+0.5*kl(q,m)\n",
        "\n",
        "        ev = {**g, \"n2016\": int(m2016.sum()), \"n2020\": int(m2020.sum())}\n",
        "        ev[\"RMSE\"] = rmse(emp2020, pred2020)\n",
        "        ev[\"JSD\"]  = jsd(emp2020, pred2020)\n",
        "        for j, cat in enumerate(CANON5):\n",
        "            ev[f\"emp2020_{cat}\"]  = float(emp2020[j])\n",
        "            ev[f\"pred2020_{cat}\"] = float(pred2020[j])\n",
        "        rows_eval.append(ev)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save outputs\n",
        "df_pred = pd.DataFrame(rows_pred).sort_values(GROUP_COLS).reset_index(drop=True)\n",
        "df_pred.to_csv(PRED_OUT, index=False)\n",
        "print(\"Saved projections:\", PRED_OUT)\n",
        "\n",
        "if len(rows_eval) > 0:\n",
        "    df_eval = pd.DataFrame(rows_eval).sort_values(GROUP_COLS).reset_index(drop=True)\n",
        "    df_eval.to_csv(EVAL_OUT, index=False)\n",
        "    print(\"Saved eval:\", EVAL_OUT)\n",
        "else:\n",
        "    print(\"No empirical 2020 rows available for evaluation (check GSS mapping).\")\n",
        "\n",
        "# Year-over-year deltas (predicted) per subgroup\n",
        "if not df_pred.empty:\n",
        "    wide = df_pred[[*GROUP_COLS, *[f\"emp2016_{c}\" for c in CANON5], *[f\"pred2020_{c}\" for c in CANON5]]].copy()\n",
        "    # compute 2020-2016 deltas for each category\n",
        "    for c in CANON5:\n",
        "        wide[f\"delta_{c}\"] = wide[f\"pred2020_{c}\"] - wide[f\"emp2016_{c}\"]\n",
        "    wide.to_csv(DELTA_OUT, index=False)\n",
        "    print(\"Saved deltas:\", DELTA_OUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXg7pfwi_b_4",
        "outputId": "fb1c017b-c64a-4401-88c9-109da3e4370d"
      },
      "id": "xXg7pfwi_b_4",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved projections: /content/drive/MyDrive/LLM_POC_Study_2025_v2/tllm_fixA_head/gss_tllm_fixA_proj_2020_from_2016.csv\n",
            "Saved eval: /content/drive/MyDrive/LLM_POC_Study_2025_v2/tllm_fixA_head/gss_tllm_fixA_eval_2020_vs_empirical.csv\n",
            "Saved deltas: /content/drive/MyDrive/LLM_POC_Study_2025_v2/tllm_fixA_head/gss_tllm_fixA_pred_deltas_2020_minus_2016.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d2bf5ef8b67a49329fed5d36e2871545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_182e5a34e1444810b371dfd5ee5b8fa0",
              "IPY_MODEL_c690d018f9c94d648a4f933ec098bf8c",
              "IPY_MODEL_3071eece7c74410b8275b987c6d8e62e"
            ],
            "layout": "IPY_MODEL_bfd9769e86904fe8bc2493809f26a7db"
          }
        },
        "182e5a34e1444810b371dfd5ee5b8fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20182c14a53c4d3e9a729e28810d02aa",
            "placeholder": "​",
            "style": "IPY_MODEL_e3119156877045d58ab03bbb478484ef",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c690d018f9c94d648a4f933ec098bf8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7205aea34cf1479fa1da8c8c1e4c3e80",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ef15f106e924dcaa27cf465109c6bbf",
            "value": 4
          }
        },
        "3071eece7c74410b8275b987c6d8e62e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25ffd2fa116140f2a75ca182a7686cca",
            "placeholder": "​",
            "style": "IPY_MODEL_498262ff71b34956a0169a7a8176f826",
            "value": " 4/4 [00:05&lt;00:00,  1.17s/it]"
          }
        },
        "bfd9769e86904fe8bc2493809f26a7db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20182c14a53c4d3e9a729e28810d02aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3119156877045d58ab03bbb478484ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7205aea34cf1479fa1da8c8c1e4c3e80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef15f106e924dcaa27cf465109c6bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25ffd2fa116140f2a75ca182a7686cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "498262ff71b34956a0169a7a8176f826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}