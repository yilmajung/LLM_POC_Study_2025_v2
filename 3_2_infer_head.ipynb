{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05409eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inference with small classification head (Fix A)\n",
    "!pip -q install peft transformers accelerate sentencepiece\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebe9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Config (EDIT ME)\n",
    "# -----------------\n",
    "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"   # or \"mistralai/Mistral-7B-v0.3\"\n",
    "INF_DIR         = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/tllm_fixA_head\"     # where you saved training outputs\n",
    "LORA_DIR        = os.path.join(INF_DIR, \"lora\")                                    # saved LoRA+tokenizer folder\n",
    "HEAD_PATH       = os.path.join(INF_DIR, \"dist_head.pt\")                             # saved small head weights\n",
    "GSS_PATH        = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_panel_2016_2020_long.parquet\"\n",
    "\n",
    "# Output CSVs\n",
    "PRED_OUT   = os.path.join(INF_DIR, \"gss_tllm_fixA_proj_2020_from_2016.csv\")\n",
    "EVAL_OUT   = os.path.join(INF_DIR, \"gss_tllm_fixA_eval_2020_vs_empirical.csv\")\n",
    "DELTA_OUT  = os.path.join(INF_DIR, \"gss_tllm_fixA_pred_deltas_2020_minus_2016.csv\")\n",
    "\n",
    "# If you need HF auth in Colab:\n",
    "hf_token = userdata.get('HF_TOKEN', None)\n",
    "\n",
    "# -----------------\n",
    "# Canonical bins (5-way; UNSURE excluded at inference)\n",
    "# -----------------\n",
    "CANON5 = [\"strong_anti\", \"anti\", \"neutral\", \"pro\", \"strong_pro\"]\n",
    "IDX = {k:i for i,k in enumerate(CANON5)}\n",
    "\n",
    "# Map your GSS att5 -> 5 canonical bins (EDIT if your labels differ)\n",
    "GSS_ATT5_TO_5CANON = {\n",
    "    \"Illegal in all cases\": \"strong_anti\",\n",
    "    \"Illegal in most cases\": \"anti\",\n",
    "    \"Legal in most cases\": \"pro\",\n",
    "    \"Legal in all cases\": \"strong_pro\",\n",
    "    # Any other responses (DK/NA/Refused/etc.) will be dropped for the 5-bin distribution\n",
    "}\n",
    "\n",
    "def fivebin_empirical(series):\n",
    "    \"\"\"Return 5-bin prob vector in CANON5 order; None if no mappable answers.\"\"\"\n",
    "    mapped = series.map(GSS_ATT5_TO_5CANON).dropna()\n",
    "    if mapped.empty:\n",
    "        return None\n",
    "    cnt = mapped.value_counts()\n",
    "    vec = np.array([cnt.get(k, 0.0) for k in CANON5], dtype=float)\n",
    "    s = vec.sum()\n",
    "    if s <= 0:\n",
    "        return None\n",
    "    return vec / s\n",
    "\n",
    "# -----------------\n",
    "# Load model + LoRA + small head\n",
    "# -----------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_DIR, use_fast=True, token=hf_token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_token,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, LORA_DIR)\n",
    "model.eval()\n",
    "\n",
    "# Small 5-way classification head must match training definition\n",
    "class DistHead(nn.Module):\n",
    "    def __init__(self, hidden_size, K=5):\n",
    "        super().__init__()\n",
    "        self.out = nn.Linear(hidden_size, K)\n",
    "    def forward(self, x):  # x: [B, H]\n",
    "        return self.out(x) #   -> [B, K]\n",
    "\n",
    "hidden_size = base.config.hidden_size\n",
    "dist_head = DistHead(hidden_size, K=5).to(model.device)\n",
    "dist_head.load_state_dict(torch.load(HEAD_PATH, map_location=model.device))\n",
    "dist_head.eval()\n",
    "\n",
    "# -----------------\n",
    "# GSS data prep (2016 & 2020)\n",
    "# -----------------\n",
    "gss = pd.read_parquet(GSS_PATH)\n",
    "gss = gss[gss[\"year\"].isin([2016, 2020])].copy()\n",
    "\n",
    "# We ignore education in prompts now, per your request.\n",
    "# Grouping keys (adjust if you need to split race/gender differently)\n",
    "GROUP_COLS = [\"generation\", \"gender\", \"race\"]\n",
    "\n",
    "# -----------------\n",
    "# Prompt builder (mirror your training style, but set edu_* = NA)\n",
    "# -----------------\n",
    "QUESTION_TEXT = \"Harmonized abortion attitude across waves\"\n",
    "\n",
    "def build_transition_prompt(group_meta, from_option):\n",
    "    \"\"\"\n",
    "    Training-style prompt you used, with edu_2016/edu_2020 present but set to NA.\n",
    "    Example:\n",
    "    [Task: Predict transition distribution]\n",
    "    Survey: UAS\n",
    "    From wave: 2016  →  To wave: 2020\n",
    "    Group: edu_2016=NA; edu_2020=NA; gender=Female; generation=Baby Boomer; race=Asian\n",
    "    Question: Harmonized abortion attitude across waves\n",
    "    From option: anti\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"[Task: Predict transition distribution]\\n\"\n",
    "        \"Survey: UAS\\n\"\n",
    "        \"From wave: 2016  \\u2192  To wave: 2020\\n\"\n",
    "        f\"Group: edu_2016=NA; edu_2020=NA; \"\n",
    "        f\"gender={group_meta['gender']}; \"\n",
    "        f\"generation={group_meta['generation']}; \"\n",
    "        f\"race={group_meta['race']}\\n\"\n",
    "        f\"Question: {QUESTION_TEXT}\\n\"\n",
    "        f\"From option: {from_option}\\n\"\n",
    "    )\n",
    "\n",
    "# -----------------\n",
    "# Head-based predictor\n",
    "# -----------------\n",
    "def predict_probs_with_head(texts, max_length=768, batch_size=32):\n",
    "    \"\"\"\n",
    "    Given a list of prompts (same style as training), return [N,5] probs over CANON5.\n",
    "    \"\"\"\n",
    "    out = np.zeros((len(texts), 5), dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            chunk = texts[i:i+batch_size]\n",
    "            enc = tokenizer(\n",
    "                chunk, return_tensors=\"pt\",\n",
    "                padding=True, truncation=True, max_length=max_length\n",
    "            )\n",
    "            ids = enc[\"input_ids\"].to(model.device)\n",
    "            attn = enc[\"attention_mask\"].to(model.device)\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                m_out = model(input_ids=ids, attention_mask=attn, output_hidden_states=True)\n",
    "            last_idx = attn.sum(dim=1) - 1\n",
    "            # Last hidden of last layer\n",
    "            last_vec = m_out.hidden_states[-1][torch.arange(m_out.hidden_states[-1].size(0)), last_idx]  # [B,H]\n",
    "            logits5 = dist_head(last_vec)  # [B,5]\n",
    "            p = F.softmax(logits5, dim=1).float().cpu().numpy()\n",
    "            out[i:i+batch_size] = p\n",
    "    return out\n",
    "\n",
    "# -----------------\n",
    "# Build subgroup list (only those present in GSS)\n",
    "# -----------------\n",
    "subgroups = gss[GROUP_COLS].drop_duplicates().reset_index(drop=True)\n",
    "subgroup_dicts = subgroups.to_dict(orient=\"records\")\n",
    "\n",
    "# -----------------\n",
    "# Build prompts for each subgroup × from_option (5 prompts per subgroup)\n",
    "# -----------------\n",
    "all_prompts = []\n",
    "index_map = []  # (group_idx, from_idx)\n",
    "for gi, g in enumerate(subgroup_dicts):\n",
    "    for k, from_opt in enumerate(CANON5):\n",
    "        all_prompts.append(build_transition_prompt(g, from_opt))\n",
    "        index_map.append((gi, k))\n",
    "\n",
    "print(f\"Total prompts to score: {len(all_prompts)}  (groups={len(subgroup_dicts)} × 5 from-options)\")\n",
    "\n",
    "# -----------------\n",
    "# Predict transitions (5x5 per subgroup)\n",
    "# -----------------\n",
    "all_probs = predict_probs_with_head(all_prompts, max_length=768, batch_size=32)  # [G*5, 5]\n",
    "\n",
    "G = len(subgroup_dicts)\n",
    "T_mats = [np.zeros((5,5), dtype=np.float32) for _ in range(G)]\n",
    "for (gi, from_k), row_prob in zip(index_map, all_probs):\n",
    "    T_mats[gi][from_k, :] = row_prob\n",
    "\n",
    "# -----------------\n",
    "# Empirical 2016 for each subgroup; project to 2020: p2020 = p2016 @ T\n",
    "# Also (optional) empirical 2020 for evaluation\n",
    "# -----------------\n",
    "def mask_group(df, g):\n",
    "    m = (df[\"generation\"]==g[\"generation\"]) & (df[\"gender\"]==g[\"gender\"]) & (df[\"race\"]==g[\"race\"])\n",
    "    return m\n",
    "\n",
    "rows_pred, rows_eval = [], []\n",
    "for gi, g in enumerate(subgroup_dicts):\n",
    "    m2016 = mask_group(gss, g) & (gss[\"year\"]==2016)\n",
    "    emp2016 = fivebin_empirical(gss.loc[m2016, \"att5\"])\n",
    "    if emp2016 is None:\n",
    "        # skip groups with no mappable 2016 answers\n",
    "        continue\n",
    "\n",
    "    T = T_mats[gi]  # [5,5]\n",
    "    pred2020 = emp2016 @ T\n",
    "\n",
    "    rec = {**g}\n",
    "    for j, cat in enumerate(CANON5):\n",
    "        rec[f\"emp2016_{cat}\"] = float(emp2016[j])\n",
    "        rec[f\"pred2020_{cat}\"] = float(pred2020[j])\n",
    "    rows_pred.append(rec)\n",
    "\n",
    "    # Optional evaluation vs empirical 2020\n",
    "    m2020 = mask_group(gss, g) & (gss[\"year\"]==2020)\n",
    "    emp2020 = fivebin_empirical(gss.loc[m2020, \"att5\"])\n",
    "    if emp2020 is not None:\n",
    "        def rmse(a,b): return float(np.sqrt(np.mean((a-b)**2)))\n",
    "        def jsd(p,q,eps=1e-9):\n",
    "            p = np.clip(p,eps,1); q = np.clip(q,eps,1)\n",
    "            p/=p.sum(); q/=q.sum()\n",
    "            m = 0.5*(p+q)\n",
    "            def kl(x,y): return float(np.sum(x*(np.log(x+eps)-np.log(y+eps))))\n",
    "            return 0.5*kl(p,m)+0.5*kl(q,m)\n",
    "\n",
    "        ev = {**g, \"n2016\": int(m2016.sum()), \"n2020\": int(m2020.sum())}\n",
    "        ev[\"RMSE\"] = rmse(emp2020, pred2020)\n",
    "        ev[\"JSD\"]  = jsd(emp2020, pred2020)\n",
    "        for j, cat in enumerate(CANON5):\n",
    "            ev[f\"emp2020_{cat}\"]  = float(emp2020[j])\n",
    "            ev[f\"pred2020_{cat}\"] = float(pred2020[j])\n",
    "        rows_eval.append(ev)\n",
    "\n",
    "# -----------------\n",
    "# Save outputs\n",
    "# -----------------\n",
    "df_pred = pd.DataFrame(rows_pred).sort_values(GROUP_COLS).reset_index(drop=True)\n",
    "df_pred.to_csv(PRED_OUT, index=False)\n",
    "print(\"Saved projections:\", PRED_OUT)\n",
    "\n",
    "if len(rows_eval) > 0:\n",
    "    df_eval = pd.DataFrame(rows_eval).sort_values(GROUP_COLS).reset_index(drop=True)\n",
    "    df_eval.to_csv(EVAL_OUT, index=False)\n",
    "    print(\"Saved eval:\", EVAL_OUT)\n",
    "else:\n",
    "    print(\"No empirical 2020 rows available for evaluation (check GSS mapping).\")\n",
    "\n",
    "# Year-over-year deltas (predicted) per subgroup\n",
    "if not df_pred.empty:\n",
    "    wide = df_pred[[*GROUP_COLS, *[f\"emp2016_{c}\" for c in CANON5], *[f\"pred2020_{c}\" for c in CANON5]]].copy()\n",
    "    # compute 2020-2016 deltas for each category\n",
    "    for c in CANON5:\n",
    "        wide[f\"delta_{c}\"] = wide[f\"pred2020_{c}\"] - wide[f\"emp2016_{c}\"]\n",
    "    wide.to_csv(DELTA_OUT, index=False)\n",
    "    print(\"Saved deltas:\", DELTA_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20ddac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb459fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbcdbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0e5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
