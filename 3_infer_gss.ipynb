{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52e62ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install + imports (Colab / A100)\n",
    "!pip -q install peft transformers accelerate sentencepiece\n",
    "\n",
    "import os, json, math, itertools, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonical bins (5 options; UNSURE excluded for inference)\n",
    "# Must match the OPT_TOKENS order during training.\n",
    "CANON5 = [\"strong_anti\", \"anti\", \"neutral\", \"pro\", \"strong_pro\"]\n",
    "IDX = {k:i for i,k in enumerate(CANON5)}\n",
    "\n",
    "OPT_TOKENS = [\n",
    "    \"<OPT_STRONG_ANTI>\", \"<OPT_ANTI>\", \"<OPT_NEUTRAL>\", \"<OPT_PRO>\", \"<OPT_STRONG_PRO>\"\n",
    "]\n",
    "\n",
    "# ---- Map GSS att5 -> 5 canonical bins (EDIT these to your actual labels) ----\n",
    "GSS_ATT5_TO_5CANON = {\n",
    "    \"Illegal in all cases\": \"strong_anti\",\n",
    "    \"Illegal in most cases\": \"anti\",\n",
    "    \"Legal in most cases\": \"pro\",\n",
    "    \"Legal in all cases\": \"strong_pro\",\n",
    "    # Anything else (DK/NA/Refused) will be dropped when making the 5-bin empirical dist\n",
    "}\n",
    "\n",
    "def fivebin_empirical(series):\n",
    "    \"\"\"\n",
    "    Map a Pandas Series of att5 strings to 5-bin probabilities, dropping non-mappable responses.\n",
    "    Returns np.array length 5 (ordered as CANON5) or None if no countable responses.\n",
    "    \"\"\"\n",
    "    mapped = series.map(GSS_ATT5_TO_5CANON).dropna()\n",
    "    if mapped.empty:\n",
    "        return None\n",
    "    cnt = mapped.value_counts()\n",
    "    vec = np.array([cnt.get(k, 0.0) for k in CANON5], dtype=float)\n",
    "    s = vec.sum()\n",
    "    if s <= 0:\n",
    "        return None\n",
    "    return vec / s\n",
    "\n",
    "# =========================================\n",
    "# 2) Load GSS long panel and compute modal education per group/year\n",
    "#    Required columns: yearid, year, generation, edu_level, gender, race, att5\n",
    "# =========================================\n",
    "GSS_PATH = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_panel_2016_2020_long.parquet\"\n",
    "gss = pd.read_parquet(GSS_PATH)\n",
    "gss = gss[gss['year'].isin([2016, 2020])].copy()\n",
    "\n",
    "GROUP_COLS = [\"generation\", \"edu_level\", \"gender\", \"race\"]  # edu_level is per-row; we'll create edu_2016 & edu_2020\n",
    "\n",
    "# Modal education within each (group-no-edu, year)\n",
    "BASE_GROUP = [\"generation\", \"gender\", \"race\"]\n",
    "\n",
    "def mode_or_na(s):\n",
    "    if s.dropna().empty:\n",
    "        return np.nan\n",
    "    return s.value_counts(dropna=True).idxmax()\n",
    "\n",
    "edu_mode = (\n",
    "    gss.groupby(BASE_GROUP + [\"year\"], dropna=False)[\"edu_level\"]\n",
    "       .agg(mode_or_na)\n",
    "       .reset_index()\n",
    "       .rename(columns={\"edu_level\": \"edu_mode\"})\n",
    ")\n",
    "\n",
    "edu_2016 = edu_mode[edu_mode[\"year\"]==2016][BASE_GROUP+[\"edu_mode\"]].rename(columns={\"edu_mode\":\"edu_2016\"})\n",
    "edu_2020 = edu_mode[edu_mode[\"year\"]==2020][BASE_GROUP+[\"edu_mode\"]].rename(columns={\"edu_mode\":\"edu_2020\"})\n",
    "\n",
    "# Build the set of groups present in data (based on BASE_GROUP) and attach edu_2016/edu_2020\n",
    "groups = (\n",
    "    gss[BASE_GROUP].drop_duplicates().merge(edu_2016, on=BASE_GROUP, how=\"left\")\n",
    "                                     .merge(edu_2020, on=BASE_GROUP, how=\"left\")\n",
    ")\n",
    "\n",
    "# Also keep only groups that have at least some 2016 answers for empirical baseline\n",
    "has_2016 = (\n",
    "    gss[gss[\"year\"]==2016]\n",
    "    .groupby(BASE_GROUP)[\"att5\"]\n",
    "    .size()\n",
    "    .reset_index(name=\"n2016\")\n",
    ")\n",
    "groups = groups.merge(has_2016, on=BASE_GROUP, how=\"left\").query(\"n2016 >= 1\").drop(columns=[\"n2016\"])\n",
    "\n",
    "print(\"Num unique base groups with 2016 data:\", len(groups))\n",
    "\n",
    "# =========================================\n",
    "# 3) Build prompts to mirror training EXACTLY (transition prompts)\n",
    "#    Example from your training:\n",
    "#    [Task: Predict transition distribution]\n",
    "#    Survey: UAS\n",
    "#    From wave: 2018  â†’  To wave: 2019\n",
    "#    Group: edu_2018=Bachelor's Degree; edu_2019=Bachelor's Degree; gender=Female; generation=Baby Boomer; race=Asian\n",
    "#    Question: Harmonized abortion attitude across waves\n",
    "#    From option: anti\n",
    "#    (then you appended the Options: ... line before Answer:)\n",
    "# =========================================\n",
    "QUESTION_TEXT = \"Harmonized abortion attitude across waves\"\n",
    "\n",
    "def build_transition_prompt(group_meta, edu_2016, edu_2020, from_option):\n",
    "    # IMPORTANT: keep wording/formatting consistent with training\n",
    "    # Use the same Unicode arrow and semicolon spacing, and same field order (edu first).\n",
    "    return (\n",
    "        \"[Task: Predict transition distribution]\\n\"\n",
    "        \"Survey: UAS\\n\"\n",
    "        \"From wave: 2016  \\u2192  To wave: 2020\\n\"\n",
    "        f\"Group: edu_2016={edu_2016 if pd.notna(edu_2016) else 'NA'}; \"\n",
    "        f\"edu_2020={edu_2020 if pd.notna(edu_2020) else 'NA'}; \"\n",
    "        f\"gender={group_meta['gender']}; \"\n",
    "        f\"generation={group_meta['generation']}; \"\n",
    "        f\"race={group_meta['race']}\\n\"\n",
    "        f\"Question: {QUESTION_TEXT}\\n\"\n",
    "        f\"From option: {from_option}\\n\"\n",
    "    )\n",
    "\n",
    "SUFFIX = \"Options: \" + \" \".join(OPT_TOKENS) + \"\\nAnswer:\\n\"\n",
    "\n",
    "# =========================================\n",
    "# 4) Load tokenizer + base + LoRA adapters\n",
    "# =========================================\n",
    "LORA_DIR = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/tllm_abortion_transitions_lora\"\n",
    "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"   # or \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN', None)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LORA_DIR, use_fast=True, token=hf_token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_token\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, LORA_DIR)\n",
    "model.eval()\n",
    "\n",
    "# Ensure option tokens exist (usually already in the saved tokenizer)\n",
    "missing = [t for t in OPT_TOKENS if t not in tokenizer.get_vocab()]\n",
    "if missing:\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": missing})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "opt_ids = torch.tensor([tokenizer.convert_tokens_to_ids(t) for t in OPT_TOKENS], device=model.device)\n",
    "\n",
    "# =========================================\n",
    "# 5) Batch predictor over the five option tokens (same as training head)\n",
    "# =========================================\n",
    "def predict_probs_for_texts(texts, max_length=768, batch_size=32):\n",
    "    out_probs = np.zeros((len(texts), len(OPT_TOKENS)), dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            chunk = texts[i:i+batch_size]\n",
    "            enc = tokenizer(\n",
    "                [t + SUFFIX for t in chunk],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            input_ids = enc[\"input_ids\"].to(model.device)\n",
    "            attn = enc[\"attention_mask\"].to(model.device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                out = model(input_ids=input_ids, attention_mask=attn)\n",
    "\n",
    "            last_idx = attn.sum(dim=1) - 1\n",
    "            last_logits = out.logits[torch.arange(out.logits.size(0), device=model.device), last_idx]  # [B,V]\n",
    "            opt_logits = last_logits[:, opt_ids]  # [B,5]\n",
    "            probs = F.softmax(opt_logits, dim=1).float().cpu().numpy()\n",
    "            out_probs[i:i+batch_size] = probs\n",
    "\n",
    "    return out_probs\n",
    "\n",
    "# =========================================\n",
    "# 6) Build all prompts per (group, from_option) and predict transitions\n",
    "#    For each group we need 5 prompts (one per From option) -> 5x5 transition\n",
    "# =========================================\n",
    "group_dicts = groups[BASE_GROUP + [\"edu_2016\",\"edu_2020\"]].to_dict(orient=\"records\")\n",
    "\n",
    "all_prompts = []\n",
    "index_map = []  # (group_idx, from_idx)\n",
    "for gi, g in enumerate(group_dicts):\n",
    "    for k, from_opt in enumerate(CANON5):\n",
    "        p = build_transition_prompt(g, g[\"edu_2016\"], g[\"edu_2020\"], from_opt)\n",
    "        all_prompts.append(p)\n",
    "        index_map.append((gi, k))\n",
    "\n",
    "print(\"Total prompts:\", len(all_prompts))\n",
    "\n",
    "all_probs = predict_probs_for_texts(all_prompts, max_length=768, batch_size=32)  # [G*5, 5]\n",
    "\n",
    "# Reassemble into a transition matrix per group (5x5 each)\n",
    "G = len(group_dicts)\n",
    "T_mats = [np.zeros((5,5), dtype=np.float32) for _ in range(G)]\n",
    "for (gi, from_k), row_prob in zip(index_map, all_probs):\n",
    "    T_mats[gi][from_k, :] = row_prob\n",
    "\n",
    "# =========================================\n",
    "# 7) Empirical 2016 distribution per group (5 bins, UNSURE dropped)\n",
    "#    Project to 2020 via p2020 = p2016 @ T\n",
    "# =========================================\n",
    "def sub_mask(df, g):\n",
    "    m = (df[\"generation\"]==g[\"generation\"]) & (df[\"gender\"]==g[\"gender\"]) & (df[\"race\"]==g[\"race\"])\n",
    "    return m\n",
    "\n",
    "rows_pred = []\n",
    "rows_eval = []  # optional, add empirical 2020 if available\n",
    "\n",
    "for gi, g in enumerate(group_dicts):\n",
    "    # empirical 2016 (5 bins)\n",
    "    m2016 = sub_mask(gss, g) & (gss[\"year\"]==2016)\n",
    "    emp2016 = fivebin_empirical(gss.loc[m2016, \"att5\"])\n",
    "    if emp2016 is None:\n",
    "        continue  # skip groups with no mappable 2016 answers\n",
    "\n",
    "    # project to 2020\n",
    "    T = T_mats[gi]  # 5x5\n",
    "    pred2020 = emp2016 @ T\n",
    "\n",
    "    rec = {\n",
    "        \"generation\": g[\"generation\"],\n",
    "        \"gender\": g[\"gender\"],\n",
    "        \"race\": g[\"race\"],\n",
    "        \"edu_2016\": g[\"edu_2016\"],\n",
    "        \"edu_2020\": g[\"edu_2020\"],\n",
    "    }\n",
    "    for j, cat in enumerate(CANON5):\n",
    "        rec[f\"emp2016_{cat}\"] = float(emp2016[j])\n",
    "        rec[f\"pred2020_{cat}\"] = float(pred2020[j])\n",
    "    rows_pred.append(rec)\n",
    "\n",
    "    # optional: empirical 2020 to compare\n",
    "    m2020 = sub_mask(gss, g) & (gss[\"year\"]==2020)\n",
    "    emp2020 = fivebin_empirical(gss.loc[m2020, \"att5\"])\n",
    "    if emp2020 is not None:\n",
    "        ev = {\n",
    "            **{k: rec[k] for k in [\"generation\",\"gender\",\"race\",\"edu_2016\",\"edu_2020\"]},\n",
    "            \"n2016\": int(m2016.sum()),\n",
    "            \"n2020\": int(m2020.sum())\n",
    "        }\n",
    "        # metrics\n",
    "        def rmse(a,b): return float(np.sqrt(np.mean((a-b)**2)))\n",
    "        def jsd(p,q,eps=1e-9):\n",
    "            p = np.clip(p,eps,1); q = np.clip(q,eps,1)\n",
    "            p/=p.sum(); q/=q.sum()\n",
    "            m = 0.5*(p+q)\n",
    "            def kl(x,y): return float(np.sum(x*(np.log(x+eps)-np.log(y+eps))))\n",
    "            return 0.5*kl(p,m)+0.5*kl(q,m)\n",
    "\n",
    "        ev[\"RMSE\"] = rmse(emp2020, pred2020)\n",
    "        ev[\"JSD\"]  = jsd(emp2020, pred2020)\n",
    "        for j, cat in enumerate(CANON5):\n",
    "            ev[f\"emp2020_{cat}\"]  = float(emp2020[j])\n",
    "            ev[f\"pred2020_{cat}\"] = float(pred2020[j])\n",
    "        rows_eval.append(ev)\n",
    "\n",
    "df_pred = pd.DataFrame(rows_pred).sort_values([\"generation\",\"gender\",\"race\"]).reset_index(drop=True)\n",
    "df_eval = pd.DataFrame(rows_eval).sort_values([\"generation\",\"gender\",\"race\"]).reset_index(drop=True)\n",
    "\n",
    "PRED_OUT = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_tllm_proj_2020_from_2016.csv\"\n",
    "df_pred.to_csv(PRED_OUT, index=False)\n",
    "print(\"Saved projections:\", PRED_OUT)\n",
    "\n",
    "if not df_eval.empty:\n",
    "    EVAL_OUT = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_tllm_eval_2020_vs_empirical.csv\"\n",
    "    df_eval.to_csv(EVAL_OUT, index=False)\n",
    "    print(\"Saved eval:\", EVAL_OUT)\n",
    "else:\n",
    "    print(\"No empirical 2020 (5-bin) rows available for evaluation; check GSS mapping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e693472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5c035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd6d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935281c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
