{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers accelerate peft sentencepiece pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a23af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "CS_CSV    = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_cs_rev.csv\"       # cross-sectional long\n",
    "PANEL_CSV = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_panel_rev.csv\"    # panel long\n",
    "OUT_DIR   = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_p_trial\"          # where I write JSONLs & checkpoints\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# LLM choice\n",
    "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"  # or \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "# Canonical bins (K=4)\n",
    "ABORT4 = [\"strong_anti\", \"anti\", \"pro\", \"strong_pro\"]\n",
    "TRUST3 = [\"distrust\", \"depends\", \"trust\"]\n",
    "ENVIR3 = [\"too_little\", \"about_right\", \"too_much\"]\n",
    "ABORT2ID = {c:i for i,c in enumerate(ABORT4)}\n",
    "TRUST2ID = {c:i for i,c in enumerate(TRUST3)}\n",
    "ENVIR2ID = {c:i for i,c in enumerate(ENVIR3)}\n",
    "K = len(TRUST3)\n",
    "\n",
    "YEARS_CS = list(range(2006, 2025, 2))  # 2006..2024 every 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and harmonize the data\n",
    "# --- Cross-sectional ---\n",
    "cs = pd.read_csv(CS_CSV)\n",
    "# Expect: yearid, year, abortion_att4, generation, race, gender, edu_level, wtssps\n",
    "# Map the attitude to canonical\n",
    "cs[\"att\"] = cs[\"abortion_att4\"].astype(str).str.strip()\n",
    "# keep only canon categories, drop NAs\n",
    "cs_abortion = cs[cs[\"att\"].isin(ABORT4)].copy()\n",
    "cs_abortion[\"wt\"] = cs_abortion.get(\"wtssps\", pd.Series([1.0]*len(cs_abortion)))  # default 1.0 if missing\n",
    "cs_trust = cs[cs[\"trust\"].astype(str).str.strip().isin(TRUST3)].copy()\n",
    "cs_trust['wt'] = cs_trust.get(\"wtssps\", pd.Series([1.0]*len(cs_trust)))  # default 1.0 if missing\n",
    "\n",
    "cs_envir = cs[cs[\"natenvir\"].astype(str).str.strip().isin(ENVIR3)].copy()\n",
    "cs_envir['wt'] = cs_envir.get(\"wtssps\", pd.Series([1.0]*len(cs_envir)))  # default 1.0 if missing\n",
    "\n",
    "# --- Panel ---\n",
    "pl = pd.read_csv(PANEL_CSV)\n",
    "pl[\"att\"] = pl[\"abortion_att4\"].astype(str).str.strip()\n",
    "pl_abortion = pl[pl[\"att\"].isin(ABORT4)].copy()\n",
    "pl_trust = pl[pl[\"trust\"].astype(str).str.strip().isin(TRUST3)].copy()\n",
    "pl_envir = pl[pl[\"natenvir\"].astype(str).str.strip().isin(ENVIR3)].copy()\n",
    "\n",
    "# Define grouping keys\n",
    "GROUP_COLS_4 = [\"generation\",\"gender\",\"race\",\"edu_level\"]\n",
    "GROUP_COLS_3 = [\"generation\",\"gender\",\"race\"]\n",
    "GROUP_COLS_1 = [\"generation\"]\n",
    "\n",
    "for df in (cs, pl):\n",
    "    for c in GROUP_COLS_4:\n",
    "        df[c] = df[c].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71237c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cross-section margins p_cs[g,y] (weighted)\n",
    "def group_key(row):\n",
    "    return (row[\"generation\"], row[\"gender\"], row[\"race\"], row['edu_level'])\n",
    "\n",
    "def weighted_probs(vals, wts, cats=TRUST3):\n",
    "    # vals: list of category strings; wts: weights\n",
    "    counts = {c:0.0 for c in cats}\n",
    "    for v, w in zip(vals, wts):\n",
    "        counts[v] += float(w)\n",
    "    vec = np.array([counts[c] for c in cats], dtype=float)\n",
    "    s = vec.sum()\n",
    "    if s <= 0: return None\n",
    "    return vec / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88063f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_cs[(g,y)] -> np.array[K]\n",
    "p_cs = {}\n",
    "effN_cs = {}  # effective N for weighting samples in Task B\n",
    "for y in YEARS_CS:\n",
    "    sub = cs_trust[cs_trust[\"year\"]==y]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    for g_vals, df_g in sub.groupby(GROUP_COLS_4):\n",
    "        p = weighted_probs(df_g[\"att\"].tolist(), df_g[\"wt\"].tolist(), TRUST3)\n",
    "        if p is None:\n",
    "            continue\n",
    "        p_cs[(g_vals, y)] = p\n",
    "        effN_cs[(g_vals, y)] = float(df_g[\"wt\"].sum())\n",
    "\n",
    "# Build panel transitions C[(g,t,Δ)] from consecutive waves per yearid\n",
    "# Build transitions per (g, t, Δ)\n",
    "# C[(g,t,Δ)] -> KxK counts; Nfrom[(g,t,Δ)] -> row totals length K\n",
    "from collections import defaultdict\n",
    "\n",
    "def canon_index(cat):\n",
    "    return TRUST2ID.get(cat, None)\n",
    "\n",
    "C = defaultdict(lambda: np.zeros((K,K), dtype=float))\n",
    "Nfrom = defaultdict(lambda: np.zeros((K,), dtype=float))\n",
    "\n",
    "# Weight=1.0\n",
    "pl_trust[\"w\"] = 1.0\n",
    "\n",
    "# Consecutive transitions per id\n",
    "for (pid), df_id in pl_trust.groupby(\"yearid\"):\n",
    "    df_id = df_id.sort_values(\"year\")\n",
    "    # collapse duplicates per year if any\n",
    "    df_id = df_id.drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "    years = df_id[\"year\"].values.tolist()\n",
    "    atts  = df_id[\"att\"].values.tolist()\n",
    "    wgts  = df_id[\"w\"].values.astype(float).tolist()\n",
    "    gens  = df_id[\"generation\"].values.tolist()\n",
    "    gend  = df_id[\"gender\"].values.tolist()\n",
    "    race  = df_id[\"race\"].values.tolist()\n",
    "    edu = df_id[\"edu_level\"].values.tolist()\n",
    "\n",
    "    # require consistent group labels across waves for this id (common in panels)\n",
    "    if not (len(set(gens))==1 and len(set(gend))==1 and len(set(race))==1 and len(set(edu))==1):\n",
    "        # if you prefer, skip inconsistent cases\n",
    "        continue\n",
    "\n",
    "    g = (gens[0], gend[0], race[0], edu[0])\n",
    "    for i in range(len(years)-1):\n",
    "        t, t1 = int(years[i]), int(years[i+1])\n",
    "        Δ = t1 - t\n",
    "        if Δ not in (2,4):  # we only have 2-yr and 4-yr gaps here\n",
    "            continue\n",
    "        ai = canon_index(atts[i])\n",
    "        aj = canon_index(atts[i+1])\n",
    "        if ai is None or aj is None:\n",
    "            continue\n",
    "        w = float(wgts[i])  # or min(wgts[i], wgts[i+1])\n",
    "        C[(g,t,Δ)][ai, aj] += w\n",
    "        Nfrom[(g,t,Δ)][ai] += w\n",
    "\n",
    "\n",
    "# JSONL builders: Task A (panel rows) and Task B (margins)\n",
    "def smooth_row(row_counts, alpha):\n",
    "    rc = np.array(row_counts, dtype=float) + alpha\n",
    "    s = rc.sum()\n",
    "    if s <= 0:\n",
    "        return np.ones_like(rc)/len(rc)\n",
    "    return rc / s\n",
    "\n",
    "def build_taskA_rows(C, Nfrom, p_cs, out_jsonl, alpha_small=0.05, alpha_big=0.25, n_thresh=20, cap_weight=100.0):\n",
    "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
    "    n_rows = 0\n",
    "    for (g,t,Δ), mat in C.items():\n",
    "        nrow = Nfrom[(g,t,Δ)]\n",
    "        for i in range(K):\n",
    "            alpha = alpha_small if nrow[i] >= n_thresh else alpha_big\n",
    "            tgt = smooth_row(mat[i,:], alpha=alpha).tolist()\n",
    "            # Build prompt\n",
    "            prompt = (\n",
    "                \"[Task: Predict transition row]\\n\"\n",
    "                f\"From: <Y{t}> → To: <Y{t+Δ}> <DT{Δ}>\\n\"\n",
    "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}; edu_level={g[3]}\\n\"\n",
    "                f\"From option: {TRUST3[i]}\\n\"\n",
    "                \"Answer:\\n\"\n",
    "            )\n",
    "            rec = {\n",
    "                \"task\": \"row\",\n",
    "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3]},\n",
    "                \"year_t\": t,\n",
    "                \"year_t1\": t+Δ,\n",
    "                \"dt\": Δ,\n",
    "                \"from_bin\": TRUST3[i],\n",
    "                \"prompt_text\": prompt,\n",
    "                \"to_dist\": tgt,\n",
    "                \"weight\": float(min(nrow[i], cap_weight))\n",
    "            }\n",
    "            # Attach margins if both available (for consistency loss later)\n",
    "            if ((g,t) in p_cs) and ((g,t+Δ) in p_cs):\n",
    "                rec[\"p_curr\"] = p_cs[(g,t)].tolist()\n",
    "                rec[\"p_next\"] = p_cs[(g,t+Δ)].tolist()\n",
    "            out.write(json.dumps(rec) + \"\\n\")\n",
    "            n_rows += 1\n",
    "    out.close()\n",
    "    return n_rows\n",
    "\n",
    "def build_taskB_rows(p_cs, effN_cs, out_jsonl, lags=(2,4), cap_weight=500.0):\n",
    "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
    "    n_rows = 0\n",
    "    # group by g\n",
    "    by_g = {}\n",
    "    for (g,y) in p_cs.keys():\n",
    "        by_g.setdefault(g, []).append(y)\n",
    "    for g, years in by_g.items():\n",
    "        ys = sorted(years)\n",
    "        for y in ys:\n",
    "            # context from previous lags\n",
    "            ctx = []\n",
    "            for L in lags:\n",
    "                yprev = y - L\n",
    "                if (g, yprev) in p_cs:\n",
    "                    ctx.append((yprev, p_cs[(g, yprev)]))\n",
    "            if len(ctx) == 0:\n",
    "                continue\n",
    "            # Build prompt\n",
    "            ctx_parts = \" \".join([f\"<Y{yy}>[{','.join(f'{x:.4f}' for x in p)}]\" for (yy,p) in ctx])\n",
    "            prompt = (\n",
    "                \"[Task: Forecast next-wave margin]\\n\"\n",
    "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}; edu_level={g[3]}\\n\"\n",
    "                f\"Context: {ctx_parts}\\n\"\n",
    "                f\"Predict: <Y{y}>\\n\"\n",
    "                \"Answer:\\n\"\n",
    "            )\n",
    "            w = float(min(effN_cs.get((g,y), 1.0), cap_weight))\n",
    "            rec = {\n",
    "                \"task\": \"margin\",\n",
    "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3]},\n",
    "                \"year\": y,\n",
    "                \"prompt_text\": prompt,\n",
    "                \"to_dist\": p_cs[(g,y)].tolist(),\n",
    "                \"weight\": w\n",
    "            }\n",
    "            out.write(json.dumps(rec) + \"\\n\")\n",
    "            n_rows += 1\n",
    "    out.close()\n",
    "    return n_rows\n",
    "\n",
    "TASKA_JSONL = os.path.join(OUT_DIR, \"taskA_panel_rows_trust_g4.jsonl\")\n",
    "TASKB_JSONL = os.path.join(OUT_DIR, \"taskB_margins_trust_g4.jsonl\")\n",
    "\n",
    "na = build_taskA_rows(C, Nfrom, p_cs, TASKA_JSONL)\n",
    "nb = build_taskB_rows(p_cs, effN_cs, TASKB_JSONL)\n",
    "print(f\"Wrote Task A rows: {na}\")\n",
    "print(f\"Wrote Task B rows: {nb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297302ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and Collator (multitask)\n",
    "class MTJsonlDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, jsonl_paths: List[str], tokenizer, max_len=768):\n",
    "        self.rows = []\n",
    "        for p in jsonl_paths:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.rows.extend([json.loads(x) for x in f])\n",
    "        random.shuffle(self.rows)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.rows[i]\n",
    "        enc = self.tok(r[\"prompt_text\"], return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "        out = {\n",
    "            \"task_type\": r[\"task\"],\n",
    "            \"input_ids\": enc[\"input_ids\"][0],\n",
    "            \"attention_mask\": enc[\"attention_mask\"][0],\n",
    "            \"to_dist\": torch.tensor(r[\"to_dist\"], dtype=torch.float),\n",
    "            \"weight\": torch.tensor(float(r.get(\"weight\", 1.0)), dtype=torch.float),\n",
    "        }\n",
    "        # add optional consistency fields\n",
    "        if r[\"task\"]==\"row\" and (\"p_curr\" in r) and (\"p_next\" in r):\n",
    "            out[\"p_curr\"] = torch.tensor(r[\"p_curr\"], dtype=torch.float)\n",
    "            out[\"p_next\"] = torch.tensor(r[\"p_next\"], dtype=torch.float)\n",
    "            out[\"has_consistency\"] = torch.tensor(1, dtype=torch.long)\n",
    "        else:\n",
    "            out[\"has_consistency\"] = torch.tensor(0, dtype=torch.long)\n",
    "        return out\n",
    "\n",
    "# Model: LLM backbone + two small heads + last-K pooling\n",
    "# Tokenizer & backbone\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def mt_collate(batch):\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    maxlen = max(x[\"input_ids\"].shape[0] for x in batch)\n",
    "\n",
    "    def pad(seq, pad_val, target_len):\n",
    "        pad_n = target_len - seq.shape[0]\n",
    "        if pad_n <= 0: return seq\n",
    "        return torch.cat([seq, torch.full((pad_n,), pad_val, dtype=seq.dtype)])\n",
    "\n",
    "    input_ids      = torch.stack([pad(x[\"input_ids\"], pad_id, maxlen) for x in batch])\n",
    "    attention_mask = torch.stack([pad(x[\"attention_mask\"], 0, maxlen) for x in batch])\n",
    "    to_dist        = torch.stack([x[\"to_dist\"] for x in batch])\n",
    "    weight         = torch.stack([x[\"weight\"] for x in batch])\n",
    "    has_cons       = torch.stack([x[\"has_consistency\"] for x in batch])\n",
    "    # p_curr/p_next if present; else zeros\n",
    "    p_curr = torch.zeros(len(batch), K, dtype=torch.float)\n",
    "    p_next = torch.zeros(len(batch), K, dtype=torch.float)\n",
    "    for i, x in enumerate(batch):\n",
    "        if x[\"has_consistency\"]==1:\n",
    "            p_curr[i] = x[\"p_curr\"]\n",
    "            p_next[i] = x[\"p_next\"]\n",
    "\n",
    "    task_types = [x[\"task_type\"] for x in batch]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"to_dist\": to_dist,\n",
    "        \"weight\": weight,\n",
    "        \"has_consistency\": has_cons,\n",
    "        \"p_curr\": p_curr,\n",
    "        \"p_next\": p_next,\n",
    "        \"task_types\": task_types,\n",
    "    }\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "# Add LoRA to attention/MLP projections\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "# Two task heads\n",
    "class TwoHead(nn.Module):\n",
    "    def __init__(self, hidden_size, K):\n",
    "        super().__init__()\n",
    "        self.head_row    = nn.Linear(hidden_size, K)   # Task A\n",
    "        self.head_margin = nn.Linear(hidden_size, K)   # Task B\n",
    "\n",
    "    def forward(self, feats):\n",
    "        return self.head_row(feats), self.head_margin(feats)\n",
    "\n",
    "hidden_size = base.config.hidden_size\n",
    "two_head = TwoHead(hidden_size, K).to(model.device)\n",
    "\n",
    "# Simple pooled features: mean of last K tokens (tail window)\n",
    "def pooled_features(outputs, attention_mask, tail=96):\n",
    "    hs = outputs.hidden_states[-1]     # [B,T,H]\n",
    "    B, T, H = hs.shape\n",
    "    valid_lens = attention_mask.sum(dim=1)  # [B]\n",
    "    feats = []\n",
    "    for b in range(B):\n",
    "        L = int(valid_lens[b].item())\n",
    "        s = max(0, L - tail); e = L\n",
    "        if e <= s: s, e = max(0, L-32), L\n",
    "        feats.append(hs[b, s:e, :].mean(dim=0))\n",
    "    feats = torch.stack(feats, dim=0)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer with multitask losses (KL + optional consistency)\n",
    "class MTTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.two_head = kwargs.pop(\"two_head\")\n",
    "        self.lambda_A = kwargs.pop(\"lambda_A\", 1.0)\n",
    "        self.lambda_B = kwargs.pop(\"lambda_B\", 1.0)\n",
    "        self.lambda_C = kwargs.pop(\"lambda_C\", 0.5)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: Optional[int] = None,  # <-- accept the kwarg\n",
    "    ):\n",
    "        input_ids      = inputs[\"input_ids\"].to(model.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "        to_dist        = inputs[\"to_dist\"].to(model.device)     # [B,K]\n",
    "        weight         = inputs[\"weight\"].to(model.device)      # [B]\n",
    "        has_cons       = inputs[\"has_consistency\"].to(model.device)  # [B]\n",
    "        p_curr         = inputs[\"p_curr\"].to(model.device)      # [B,K]\n",
    "        p_next         = inputs[\"p_next\"].to(model.device)      # [B,K]\n",
    "        task_types     = inputs[\"task_types\"]                   # list[str]\n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        feats = pooled_features(out, attention_mask, tail=96).to(model.device)  # [B,H]\n",
    "\n",
    "        logits_row, logits_margin = self.two_head(feats)   # [B,K], [B,K]\n",
    "        p_row_hat    = F.softmax(logits_row, dim=1)\n",
    "        p_margin_hat = F.softmax(logits_margin, dim=1)\n",
    "\n",
    "        # Build masks by task\n",
    "        is_row    = torch.tensor([t == \"row\"    for t in task_types], device=model.device, dtype=torch.bool)\n",
    "        is_margin = torch.tensor([t == \"margin\" for t in task_types], device=model.device, dtype=torch.bool)\n",
    "\n",
    "        eps = 1e-8\n",
    "        def fwd_kl(p, q):\n",
    "            p = p.clamp_min(eps); q = q.clamp_min(eps)\n",
    "            return (p * (p.log() - q.log())).sum(dim=1)\n",
    "\n",
    "        loss = torch.tensor(0.0, device=model.device)\n",
    "\n",
    "        # Task A: panel rows\n",
    "        if is_row.any():\n",
    "            L_A = fwd_kl(to_dist[is_row], p_row_hat[is_row])\n",
    "            loss = loss + self.lambda_A * (weight[is_row] * L_A).mean()\n",
    "\n",
    "        # Task B: margins\n",
    "        if is_margin.any():\n",
    "            L_B = fwd_kl(to_dist[is_margin], p_margin_hat[is_margin])\n",
    "            loss = loss + self.lambda_B * (weight[is_margin] * L_B).mean()\n",
    "\n",
    "        # Consistency (disabled unless you wire full-row assembly):\n",
    "        # cons_mask = is_row & (has_cons==1)\n",
    "        # if cons_mask.any() and self.lambda_C > 0:\n",
    "        #     ...\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, {\"logits_row\": logits_row, \"logits_margin\": logits_margin, \"labels\": to_dist}\n",
    "        else:\n",
    "            return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479cfdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "tokenizer.padding_side = \"left\"  # (helps with some backbones; optional)\n",
    "\n",
    "train_ds = MTJsonlDataset([TASKA_JSONL, TASKB_JSONL], tokenizer, max_len=768)\n",
    "# For a quick start you can split train/val here:\n",
    "val_frac = 0.1\n",
    "n_val = int(len(train_ds) * val_frac)\n",
    "indices = list(range(len(train_ds)))\n",
    "random.seed(0); random.shuffle(indices)\n",
    "val_idx  = set(indices[:n_val])\n",
    "train_idx= set(indices[n_val:])\n",
    "\n",
    "class SubsetDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, base, keep_idx):\n",
    "        self.base = base\n",
    "        self.keep = sorted(list(keep_idx))\n",
    "    def __len__(self): return len(self.keep)\n",
    "    def __getitem__(self, i): return self.base[self.keep[i]]\n",
    "\n",
    "ds_train = SubsetDS(train_ds, train_idx)\n",
    "ds_val   = SubsetDS(train_ds, val_idx)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUT_DIR, \"ckpt\"),\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    warmup_steps=300,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,  # keep our custom fields\n",
    ")\n",
    "\n",
    "trainer = MTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    data_collator=mt_collate,\n",
    "    two_head=two_head,\n",
    "    lambda_A=1.0,\n",
    "    lambda_B=1.0,\n",
    "    lambda_C=0.0,   # keep 0.0 for now; we’ll add strict consistency later\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter + heads\n",
    "save_dir = os.path.join(OUT_DIR, \"final_multitask_trust_g4\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "torch.save(two_head.state_dict(), os.path.join(save_dir, \"two_head_trust_g4.pt\"))\n",
    "print(\"Saved to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe06136",
   "metadata": {},
   "source": [
    "### Trust, Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f19d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_cs[(g,y)] -> np.array[K]\n",
    "p_cs = {}\n",
    "effN_cs = {}  # effective N for weighting samples in Task B\n",
    "for y in YEARS_CS:\n",
    "    sub = cs_trust[cs_trust[\"year\"]==y]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    for g_vals, df_g in sub.groupby(GROUP_COLS_3):\n",
    "        p = weighted_probs(df_g[\"att\"].tolist(), df_g[\"wt\"].tolist(), TRUST3)\n",
    "        if p is None:\n",
    "            continue\n",
    "        p_cs[(g_vals, y)] = p\n",
    "        effN_cs[(g_vals, y)] = float(df_g[\"wt\"].sum())\n",
    "\n",
    "# Build panel transitions C[(g,t,Δ)] from consecutive waves per yearid\n",
    "# Build transitions per (g, t, Δ)\n",
    "# C[(g,t,Δ)] -> KxK counts; Nfrom[(g,t,Δ)] -> row totals length K\n",
    "from collections import defaultdict\n",
    "\n",
    "def canon_index(cat):\n",
    "    return TRUST2ID.get(cat, None)\n",
    "\n",
    "C = defaultdict(lambda: np.zeros((K,K), dtype=float))\n",
    "Nfrom = defaultdict(lambda: np.zeros((K,), dtype=float))\n",
    "\n",
    "# Weight=1.0\n",
    "pl_trust[\"w\"] = 1.0\n",
    "\n",
    "# Consecutive transitions per id\n",
    "for (pid), df_id in pl_trust.groupby(\"yearid\"):\n",
    "    df_id = df_id.sort_values(\"year\")\n",
    "    # collapse duplicates per year if any\n",
    "    df_id = df_id.drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "    years = df_id[\"year\"].values.tolist()\n",
    "    atts  = df_id[\"att\"].values.tolist()\n",
    "    wgts  = df_id[\"w\"].values.astype(float).tolist()\n",
    "    gens  = df_id[\"generation\"].values.tolist()\n",
    "    gend  = df_id[\"gender\"].values.tolist()\n",
    "    race  = df_id[\"race\"].values.tolist()\n",
    "    #edu = df_id[\"edu_level\"].values.tolist()\n",
    "\n",
    "    # require consistent group labels across waves for this id (common in panels)\n",
    "    if not (len(set(gens))==1 and len(set(gend))==1 and len(set(race))==1):\n",
    "        # if you prefer, skip inconsistent cases\n",
    "        continue\n",
    "\n",
    "    g = (gens[0], gend[0], race[0])\n",
    "    for i in range(len(years)-1):\n",
    "        t, t1 = int(years[i]), int(years[i+1])\n",
    "        Δ = t1 - t\n",
    "        if Δ not in (2,4):  # we only have 2-yr and 4-yr gaps here\n",
    "            continue\n",
    "        ai = canon_index(atts[i])\n",
    "        aj = canon_index(atts[i+1])\n",
    "        if ai is None or aj is None:\n",
    "            continue\n",
    "        w = float(wgts[i])  # or min(wgts[i], wgts[i+1])\n",
    "        C[(g,t,Δ)][ai, aj] += w\n",
    "        Nfrom[(g,t,Δ)][ai] += w\n",
    "\n",
    "\n",
    "# JSONL builders: Task A (panel rows) and Task B (margins)\n",
    "def smooth_row(row_counts, alpha):\n",
    "    rc = np.array(row_counts, dtype=float) + alpha\n",
    "    s = rc.sum()\n",
    "    if s <= 0:\n",
    "        return np.ones_like(rc)/len(rc)\n",
    "    return rc / s\n",
    "\n",
    "def build_taskA_rows(C, Nfrom, p_cs, out_jsonl, alpha_small=0.05, alpha_big=0.25, n_thresh=20, cap_weight=100.0):\n",
    "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
    "    n_rows = 0\n",
    "    for (g,t,Δ), mat in C.items():\n",
    "        nrow = Nfrom[(g,t,Δ)]\n",
    "        for i in range(K):\n",
    "            alpha = alpha_small if nrow[i] >= n_thresh else alpha_big\n",
    "            tgt = smooth_row(mat[i,:], alpha=alpha).tolist()\n",
    "            # Build prompt\n",
    "            prompt = (\n",
    "                \"[Task: Predict transition row]\\n\"\n",
    "                f\"From: <Y{t}> → To: <Y{t+Δ}> <DT{Δ}>\\n\"\n",
    "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}\\n\"\n",
    "                f\"From option: {TRUST3[i]}\\n\"\n",
    "                \"Answer:\\n\"\n",
    "            )\n",
    "            rec = {\n",
    "                \"task\": \"row\",\n",
    "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2]},\n",
    "                \"year_t\": t,\n",
    "                \"year_t1\": t+Δ,\n",
    "                \"dt\": Δ,\n",
    "                \"from_bin\": TRUST3[i],\n",
    "                \"prompt_text\": prompt,\n",
    "                \"to_dist\": tgt,\n",
    "                \"weight\": float(min(nrow[i], cap_weight))\n",
    "            }\n",
    "            # Attach margins if both available (for consistency loss later)\n",
    "            if ((g,t) in p_cs) and ((g,t+Δ) in p_cs):\n",
    "                rec[\"p_curr\"] = p_cs[(g,t)].tolist()\n",
    "                rec[\"p_next\"] = p_cs[(g,t+Δ)].tolist()\n",
    "            out.write(json.dumps(rec) + \"\\n\")\n",
    "            n_rows += 1\n",
    "    out.close()\n",
    "    return n_rows\n",
    "\n",
    "def build_taskB_rows(p_cs, effN_cs, out_jsonl, lags=(2,4), cap_weight=500.0):\n",
    "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
    "    n_rows = 0\n",
    "    # group by g\n",
    "    by_g = {}\n",
    "    for (g,y) in p_cs.keys():\n",
    "        by_g.setdefault(g, []).append(y)\n",
    "    for g, years in by_g.items():\n",
    "        ys = sorted(years)\n",
    "        for y in ys:\n",
    "            # context from previous lags\n",
    "            ctx = []\n",
    "            for L in lags:\n",
    "                yprev = y - L\n",
    "                if (g, yprev) in p_cs:\n",
    "                    ctx.append((yprev, p_cs[(g, yprev)]))\n",
    "            if len(ctx) == 0:\n",
    "                continue\n",
    "            # Build prompt\n",
    "            ctx_parts = \" \".join([f\"<Y{yy}>[{','.join(f'{x:.4f}' for x in p)}]\" for (yy,p) in ctx])\n",
    "            prompt = (\n",
    "                \"[Task: Forecast next-wave margin]\\n\"\n",
    "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}\\n\"\n",
    "                f\"Context: {ctx_parts}\\n\"\n",
    "                f\"Predict: <Y{y}>\\n\"\n",
    "                \"Answer:\\n\"\n",
    "            )\n",
    "            w = float(min(effN_cs.get((g,y), 1.0), cap_weight))\n",
    "            rec = {\n",
    "                \"task\": \"margin\",\n",
    "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2]},\n",
    "                \"year\": y,\n",
    "                \"prompt_text\": prompt,\n",
    "                \"to_dist\": p_cs[(g,y)].tolist(),\n",
    "                \"weight\": w\n",
    "            }\n",
    "            out.write(json.dumps(rec) + \"\\n\")\n",
    "            n_rows += 1\n",
    "    out.close()\n",
    "    return n_rows\n",
    "\n",
    "TASKA_JSONL = os.path.join(OUT_DIR, \"taskA_panel_rows_trust_g3.jsonl\")\n",
    "TASKB_JSONL = os.path.join(OUT_DIR, \"taskB_margins_trust_g3.jsonl\")\n",
    "\n",
    "na = build_taskA_rows(C, Nfrom, p_cs, TASKA_JSONL)\n",
    "nb = build_taskB_rows(p_cs, effN_cs, TASKB_JSONL)\n",
    "print(f\"Wrote Task A rows: {na}\")\n",
    "print(f\"Wrote Task B rows: {nb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097054ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and Collator (multitask)\n",
    "class MTJsonlDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, jsonl_paths: List[str], tokenizer, max_len=768):\n",
    "        self.rows = []\n",
    "        for p in jsonl_paths:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.rows.extend([json.loads(x) for x in f])\n",
    "        random.shuffle(self.rows)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.rows[i]\n",
    "        enc = self.tok(r[\"prompt_text\"], return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "        out = {\n",
    "            \"task_type\": r[\"task\"],\n",
    "            \"input_ids\": enc[\"input_ids\"][0],\n",
    "            \"attention_mask\": enc[\"attention_mask\"][0],\n",
    "            \"to_dist\": torch.tensor(r[\"to_dist\"], dtype=torch.float),\n",
    "            \"weight\": torch.tensor(float(r.get(\"weight\", 1.0)), dtype=torch.float),\n",
    "        }\n",
    "        # add optional consistency fields\n",
    "        if r[\"task\"]==\"row\" and (\"p_curr\" in r) and (\"p_next\" in r):\n",
    "            out[\"p_curr\"] = torch.tensor(r[\"p_curr\"], dtype=torch.float)\n",
    "            out[\"p_next\"] = torch.tensor(r[\"p_next\"], dtype=torch.float)\n",
    "            out[\"has_consistency\"] = torch.tensor(1, dtype=torch.long)\n",
    "        else:\n",
    "            out[\"has_consistency\"] = torch.tensor(0, dtype=torch.long)\n",
    "        return out\n",
    "\n",
    "# Model: LLM backbone + two small heads + last-K pooling\n",
    "# Tokenizer & backbone\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def mt_collate(batch):\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    maxlen = max(x[\"input_ids\"].shape[0] for x in batch)\n",
    "\n",
    "    def pad(seq, pad_val, target_len):\n",
    "        pad_n = target_len - seq.shape[0]\n",
    "        if pad_n <= 0: return seq\n",
    "        return torch.cat([seq, torch.full((pad_n,), pad_val, dtype=seq.dtype)])\n",
    "\n",
    "    input_ids      = torch.stack([pad(x[\"input_ids\"], pad_id, maxlen) for x in batch])\n",
    "    attention_mask = torch.stack([pad(x[\"attention_mask\"], 0, maxlen) for x in batch])\n",
    "    to_dist        = torch.stack([x[\"to_dist\"] for x in batch])\n",
    "    weight         = torch.stack([x[\"weight\"] for x in batch])\n",
    "    has_cons       = torch.stack([x[\"has_consistency\"] for x in batch])\n",
    "    # p_curr/p_next if present; else zeros\n",
    "    p_curr = torch.zeros(len(batch), K, dtype=torch.float)\n",
    "    p_next = torch.zeros(len(batch), K, dtype=torch.float)\n",
    "    for i, x in enumerate(batch):\n",
    "        if x[\"has_consistency\"]==1:\n",
    "            p_curr[i] = x[\"p_curr\"]\n",
    "            p_next[i] = x[\"p_next\"]\n",
    "\n",
    "    task_types = [x[\"task_type\"] for x in batch]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"to_dist\": to_dist,\n",
    "        \"weight\": weight,\n",
    "        \"has_consistency\": has_cons,\n",
    "        \"p_curr\": p_curr,\n",
    "        \"p_next\": p_next,\n",
    "        \"task_types\": task_types,\n",
    "    }\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "# Add LoRA to attention/MLP projections\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "# Two task heads\n",
    "class TwoHead(nn.Module):\n",
    "    def __init__(self, hidden_size, K):\n",
    "        super().__init__()\n",
    "        self.head_row    = nn.Linear(hidden_size, K)   # Task A\n",
    "        self.head_margin = nn.Linear(hidden_size, K)   # Task B\n",
    "\n",
    "    def forward(self, feats):\n",
    "        return self.head_row(feats), self.head_margin(feats)\n",
    "\n",
    "hidden_size = base.config.hidden_size\n",
    "two_head = TwoHead(hidden_size, K).to(model.device)\n",
    "\n",
    "# Simple pooled features: mean of last K tokens (tail window)\n",
    "def pooled_features(outputs, attention_mask, tail=96):\n",
    "    hs = outputs.hidden_states[-1]     # [B,T,H]\n",
    "    B, T, H = hs.shape\n",
    "    valid_lens = attention_mask.sum(dim=1)  # [B]\n",
    "    feats = []\n",
    "    for b in range(B):\n",
    "        L = int(valid_lens[b].item())\n",
    "        s = max(0, L - tail); e = L\n",
    "        if e <= s: s, e = max(0, L-32), L\n",
    "        feats.append(hs[b, s:e, :].mean(dim=0))\n",
    "    feats = torch.stack(feats, dim=0)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer with multitask losses (KL + optional consistency)\n",
    "class MTTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.two_head = kwargs.pop(\"two_head\")\n",
    "        self.lambda_A = kwargs.pop(\"lambda_A\", 1.0)\n",
    "        self.lambda_B = kwargs.pop(\"lambda_B\", 1.0)\n",
    "        self.lambda_C = kwargs.pop(\"lambda_C\", 0.5)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: Optional[int] = None,  # <-- accept the kwarg\n",
    "    ):\n",
    "        input_ids      = inputs[\"input_ids\"].to(model.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "        to_dist        = inputs[\"to_dist\"].to(model.device)     # [B,K]\n",
    "        weight         = inputs[\"weight\"].to(model.device)      # [B]\n",
    "        has_cons       = inputs[\"has_consistency\"].to(model.device)  # [B]\n",
    "        p_curr         = inputs[\"p_curr\"].to(model.device)      # [B,K]\n",
    "        p_next         = inputs[\"p_next\"].to(model.device)      # [B,K]\n",
    "        task_types     = inputs[\"task_types\"]                   # list[str]\n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        feats = pooled_features(out, attention_mask, tail=96).to(model.device)  # [B,H]\n",
    "\n",
    "        logits_row, logits_margin = self.two_head(feats)   # [B,K], [B,K]\n",
    "        p_row_hat    = F.softmax(logits_row, dim=1)\n",
    "        p_margin_hat = F.softmax(logits_margin, dim=1)\n",
    "\n",
    "        # Build masks by task\n",
    "        is_row    = torch.tensor([t == \"row\"    for t in task_types], device=model.device, dtype=torch.bool)\n",
    "        is_margin = torch.tensor([t == \"margin\" for t in task_types], device=model.device, dtype=torch.bool)\n",
    "\n",
    "        eps = 1e-8\n",
    "        def fwd_kl(p, q):\n",
    "            p = p.clamp_min(eps); q = q.clamp_min(eps)\n",
    "            return (p * (p.log() - q.log())).sum(dim=1)\n",
    "\n",
    "        loss = torch.tensor(0.0, device=model.device)\n",
    "\n",
    "        # Task A: panel rows\n",
    "        if is_row.any():\n",
    "            L_A = fwd_kl(to_dist[is_row], p_row_hat[is_row])\n",
    "            loss = loss + self.lambda_A * (weight[is_row] * L_A).mean()\n",
    "\n",
    "        # Task B: margins\n",
    "        if is_margin.any():\n",
    "            L_B = fwd_kl(to_dist[is_margin], p_margin_hat[is_margin])\n",
    "            loss = loss + self.lambda_B * (weight[is_margin] * L_B).mean()\n",
    "\n",
    "        # Consistency (disabled unless you wire full-row assembly):\n",
    "        # cons_mask = is_row & (has_cons==1)\n",
    "        # if cons_mask.any() and self.lambda_C > 0:\n",
    "        #     ...\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, {\"logits_row\": logits_row, \"logits_margin\": logits_margin, \"labels\": to_dist}\n",
    "        else:\n",
    "            return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e42c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "tokenizer.padding_side = \"left\"  # (helps with some backbones; optional)\n",
    "\n",
    "train_ds = MTJsonlDataset([TASKA_JSONL, TASKB_JSONL], tokenizer, max_len=768)\n",
    "# For a quick start you can split train/val here:\n",
    "val_frac = 0.1\n",
    "n_val = int(len(train_ds) * val_frac)\n",
    "indices = list(range(len(train_ds)))\n",
    "random.seed(0); random.shuffle(indices)\n",
    "val_idx  = set(indices[:n_val])\n",
    "train_idx= set(indices[n_val:])\n",
    "\n",
    "class SubsetDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, base, keep_idx):\n",
    "        self.base = base\n",
    "        self.keep = sorted(list(keep_idx))\n",
    "    def __len__(self): return len(self.keep)\n",
    "    def __getitem__(self, i): return self.base[self.keep[i]]\n",
    "\n",
    "ds_train = SubsetDS(train_ds, train_idx)\n",
    "ds_val   = SubsetDS(train_ds, val_idx)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUT_DIR, \"ckpt\"),\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    warmup_steps=300,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,  # keep our custom fields\n",
    ")\n",
    "\n",
    "trainer = MTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    data_collator=mt_collate,\n",
    "    two_head=two_head,\n",
    "    lambda_A=1.0,\n",
    "    lambda_B=1.0,\n",
    "    lambda_C=0.0,   # keep 0.0 for now; we’ll add strict consistency later\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter + heads\n",
    "save_dir = os.path.join(OUT_DIR, \"final_multitask_trust_g3\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "torch.save(two_head.state_dict(), os.path.join(save_dir, \"two_head_trust_g3.pt\"))\n",
    "print(\"Saved to:\", save_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
