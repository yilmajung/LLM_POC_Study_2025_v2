{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vXewi0cdj3c-",
        "outputId": "54c66b06-94d4-49fc-b58d-ceb60c5aacdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vXewi0cdj3c-",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qXCg_-7jvpJ",
        "outputId": "5c21d957-d94f-42fd-fa74-aa5e1cab6cd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Config\n",
        "CS_PATH = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_cs_full.csv\"\n",
        "PL_PATH = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_panel_full.csv\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_multitask_orig_subgroup\"\n",
        "\n",
        "CANON4 = [\"strong_anti\",\"anti\",\"pro\",\"strong_pro\"]; K = len(CANON4)\n",
        "CAT2ID = {c:i for i,c in enumerate(CANON4)}\n",
        "\n",
        "GROUP_SCHEMES = {\n",
        "    \"GROUP_COLS_1\": [\"gender\"],\n",
        "    \"GROUP_COLS_2\": [\"gender\",\"political_views\"],\n",
        "    \"GROUP_COLS_3\": [\"gender\",\"political_views\",\"edu_level\"],\n",
        "    \"GROUP_COLS_4\": [\"gender\",\"political_views\",\"edu_level\",\"generation\"],\n",
        "    \"GROUP_COLS_5\": [\"gender\",\"political_views\",\"edu_level\",\"generation\",\"race\"],\n",
        "    \"GROUP_COLS_ORIG\": [\"gender\",\"edu_level\",\"generation\",\"race\"],\n",
        "}\n",
        "CURRENT_GROUP_SCHEME = \"GROUP_COLS_ORIG\"  # <<< pick a scheme\n",
        "\n",
        "def get_group_cols(): return GROUP_SCHEMES[CURRENT_GROUP_SCHEME]\n",
        "print(\"Active grouping:\", get_group_cols())\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Active grouping: ['gender', 'edu_level', 'generation', 'race']\n"
          ]
        }
      ],
      "execution_count": 2,
      "id": "2qXCg_-7jvpJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmFf4rJ8jvpK"
      },
      "source": [
        "import os, json, numpy as np, pandas as pd\n",
        "from collections import defaultdict\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "id": "HmFf4rJ8jvpK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM3hsFNejvpL"
      },
      "source": [
        "def group_meta_from_tuple(g_tuple, cols=None):\n",
        "    if cols is None: cols = get_group_cols()\n",
        "    return {c:v for c,v in zip(cols, g_tuple)}\n",
        "\n",
        "def format_group_for_prompt(meta, cols=None):\n",
        "    import pandas as pd\n",
        "    if cols is None: cols = get_group_cols()\n",
        "    parts=[]\n",
        "    for c in cols:\n",
        "        v=meta.get(c,\"NA\")\n",
        "        if isinstance(v,float) and pd.isna(v): v=\"NA\"\n",
        "        parts.append(f\"{c}={v}\")\n",
        "    return \"Group: \" + \"; \".join(parts)\n",
        "\n",
        "def canon_index(x):\n",
        "    if isinstance(x,(int,float)) and not pd.isna(x):\n",
        "        mapping={1:\"strong_anti\",2:\"anti\",3:\"pro\",4:\"strong_pro\"}\n",
        "        return CAT2ID.get(mapping.get(int(x), None), None)\n",
        "    if isinstance(x,str):\n",
        "        x=x.strip().lower()\n",
        "        alias={\"strongly oppose\":\"strong_anti\",\"strong_anti\":\"strong_anti\",\"oppose\":\"anti\",\"anti\":\"anti\",\n",
        "               \"favor\":\"pro\",\"pro\":\"pro\",\"strongly favor\":\"strong_pro\",\"strong_pro\":\"strong_pro\"}\n",
        "        return CAT2ID.get(alias.get(x, None), None)\n",
        "    return None\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "id": "RM3hsFNejvpL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLqponW6jvpL",
        "outputId": "5ded3dbd-d078-4f83-fa6e-5e979e9ee857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load data\n",
        "cs = pd.read_csv(CS_PATH)\n",
        "pl = pd.read_csv(PL_PATH)\n",
        "for df in (cs, pl):\n",
        "    if \"id\" in df.columns: df.rename(columns={\"id\":\"yearid\"}, inplace=True)\n",
        "    df[\"year\"] = df[\"year\"].astype(int)\n",
        "\n",
        "cs[\"att_id\"] = cs[\"abortion_att4\"].apply(canon_index)\n",
        "pl[\"att_id\"] = pl[\"abortion_att4\"].apply(canon_index)\n",
        "cs = cs.dropna(subset=[\"att_id\"]).copy(); cs[\"att_id\"]=cs[\"att_id\"].astype(int)\n",
        "pl = pl.dropna(subset=[\"att_id\"]).copy(); pl[\"att_id\"]=pl[\"att_id\"].astype(int)\n",
        "print(\"CS rows:\", len(cs), \"PL rows:\", len(pl))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CS rows: 13351 PL rows: 12610\n"
          ]
        }
      ],
      "execution_count": 5,
      "id": "tLqponW6jvpL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0gfJAUxjvpM",
        "outputId": "b5ee7435-7785-4352-df96-f52f2a2185c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross-sectional margins (weighted)\n",
        "def weighted_margins(df, group_cols, w_col=\"wtssps\"):\n",
        "    out={}; effN={}\n",
        "    for (keys,gdf) in df.groupby(group_cols+[\"year\"], dropna=False):\n",
        "        *g_vals, y = keys\n",
        "        w = gdf[w_col].fillna(0.0).to_numpy(float)\n",
        "        k = gdf[\"att_id\"].to_numpy(int)\n",
        "        vec = np.zeros(K, dtype=float)\n",
        "        for ki, wi in zip(k, w):\n",
        "            if 0 <= ki < K: vec[ki] += wi\n",
        "        tot = vec.sum()\n",
        "        if tot <= 0: continue\n",
        "        out[(tuple(g_vals), int(y))] = vec/tot\n",
        "        sw = w.sum(); sw2 = (w**2).sum()\n",
        "        eff = (sw**2/sw2) if sw2>0 else len(w)\n",
        "        effN[(tuple(g_vals), int(y))] = float(eff)\n",
        "    return out, effN\n",
        "\n",
        "p_cs, effN_cs = weighted_margins(cs, get_group_cols())\n",
        "print(\"CS cells:\", len(p_cs))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CS cells: 821\n"
          ]
        }
      ],
      "execution_count": 6,
      "id": "H0gfJAUxjvpM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfXLPMyrjvpM",
        "outputId": "a5b27966-040b-48aa-be19-a324703b233c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Panel transitions with dynamic grouping\n",
        "INCONSISTENCY_STRATEGY = \"anchor_t\"  # \"strict\" | \"anchor_t\" | \"mode\"\n",
        "C = defaultdict(lambda: np.zeros((K,K), dtype=float))\n",
        "Nfrom = defaultdict(lambda: np.zeros((K,), dtype=float))\n",
        "\n",
        "pl2 = pl.sort_values([\"yearid\",\"year\"]).drop_duplicates([\"yearid\",\"year\"], keep=\"last\").copy()\n",
        "pl2[\"w\"] = 1.0\n",
        "\n",
        "def group_tuple_at(df_id, t, cols):\n",
        "    row = df_id.loc[df_id[\"year\"]==t]\n",
        "    if row.empty: return None\n",
        "    row = row.iloc[-1]\n",
        "    return tuple(row[c] for c in cols)\n",
        "\n",
        "for pid, df_id in pl2.groupby(\"yearid\", sort=False):\n",
        "    df_id = df_id.sort_values(\"year\")\n",
        "    yrs = df_id[\"year\"].astype(int).tolist()\n",
        "    att = df_id[\"att_id\"].astype(int).tolist()\n",
        "    wg  = df_id[\"w\"].astype(float).tolist()\n",
        "    if len(yrs) < 2: continue\n",
        "    for i in range(len(yrs)-1):\n",
        "        t, t1 = yrs[i], yrs[i+1]; d = t1 - t\n",
        "        if d not in (2,4): continue\n",
        "        ai, aj = att[i], att[i+1]\n",
        "        if INCONSISTENCY_STRATEGY == \"anchor_t\":\n",
        "            g_tuple = group_tuple_at(df_id, t, get_group_cols())\n",
        "        elif INCONSISTENCY_STRATEGY == \"mode\":\n",
        "            g_tuple = tuple(df_id[c].mode(dropna=False).iloc[0] for c in get_group_cols())\n",
        "        else:\n",
        "            ok = all(len(set(df_id[c].tolist()))==1 for c in get_group_cols())\n",
        "            g_tuple = tuple(df_id.iloc[0][c] for c in get_group_cols()) if ok else None\n",
        "        if g_tuple is None: continue\n",
        "        w = float(wg[i])\n",
        "        C[(g_tuple, t, d)][ai, aj] += w\n",
        "        Nfrom[(g_tuple, t, d)][ai]  += w\n",
        "\n",
        "print(\"Panel cells:\", len(C))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Panel cells: 400\n"
          ]
        }
      ],
      "execution_count": 7,
      "id": "yfXLPMyrjvpM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cPFYUhMjvpN",
        "outputId": "c7ec0388-7669-4a3c-c9f5-8ee95927e7fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Write training JSONL\n",
        "import json, os\n",
        "TA_PATH = os.path.join(OUT_DIR, f\"taskA_panel_rows_{CURRENT_GROUP_SCHEME}.jsonl\")\n",
        "TB_PATH = os.path.join(OUT_DIR, f\"taskB_cs_margins_{CURRENT_GROUP_SCHEME}.jsonl\")\n",
        "\n",
        "def write_taskA(C,Nfrom,p_cs,out_path,alpha=0.25):\n",
        "    n=0\n",
        "    with open(out_path,\"w\",encoding=\"utf-8\") as f:\n",
        "        for (g_tuple,t,d), mat in C.items():\n",
        "            rowsums=Nfrom[(g_tuple,t,d)]\n",
        "            meta = group_meta_from_tuple(g_tuple)\n",
        "            for k_from in range(K):\n",
        "                if rowsums[k_from] <= 0: continue\n",
        "                row = mat[k_from,:].astype(float)\n",
        "                vec = (row + alpha) / max(row.sum() + alpha*K, 1e-12)\n",
        "                prompt = (\n",
        "                    \"[Task: Predict transition row]\\n\"\n",
        "                    f\"From: <Y{t}> → To: <Y{t+d}> <DT{d}>\\n\"\n",
        "                    f\"{format_group_for_prompt(meta)}\\n\"\n",
        "                    f\"From option: {CANON4[k_from]}\\n\"\n",
        "                    \"Answer:\\n\"\n",
        "                )\n",
        "                rec = {\"task\":\"row\",\"group\":meta,\"year_t\":int(t),\"year_t1\":int(t+d),\"dt\":int(d),\n",
        "                       \"from_bin\": CANON4[k_from],\n",
        "                       \"prompt_text\": prompt, \"to_dist\": vec.tolist(), \"weight\": float(rowsums[k_from])}\n",
        "                p_curr = p_cs.get((g_tuple, int(t)))\n",
        "                p_next = p_cs.get((g_tuple, int(t+d)))\n",
        "                if p_curr is not None and p_next is not None:\n",
        "                    rec[\"p_curr\"] = [float(x) for x in p_curr]\n",
        "                    rec[\"p_next\"] = [float(x) for x in p_next]\n",
        "                f.write(json.dumps(rec)+\"\\n\"); n+=1\n",
        "    return n\n",
        "\n",
        "def write_taskB(p_cs, effN_cs, out_path, max_ctx=3):\n",
        "    n=0\n",
        "    by_group={}\n",
        "    for (g,y), _ in p_cs.items(): by_group.setdefault(g, []).append(int(y))\n",
        "    with open(out_path,\"w\",encoding=\"utf-8\") as f:\n",
        "        for g, ys in by_group.items():\n",
        "            ys = sorted(ys)\n",
        "            for i in range(1, len(ys)):\n",
        "                target = ys[i]; prevs = ys[max(0, i-max_ctx):i]\n",
        "                ctx_pairs = [(yy, p_cs[(g, yy)]) for yy in prevs if (g,yy) in p_cs]\n",
        "                if not ctx_pairs: continue\n",
        "                meta = group_meta_from_tuple(g)\n",
        "                ctx_str = \" \".join([f\"<Y{yy}>[\" + \",\".join(f\"{x:.4f}\" for x in vec) + \"]\" for yy, vec in ctx_pairs])\n",
        "                prompt = (\n",
        "                    \"[Task: Forecast next-wave margin]\\n\"\n",
        "                    f\"{format_group_for_prompt(meta)}\\n\"\n",
        "                    f\"Context: {ctx_str}\\n\"\n",
        "                    f\"Predict: <Y{target}>\\n\"\n",
        "                    \"Answer:\\n\"\n",
        "                )\n",
        "                rec = {\"task\":\"margin\",\"group\":meta,\"year\":int(target),\n",
        "                       \"prompt_text\":prompt,\"to_dist\":[float(x) for x in p_cs[(g,target)]],\n",
        "                       \"weight\": float(min(effN_cs.get((g,target), 200.0), 1000.0))}\n",
        "                f.write(json.dumps(rec)+\"\\n\"); n+=1\n",
        "    return n\n",
        "\n",
        "nA = write_taskA(C,Nfrom,p_cs,TA_PATH); nB = write_taskB(p_cs,effN_cs,TB_PATH)\n",
        "print(\"Wrote TaskA:\", nA, \"TaskB:\", nB)\n",
        "print(\"Paths:\", TA_PATH, TB_PATH)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote TaskA: 1062 TaskB: 675\n",
            "Paths: /content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_multitask_orig_subgroup/taskA_panel_rows_GROUP_COLS_ORIG.jsonl /content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_multitask_orig_subgroup/taskB_cs_margins_GROUP_COLS_ORIG.jsonl\n"
          ]
        }
      ],
      "execution_count": 8,
      "id": "5cPFYUhMjvpN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGU8TNcajvpU",
        "outputId": "919c713a-6c75-4544-9f1f-44b6aaa7606b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip -q install transformers peft accelerate bitsandbytes\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 9,
      "id": "zGU8TNcajvpU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMJdaACBjvpU",
        "outputId": "cf3a2786-b7e9-40fa-b6fb-c137ad02208c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347,
          "referenced_widgets": [
            "94df949aa0c144378279166682065bdf",
            "f7b615d8503b474085a6173ad69c4b7b",
            "c343c4390fde4971a52138fe3a663d2c",
            "0413a3a9c5ac4092aee4472ce2c3926c",
            "62620a00b07a434c8cd7cad8cb69d38f",
            "fa4c26bea52f4dc2bb09698ef692276a",
            "2514d4df3860456090e1dc68958d9ebf",
            "6835bf68afaa4233b81b7041a05f97c5",
            "e86e64492eaa4111ae23834148acb024",
            "a5d02f9feba041cca712c0d82c93eaae",
            "5035752d33c64628bf9fd40d37106a46"
          ]
        }
      },
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "MODEL_NAME = \"meta-llama/llama-3.1-8b\"\n",
        "USE_BF16 = torch.cuda.is_available()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16 if USE_BF16 else torch.float16, device_map=\"auto\")\n",
        "hidden_size = base.config.hidden_size\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    base.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "class TwoHead(nn.Module):\n",
        "    def __init__(self,H,K):\n",
        "        super().__init__(); self.trans=nn.Linear(H,K); self.margin=nn.Linear(H,K)\n",
        "    def forward(self,z): return self.trans(z), self.margin(z)\n",
        "\n",
        "two_head = TwoHead(hidden_size, K).to(next(base.parameters()).device)\n",
        "\n",
        "lora_cfg = LoraConfig(r=16,lora_alpha=32,lora_dropout=0.05,\n",
        "                      target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\"])\n",
        "model = get_peft_model(base, lora_cfg); model.train()\n",
        "\n",
        "def span_indices(text, tokenizer):\n",
        "    gpos = text.find(\"Group:\")\n",
        "    if gpos < 0:\n",
        "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "        L = int(enc[\"input_ids\"].shape[1]); return 0, max(1, L-1)\n",
        "    end = text.find(\"\\n\", gpos); end = len(text) if end < 0 else end\n",
        "    enc_pre = tokenizer(text[:gpos], return_tensors=\"pt\")\n",
        "    enc_end = tokenizer(text[:end],  return_tensors=\"pt\")\n",
        "    return int(enc_pre[\"input_ids\"].shape[1]), int(enc_end[\"input_ids\"].shape[1])\n",
        "\n",
        "class JsonlDistDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, paths, tokenizer, max_len=1024):\n",
        "        self.rows=[]\n",
        "        for p in (paths if isinstance(paths,(list,tuple)) else [paths]):\n",
        "            self.rows.extend([json.loads(x) for x in open(p,\"r\",encoding=\"utf-8\")])\n",
        "        self.tok=tokenizer; self.max_len=max_len\n",
        "    def __len__(self): return len(self.rows)\n",
        "    def __getitem__(self,i):\n",
        "        r=self.rows[i]\n",
        "        enc=self.tok(r[\"prompt_text\"], return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
        "        # pool over Group: line\n",
        "        gpos=r[\"prompt_text\"].find(\"Group:\")\n",
        "        if gpos<0:\n",
        "            s,e=0,int(enc[\"attention_mask\"].sum().item())-1\n",
        "        else:\n",
        "            end=r[\"prompt_text\"].find(\"\\n\", gpos); end=len(r[\"prompt_text\"]) if end<0 else end\n",
        "            s = self.tok(r[\"prompt_text\"][:gpos], return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
        "            e = self.tok(r[\"prompt_text\"][:end],  return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
        "        return {\"input_ids\":enc[\"input_ids\"][0], \"attention_mask\":enc[\"attention_mask\"][0],\n",
        "                \"span\":torch.tensor([s,e]), \"to_dist\":torch.tensor(r[\"to_dist\"],dtype=torch.float32),\n",
        "                \"weight\":torch.tensor(r.get(\"weight\",1.0),dtype=torch.float32),\n",
        "                \"task\":0 if r[\"task\"]==\"row\" else 1}\n",
        "\n",
        "def collate(batch):\n",
        "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "    maxT = max(x[\"input_ids\"].shape[0] for x in batch)\n",
        "    input_ids=[]; attn=[]; spans=[]; y=[]; w=[]; task=[]\n",
        "    for x in batch:\n",
        "        pad=maxT-x[\"input_ids\"].shape[0]\n",
        "        input_ids.append(torch.cat([x[\"input_ids\"], torch.full((pad,), pad_id)]))\n",
        "        attn.append(torch.cat([x[\"attention_mask\"], torch.zeros(pad, dtype=torch.long)]))\n",
        "        spans.append(x[\"span\"]); y.append(x[\"to_dist\"]); w.append(x[\"weight\"]); task.append(x[\"task\"])\n",
        "    return {\"input_ids\":torch.stack(input_ids),\"attention_mask\":torch.stack(attn),\n",
        "            \"span\":torch.stack(spans),\"to_dist\":torch.stack(y),\"weight\":torch.stack(w),\n",
        "            \"task\":torch.tensor(task)}\n",
        "\n",
        "def js_divergence(p,q,eps=1e-12):\n",
        "    p=torch.clamp(p,min=eps); q=torch.clamp(q,min=eps)\n",
        "    p=p/p.sum(dim=1,keepdim=True); q=q/q.sum(dim=1,keepdim=True)\n",
        "    m=0.5*(p+q);\n",
        "    return 0.5*torch.sum(p*(torch.log(p)-torch.log(m)),dim=1) + 0.5*torch.sum(q*(torch.log(q)-torch.log(m)),dim=1)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "class MTTrainer(Trainer):\n",
        "    def __init__(self, *args, two_head=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.two_head = two_head\n",
        "\n",
        "    # Accept any future kwargs (e.g., num_items_in_batch)\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "        attn      = inputs[\"attention_mask\"].to(model.device)\n",
        "        span      = inputs[\"span\"].to(model.device)\n",
        "        y         = inputs[\"to_dist\"].to(model.device)\n",
        "        w         = inputs[\"weight\"].to(model.device)\n",
        "        task      = inputs[\"task\"].to(model.device)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            out = model(input_ids=input_ids, attention_mask=attn, output_hidden_states=True)\n",
        "            hs = out.hidden_states[-1]  # [B, T, H]\n",
        "            B, T, H = hs.shape\n",
        "\n",
        "            feats = []\n",
        "            for b in range(B):\n",
        "                s, e = int(span[b, 0]), int(span[b, 1])\n",
        "                s = max(0, min(s, T - 1))\n",
        "                e = max(s + 1, min(e, T))\n",
        "                feats.append(hs[b, s:e, :].mean(dim=0))\n",
        "            z = torch.stack(feats, dim=0)\n",
        "\n",
        "            lr, lm = self.two_head(z)\n",
        "            p_row  = torch.softmax(lr, dim=1)\n",
        "            p_mrg  = torch.softmax(lm, dim=1)\n",
        "            p_pred = torch.where(task.unsqueeze(1) == 0, p_row, p_mrg)\n",
        "\n",
        "            loss_vec = js_divergence(y, p_pred)\n",
        "            loss = torch.mean(w * loss_vec)\n",
        "\n",
        "        return (loss, out) if return_outputs else loss\n",
        "\n",
        "\n",
        "train_ds = JsonlDistDataset([TA_PATH, TB_PATH], tokenizer)\n",
        "args = TrainingArguments(output_dir=os.path.join(OUT_DIR,f\"runs_{CURRENT_GROUP_SCHEME}\"),\n",
        "                         per_device_train_batch_size=8, gradient_accumulation_steps=2,\n",
        "                         learning_rate=1e-4, num_train_epochs=3,\n",
        "                         logging_steps=50, save_steps=400, save_total_limit=2, bf16=USE_BF16, report_to=\"none\",\n",
        "                         remove_unused_columns=False)\n",
        "trainer = MTTrainer(model=model, args=args, train_dataset=train_ds, data_collator=collate, two_head=two_head)\n",
        "trainer.train()\n",
        "\n",
        "SAVE_DIR = os.path.join(OUT_DIR, f\"final_{CURRENT_GROUP_SCHEME}\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "model.save_pretrained(os.path.join(SAVE_DIR,\"lora\"))\n",
        "tokenizer.save_pretrained(os.path.join(SAVE_DIR,\"lora\"))\n",
        "torch.save(two_head.state_dict(), os.path.join(SAVE_DIR,\"two_head.pt\"))\n",
        "print(\"Saved:\", SAVE_DIR)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94df949aa0c144378279166682065bdf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-924478407.py:96: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='327' max='327' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [327/327 03:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.636700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.579900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.555000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.530600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.553400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.508200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_multitask_orig_subgroup/final_GROUP_COLS_ORIG\n"
          ]
        }
      ],
      "execution_count": 12,
      "id": "KMJdaACBjvpU"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYEBxE0ilR-V"
      },
      "id": "wYEBxE0ilR-V",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94df949aa0c144378279166682065bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7b615d8503b474085a6173ad69c4b7b",
              "IPY_MODEL_c343c4390fde4971a52138fe3a663d2c",
              "IPY_MODEL_0413a3a9c5ac4092aee4472ce2c3926c"
            ],
            "layout": "IPY_MODEL_62620a00b07a434c8cd7cad8cb69d38f"
          }
        },
        "f7b615d8503b474085a6173ad69c4b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa4c26bea52f4dc2bb09698ef692276a",
            "placeholder": "​",
            "style": "IPY_MODEL_2514d4df3860456090e1dc68958d9ebf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c343c4390fde4971a52138fe3a663d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6835bf68afaa4233b81b7041a05f97c5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e86e64492eaa4111ae23834148acb024",
            "value": 4
          }
        },
        "0413a3a9c5ac4092aee4472ce2c3926c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5d02f9feba041cca712c0d82c93eaae",
            "placeholder": "​",
            "style": "IPY_MODEL_5035752d33c64628bf9fd40d37106a46",
            "value": " 4/4 [00:04&lt;00:00,  1.06s/it]"
          }
        },
        "62620a00b07a434c8cd7cad8cb69d38f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4c26bea52f4dc2bb09698ef692276a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2514d4df3860456090e1dc68958d9ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6835bf68afaa4233b81b7041a05f97c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e86e64492eaa4111ae23834148acb024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5d02f9feba041cca712c0d82c93eaae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5035752d33c64628bf9fd40d37106a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}