{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n2_finetune_llms_multi.ipynb\n",
    "Fine-tuning the T-LLM with dynamic subgroup schemes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Config ---\n",
    "CS_PATH = \"data/GSS/gss_abt_cs_full.csv\"\n",
    "PL_PATH = \"data/GSS/gss_abt_panel_full.csv\"\n",
    "OUT_DIR = \"outputs_gss_multitask\"\n",
    "\n",
    "CANON4 = [\"strong_anti\",\"anti\",\"pro\",\"strong_pro\"]; K = len(CANON4)\n",
    "CAT2ID = {c:i for i,c in enumerate(CANON4)}\n",
    "\n",
    "GROUP_SCHEMES = {\n",
    "    \"GROUP_COLS_1\": [\"gender\"],\n",
    "    \"GROUP_COLS_2\": [\"gender\",\"political_views\"],\n",
    "    \"GROUP_COLS_3\": [\"gender\",\"political_views\",\"edu_level\"],\n",
    "    \"GROUP_COLS_4\": [\"gender\",\"political_views\",\"edu_level\",\"generation\"],\n",
    "    \"GROUP_COLS_5\": [\"gender\",\"political_views\",\"edu_level\",\"generation\",\"race\"],\n",
    "}\n",
    "CURRENT_GROUP_SCHEME = \"GROUP_COLS_3\"  # <<< pick a scheme\n",
    "\n",
    "def get_group_cols(): return GROUP_SCHEMES[CURRENT_GROUP_SCHEME]\n",
    "print(\"Active grouping:\", get_group_cols())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, json, numpy as np, pandas as pd\n",
    "from collections import defaultdict\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def group_meta_from_tuple(g_tuple, cols=None):\n",
    "    if cols is None: cols = get_group_cols()\n",
    "    return {c:v for c,v in zip(cols, g_tuple)}\n",
    "\n",
    "def format_group_for_prompt(meta, cols=None):\n",
    "    import pandas as pd\n",
    "    if cols is None: cols = get_group_cols()\n",
    "    parts=[]\n",
    "    for c in cols:\n",
    "        v=meta.get(c,\"NA\")\n",
    "        if isinstance(v,float) and pd.isna(v): v=\"NA\"\n",
    "        parts.append(f\"{c}={v}\")\n",
    "    return \"Group: \" + \"; \".join(parts)\n",
    "\n",
    "def canon_index(x):\n",
    "    if isinstance(x,(int,float)) and not pd.isna(x):\n",
    "        mapping={1:\"strong_anti\",2:\"anti\",3:\"pro\",4:\"strong_pro\"}\n",
    "        return CAT2ID.get(mapping.get(int(x), None), None)\n",
    "    if isinstance(x,str):\n",
    "        x=x.strip().lower()\n",
    "        alias={\"strongly oppose\":\"strong_anti\",\"strong_anti\":\"strong_anti\",\"oppose\":\"anti\",\"anti\":\"anti\",\n",
    "               \"favor\":\"pro\",\"pro\":\"pro\",\"strongly favor\":\"strong_pro\",\"strong_pro\":\"strong_pro\"}\n",
    "        return CAT2ID.get(alias.get(x, None), None)\n",
    "    return None\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load data\n",
    "cs = pd.read_csv(CS_PATH)\n",
    "pl = pd.read_csv(PL_PATH)\n",
    "for df in (cs, pl):\n",
    "    if \"id\" in df.columns: df.rename(columns={\"id\":\"yearid\"}, inplace=True)\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "cs[\"att_id\"] = cs[\"abortion_att4\"].apply(canon_index)\n",
    "pl[\"att_id\"] = pl[\"abortion_att4\"].apply(canon_index)\n",
    "cs = cs.dropna(subset=[\"att_id\"]).copy(); cs[\"att_id\"]=cs[\"att_id\"].astype(int)\n",
    "pl = pl.dropna(subset=[\"att_id\"]).copy(); pl[\"att_id\"]=pl[\"att_id\"].astype(int)\n",
    "print(\"CS rows:\", len(cs), \"PL rows:\", len(pl))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cross-sectional margins (weighted)\n",
    "def weighted_margins(df, group_cols, w_col=\"wtssps\"):\n",
    "    out={}; effN={}\n",
    "    for (keys,gdf) in df.groupby(group_cols+[\"year\"], dropna=False):\n",
    "        *g_vals, y = keys\n",
    "        w = gdf[w_col].fillna(0.0).to_numpy(float)\n",
    "        k = gdf[\"att_id\"].to_numpy(int)\n",
    "        vec = np.zeros(K, dtype=float)\n",
    "        for ki, wi in zip(k, w):\n",
    "            if 0 <= ki < K: vec[ki] += wi\n",
    "        tot = vec.sum()\n",
    "        if tot <= 0: continue\n",
    "        out[(tuple(g_vals), int(y))] = vec/tot\n",
    "        sw = w.sum(); sw2 = (w**2).sum()\n",
    "        eff = (sw**2/sw2) if sw2>0 else len(w)\n",
    "        effN[(tuple(g_vals), int(y))] = float(eff)\n",
    "    return out, effN\n",
    "\n",
    "p_cs, effN_cs = weighted_margins(cs, get_group_cols())\n",
    "print(\"CS cells:\", len(p_cs))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Panel transitions with dynamic grouping\n",
    "INCONSISTENCY_STRATEGY = \"anchor_t\"  # \"strict\" | \"anchor_t\" | \"mode\"\n",
    "C = defaultdict(lambda: np.zeros((K,K), dtype=float))\n",
    "Nfrom = defaultdict(lambda: np.zeros((K,), dtype=float))\n",
    "\n",
    "pl2 = pl.sort_values([\"yearid\",\"year\"]).drop_duplicates([\"yearid\",\"year\"], keep=\"last\").copy()\n",
    "pl2[\"w\"] = 1.0\n",
    "\n",
    "def group_tuple_at(df_id, t, cols):\n",
    "    row = df_id.loc[df_id[\"year\"]==t]\n",
    "    if row.empty: return None\n",
    "    row = row.iloc[-1]\n",
    "    return tuple(row[c] for c in cols)\n",
    "\n",
    "for pid, df_id in pl2.groupby(\"yearid\", sort=False):\n",
    "    df_id = df_id.sort_values(\"year\")\n",
    "    yrs = df_id[\"year\"].astype(int).tolist()\n",
    "    att = df_id[\"att_id\"].astype(int).tolist()\n",
    "    wg  = df_id[\"w\"].astype(float).tolist()\n",
    "    if len(yrs) < 2: continue\n",
    "    for i in range(len(yrs)-1):\n",
    "        t, t1 = yrs[i], yrs[i+1]; d = t1 - t\n",
    "        if d not in (2,4): continue\n",
    "        ai, aj = att[i], att[i+1]\n",
    "        if INCONSISTENCY_STRATEGY == \"anchor_t\":\n",
    "            g_tuple = group_tuple_at(df_id, t, get_group_cols())\n",
    "        elif INCONSISTENCY_STRATEGY == \"mode\":\n",
    "            g_tuple = tuple(df_id[c].mode(dropna=False).iloc[0] for c in get_group_cols())\n",
    "        else:\n",
    "            ok = all(len(set(df_id[c].tolist()))==1 for c in get_group_cols())\n",
    "            g_tuple = tuple(df_id.iloc[0][c] for c in get_group_cols()) if ok else None\n",
    "        if g_tuple is None: continue\n",
    "        w = float(wg[i])\n",
    "        C[(g_tuple, t, d)][ai, aj] += w\n",
    "        Nfrom[(g_tuple, t, d)][ai]  += w\n",
    "\n",
    "print(\"Panel cells:\", len(C))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write training JSONL\n",
    "import json, os\n",
    "TA_PATH = os.path.join(OUT_DIR, f\"taskA_panel_rows_{CURRENT_GROUP_SCHEME}.jsonl\")\n",
    "TB_PATH = os.path.join(OUT_DIR, f\"taskB_cs_margins_{CURRENT_GROUP_SCHEME}.jsonl\")\n",
    "\n",
    "def write_taskA(C,Nfrom,p_cs,out_path,alpha=0.25):\n",
    "    n=0\n",
    "    with open(out_path,\"w\",encoding=\"utf-8\") as f:\n",
    "        for (g_tuple,t,d), mat in C.items():\n",
    "            rowsums=Nfrom[(g_tuple,t,d)]\n",
    "            meta = group_meta_from_tuple(g_tuple)\n",
    "            for k_from in range(K):\n",
    "                if rowsums[k_from] <= 0: continue\n",
    "                row = mat[k_from,:].astype(float)\n",
    "                vec = (row + alpha) / max(row.sum() + alpha*K, 1e-12)\n",
    "                prompt = (\n",
    "                    \"[Task: Predict transition row]\\n\"\n",
    "                    f\"From: <Y{t}> \u2192 To: <Y{t+d}> <DT{d}>\\n\"\n",
    "                    f\"{format_group_for_prompt(meta)}\\n\"\n",
    "                    f\"From option: {CANON4[k_from]}\\n\"\n",
    "                    \"Answer:\\n\"\n",
    "                )\n",
    "                rec = {\"task\":\"row\",\"group\":meta,\"year_t\":int(t),\"year_t1\":int(t+d),\"dt\":int(d),\n",
    "                       \"from_bin\": CANON4[k_from],\n",
    "                       \"prompt_text\": prompt, \"to_dist\": vec.tolist(), \"weight\": float(rowsums[k_from])}\n",
    "                p_curr = p_cs.get((g_tuple, int(t)))\n",
    "                p_next = p_cs.get((g_tuple, int(t+d)))\n",
    "                if p_curr is not None and p_next is not None:\n",
    "                    rec[\"p_curr\"] = [float(x) for x in p_curr]\n",
    "                    rec[\"p_next\"] = [float(x) for x in p_next]\n",
    "                f.write(json.dumps(rec)+\"\\n\"); n+=1\n",
    "    return n\n",
    "\n",
    "def write_taskB(p_cs, effN_cs, out_path, max_ctx=3):\n",
    "    n=0\n",
    "    by_group={}\n",
    "    for (g,y), _ in p_cs.items(): by_group.setdefault(g, []).append(int(y))\n",
    "    with open(out_path,\"w\",encoding=\"utf-8\") as f:\n",
    "        for g, ys in by_group.items():\n",
    "            ys = sorted(ys)\n",
    "            for i in range(1, len(ys)):\n",
    "                target = ys[i]; prevs = ys[max(0, i-max_ctx):i]\n",
    "                ctx_pairs = [(yy, p_cs[(g, yy)]) for yy in prevs if (g,yy) in p_cs]\n",
    "                if not ctx_pairs: continue\n",
    "                meta = group_meta_from_tuple(g)\n",
    "                ctx_str = \" \".join([f\"<Y{yy}>[\" + \",\".join(f\"{x:.4f}\" for x in vec) + \"]\" for yy, vec in ctx_pairs])\n",
    "                prompt = (\n",
    "                    \"[Task: Forecast next-wave margin]\\n\"\n",
    "                    f\"{format_group_for_prompt(meta)}\\n\"\n",
    "                    f\"Context: {ctx_str}\\n\"\n",
    "                    f\"Predict: <Y{target}>\\n\"\n",
    "                    \"Answer:\\n\"\n",
    "                )\n",
    "                rec = {\"task\":\"margin\",\"group\":meta,\"year\":int(target),\n",
    "                       \"prompt_text\":prompt,\"to_dist\":[float(x) for x in p_cs[(g,target)]],\n",
    "                       \"weight\": float(min(effN_cs.get((g,target), 200.0), 1000.0))}\n",
    "                f.write(json.dumps(rec)+\"\\n\"); n+=1\n",
    "    return n\n",
    "\n",
    "nA = write_taskA(C,Nfrom,p_cs,TA_PATH); nB = write_taskB(p_cs,effN_cs,TB_PATH)\n",
    "print(\"Wrote TaskA:\", nA, \"TaskB:\", nB)\n",
    "print(\"Paths:\", TA_PATH, TB_PATH)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip -q install transformers peft accelerate bitsandbytes\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "MODEL_NAME = \"meta-llama/llama-3.1-8b\"\n",
    "USE_BF16 = torch.cuda.is_available()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16 if USE_BF16 else torch.float16, device_map=\"auto\")\n",
    "hidden_size = base.config.hidden_size\n",
    "\n",
    "class TwoHead(nn.Module):\n",
    "    def __init__(self,H,K): \n",
    "        super().__init__(); self.trans=nn.Linear(H,K); self.margin=nn.Linear(H,K)\n",
    "    def forward(self,z): return self.trans(z), self.margin(z)\n",
    "\n",
    "two_head = TwoHead(hidden_size, K).to(next(base.parameters()).device)\n",
    "\n",
    "lora_cfg = LoraConfig(r=16,lora_alpha=32,lora_dropout=0.05,\n",
    "                      target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\"])\n",
    "model = get_peft_model(base, lora_cfg); model.train()\n",
    "\n",
    "def span_indices(text, tokenizer):\n",
    "    gpos = text.find(\"Group:\")\n",
    "    if gpos < 0:\n",
    "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        L = int(enc[\"input_ids\"].shape[1]); return 0, max(1, L-1)\n",
    "    end = text.find(\"\\n\", gpos); end = len(text) if end < 0 else end\n",
    "    enc_pre = tokenizer(text[:gpos], return_tensors=\"pt\")\n",
    "    enc_end = tokenizer(text[:end],  return_tensors=\"pt\")\n",
    "    return int(enc_pre[\"input_ids\"].shape[1]), int(enc_end[\"input_ids\"].shape[1])\n",
    "\n",
    "class JsonlDistDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths, tokenizer, max_len=1024):\n",
    "        self.rows=[]\n",
    "        for p in (paths if isinstance(paths,(list,tuple)) else [paths]):\n",
    "            self.rows.extend([json.loads(x) for x in open(p,\"r\",encoding=\"utf-8\")])\n",
    "        self.tok=tokenizer; self.max_len=max_len\n",
    "    def __len__(self): return len(self.rows)\n",
    "    def __getitem__(self,i):\n",
    "        r=self.rows[i]\n",
    "        enc=self.tok(r[\"prompt_text\"], return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "        # pool over Group: line\n",
    "        gpos=r[\"prompt_text\"].find(\"Group:\")\n",
    "        if gpos<0:\n",
    "            s,e=0,int(enc[\"attention_mask\"].sum().item())-1\n",
    "        else:\n",
    "            end=r[\"prompt_text\"].find(\"\\n\", gpos); end=len(r[\"prompt_text\"]) if end<0 else end\n",
    "            s = self.tok(r[\"prompt_text\"][:gpos], return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "            e = self.tok(r[\"prompt_text\"][:end],  return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "        return {\"input_ids\":enc[\"input_ids\"][0], \"attention_mask\":enc[\"attention_mask\"][0],\n",
    "                \"span\":torch.tensor([s,e]), \"to_dist\":torch.tensor(r[\"to_dist\"],dtype=torch.float32),\n",
    "                \"weight\":torch.tensor(r.get(\"weight\",1.0),dtype=torch.float32),\n",
    "                \"task\":0 if r[\"task\"]==\"row\" else 1}\n",
    "\n",
    "def collate(batch):\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    maxT = max(x[\"input_ids\"].shape[0] for x in batch)\n",
    "    input_ids=[]; attn=[]; spans=[]; y=[]; w=[]; task=[]\n",
    "    for x in batch:\n",
    "        pad=maxT-x[\"input_ids\"].shape[0]\n",
    "        input_ids.append(torch.cat([x[\"input_ids\"], torch.full((pad,), pad_id)]))\n",
    "        attn.append(torch.cat([x[\"attention_mask\"], torch.zeros(pad, dtype=torch.long)]))\n",
    "        spans.append(x[\"span\"]); y.append(x[\"to_dist\"]); w.append(x[\"weight\"]); task.append(x[\"task\"])\n",
    "    return {\"input_ids\":torch.stack(input_ids),\"attention_mask\":torch.stack(attn),\n",
    "            \"span\":torch.stack(spans),\"to_dist\":torch.stack(y),\"weight\":torch.stack(w),\n",
    "            \"task\":torch.tensor(task)}\n",
    "\n",
    "def js_divergence(p,q,eps=1e-12):\n",
    "    p=torch.clamp(p,min=eps); q=torch.clamp(q,min=eps)\n",
    "    p=p/p.sum(dim=1,keepdim=True); q=q/q.sum(dim=1,keepdim=True)\n",
    "    m=0.5*(p+q); \n",
    "    return 0.5*torch.sum(p*(torch.log(p)-torch.log(m)),dim=1) + 0.5*torch.sum(q*(torch.log(q)-torch.log(m)),dim=1)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "class MTTrainer(Trainer):\n",
    "    def __init__(self,*args,two_head=None,**kwargs):\n",
    "        super().__init__(*args,**kwargs); self.two_head=two_head\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids=inputs[\"input_ids\"].to(model.device); attn=inputs[\"attention_mask\"].to(model.device)\n",
    "        span=inputs[\"span\"].to(model.device); y=inputs[\"to_dist\"].to(model.device)\n",
    "        w=inputs[\"weight\"].to(model.device); task=inputs[\"task\"].to(model.device)\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            out=model(input_ids=input_ids, attention_mask=attn, output_hidden_states=True)\n",
    "            hs=out.hidden_states[-1]; B,T,H=hs.shape\n",
    "            feats=[]\n",
    "            for b in range(B):\n",
    "                s,e=int(span[b,0]), int(span[b,1]); s=max(0,min(s,T-1)); e=max(s+1,min(e,T))\n",
    "                feats.append(hs[b,s:e,:].mean(dim=0))\n",
    "            z=torch.stack(feats, dim=0)\n",
    "            lr,lm=self.two_head(z)\n",
    "            p_row=torch.softmax(lr,dim=1); p_mrg=torch.softmax(lm,dim=1)\n",
    "            p_pred=torch.where(task.unsqueeze(1)==0, p_row, p_mrg)\n",
    "            loss_vec=js_divergence(y,p_pred); loss=torch.mean(w*loss_vec)\n",
    "        return (loss,out) if return_outputs else loss\n",
    "\n",
    "train_ds = JsonlDistDataset([TA_PATH, TB_PATH], tokenizer)\n",
    "args = TrainingArguments(output_dir=os.path.join(OUT_DIR,f\"runs_{CURRENT_GROUP_SCHEME}\"),\n",
    "                         per_device_train_batch_size=8, gradient_accumulation_steps=2,\n",
    "                         learning_rate=1e-4, num_train_epochs=3,\n",
    "                         logging_steps=50, save_steps=400, save_total_limit=2, bf16=True, report_to=\"none\")\n",
    "trainer = MTTrainer(model=model, args=args, train_dataset=train_ds, data_collator=collate, two_head=two_head)\n",
    "trainer.train()\n",
    "\n",
    "SAVE_DIR = os.path.join(OUT_DIR, f\"final_{CURRENT_GROUP_SCHEME}\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "model.save_pretrained(os.path.join(SAVE_DIR,\"lora\"))\n",
    "tokenizer.save_pretrained(os.path.join(SAVE_DIR,\"lora\"))\n",
    "torch.save(two_head.state_dict(), os.path.join(SAVE_DIR,\"two_head.pt\"))\n",
    "print(\"Saved:\", SAVE_DIR)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}