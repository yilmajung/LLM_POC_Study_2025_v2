{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install & imports (Colab)\n",
    "!pip -q install peft transformers accelerate sentencepiece\n",
    "\n",
    "import os, json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
    "                          Trainer, TrainingArguments, PreTrainedModel)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from google.colab import userdata\n",
    "hf_token = userdata.get('HF_TOKEN', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f80789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# Config paths\n",
    "# ============\n",
    "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"   # or \"mistralai/Mistral-7B-v0.3\"\n",
    "TRAIN_JSONL = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/train_rows.jsonl\"\n",
    "VAL_JSONL   = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/val_rows.jsonl\"\n",
    "OUT_DIR     = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/tllm_fixA_head\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# =====================\n",
    "# Dataset & data collate\n",
    "# =====================\n",
    "class TLLMRowDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_len=1024):\n",
    "        self.rows = [json.loads(x) for x in open(jsonl_path, \"r\", encoding=\"utf-8\")]\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.rows)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.rows[i]\n",
    "        # EXACT same prompt_text you used in training\n",
    "        text = r[\"prompt_text\"]\n",
    "        enc = self.tok(text, return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"][0],\n",
    "            \"attention_mask\": enc[\"attention_mask\"][0],\n",
    "            \"to_dist\": torch.tensor(r[\"to_dist\"], dtype=torch.float),\n",
    "            \"weight\": torch.tensor(float(r.get(\"weight\", 1.0)), dtype=torch.float)\n",
    "        }\n",
    "\n",
    "def data_collate(batch):\n",
    "    # simple pad to max len\n",
    "    maxlen = max(x[\"input_ids\"].shape[0] for x in batch)\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    input_ids, attn, targets, weights = [], [], [], []\n",
    "    for x in batch:\n",
    "        pad = maxlen - x[\"input_ids\"].shape[0]\n",
    "        input_ids.append(torch.cat([x[\"input_ids\"], torch.full((pad,), pad_id)]))\n",
    "        attn.append(torch.cat([x[\"attention_mask\"], torch.zeros(pad, dtype=torch.long)]))\n",
    "        targets.append(x[\"to_dist\"])\n",
    "        weights.append(x[\"weight\"])\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(input_ids),\n",
    "        \"attention_mask\": torch.stack(attn),\n",
    "        \"to_dist\": torch.stack(targets),\n",
    "        \"weight\": torch.stack(weights),\n",
    "    }\n",
    "\n",
    "# =======================\n",
    "# Load tokenizer & base LM\n",
    "# =======================\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True, token=hf_token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_token,\n",
    "    # ensure hidden states are returned\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "# =========\n",
    "# Add LoRA (optional but helpful)\n",
    "# =========\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "# ==========================\n",
    "# Small 5-way classification head\n",
    "# ==========================\n",
    "class DistHead(nn.Module):\n",
    "    def __init__(self, hidden_size, K=5):\n",
    "        super().__init__()\n",
    "        self.out = nn.Linear(hidden_size, K)\n",
    "    def forward(self, last_hidden_vec):  # [B, H]\n",
    "        return self.out(last_hidden_vec) # [B, K]\n",
    "\n",
    "# attach the head to the model\n",
    "hidden_size = base.config.hidden_size\n",
    "model.dist_head = DistHead(hidden_size, K=5).to(model.device)\n",
    "\n",
    "# ==========================\n",
    "# Custom Trainer with JS loss\n",
    "# ==========================\n",
    "class HeadTrainer(Trainer):\n",
    "    def compute_loss(self, model: PreTrainedModel, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "        to_dist = inputs[\"to_dist\"].to(model.device)        # [B,5]\n",
    "        weight  = inputs[\"weight\"].to(model.device)         # [B]\n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "            # last token index\n",
    "            last_idx = attention_mask.sum(dim=1) - 1\n",
    "            # last hidden state from last layer: out.hidden_states[-1] shape [B, T, H]\n",
    "            last_vec = out.hidden_states[-1][torch.arange(out.hidden_states[-1].size(0)), last_idx]  # [B, H]\n",
    "            logits5 = model.dist_head(last_vec)         # [B,5]\n",
    "            p_llm = F.softmax(logits5, dim=1)           # [B,5]\n",
    "\n",
    "        # normalize targets\n",
    "        p_h = to_dist / (to_dist.sum(dim=1, keepdim=True) + 1e-12)\n",
    "        p_h = p_h.clamp_min(1e-8)\n",
    "\n",
    "        # Jensen-Shannon divergence\n",
    "        m = 0.5*(p_h + p_llm)\n",
    "        kl1 = (p_h * (p_h.add(1e-12).log() - m.add(1e-12).log())).sum(dim=1)\n",
    "        kl2 = (p_llm * (p_llm.add(1e-12).log() - m.add(1e-12).log())).sum(dim=1)\n",
    "        loss_vec = 0.5*(kl1 + kl2)\n",
    "\n",
    "        loss = (weight * loss_vec).mean()\n",
    "        return (loss, {\"logits5\": logits5}) if return_outputs else loss\n",
    "\n",
    "# =========\n",
    "# Datasets\n",
    "# =========\n",
    "train_ds = TLLMRowDataset(TRAIN_JSONL, tokenizer, max_len=1024)\n",
    "eval_ds  = TLLMRowDataset(VAL_JSONL,   tokenizer, max_len=1024)\n",
    "\n",
    "# =================\n",
    "# Training settings\n",
    "# =================\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    learning_rate=1e-4,                   # LoRA + small head\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=6,                   # give it a bit more runway\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    label_names=[\"to_dist\", \"weight\"]    # keep labels\n",
    ")\n",
    "\n",
    "# Ensure the headâ€™s parameters are trainable\n",
    "for n,p in model.dist_head.named_parameters():\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "trainer = HeadTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collate,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# =========================\n",
    "# Save adapter + head + tok\n",
    "# =========================\n",
    "model.save_pretrained(os.path.join(OUT_DIR, \"lora\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUT_DIR, \"lora\"))\n",
    "# Save the small head weights separately\n",
    "torch.save(model.dist_head.state_dict(), os.path.join(OUT_DIR, \"dist_head.pt\"))\n",
    "print(\"Saved LoRA + head to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591698b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e554c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
