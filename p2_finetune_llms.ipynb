{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yilmajung/LLM_POC_Study_2025_v2/blob/main/p2_finetune_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d3ab407b",
      "metadata": {
        "id": "d3ab407b"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers accelerate peft sentencepiece pandas pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "hBmUogRwruoZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBmUogRwruoZ",
        "outputId": "1e785589-cb3b-4c8b-b8df-1980054b20ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3eecf706",
      "metadata": {
        "id": "3eecf706"
      },
      "outputs": [],
      "source": [
        "import os, json, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "86667df6",
      "metadata": {
        "id": "86667df6"
      },
      "outputs": [],
      "source": [
        "# Set up paths\n",
        "CS_CSV    = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_cs_abortion.csv\"       # cross-sectional long\n",
        "PANEL_CSV = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_panel_abortion.csv\"    # panel long\n",
        "OUT_DIR   = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_p_trial\"          # where I write JSONLs & checkpoints\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# LLM choice\n",
        "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"  # or \"mistralai/Mistral-7B-v0.3\"\n",
        "\n",
        "# Canonical bins (K=4)\n",
        "ABORT4 = [\"strong_anti\", \"anti\", \"pro\", \"strong_pro\"]\n",
        "TRUST3 = [\"distrust\", \"depends\", \"trust\"]\n",
        "ENVIR3 = [\"too_little\", \"about_right\", \"too_much\"]\n",
        "ABORT2ID = {c:i for i,c in enumerate(ABORT4)}\n",
        "TRUST2ID = {c:i for i,c in enumerate(TRUST3)}\n",
        "ENVIR2ID = {c:i for i,c in enumerate(ENVIR3)}\n",
        "K = len(ABORT4)\n",
        "\n",
        "YEARS_CS = list(range(2006, 2025, 2))  # 2006..2024 every 2 years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cd2f93df",
      "metadata": {
        "id": "cd2f93df"
      },
      "outputs": [],
      "source": [
        "# Load and harmonize the data\n",
        "# --- Cross-sectional ---\n",
        "cs = pd.read_csv(CS_CSV)\n",
        "# Expect: yearid, year, abortion_att4, generation, race, gender, edu_level, wtssps\n",
        "# Map the attitude to canonical\n",
        "cs[\"att\"] = cs[\"abortion_att4\"].astype(str).str.strip()\n",
        "# keep only canon categories, drop NAs\n",
        "cs_abortion = cs[cs[\"att\"].isin(ABORT4)].copy()\n",
        "cs_abortion[\"wt\"] = cs_abortion.get(\"wtssps\", pd.Series([1.0]*len(cs_abortion)))  # default 1.0 if missing\n",
        "cs_trust = cs[cs[\"trust\"].astype(str).str.strip().isin(TRUST3)].copy()\n",
        "cs_trust['wt'] = cs_trust.get(\"wtssps\", pd.Series([1.0]*len(cs_trust)))  # default 1.0 if missing\n",
        "\n",
        "cs_envir = cs[cs[\"natenvir\"].astype(str).str.strip().isin(ENVIR3)].copy()\n",
        "cs_envir['wt'] = cs_envir.get(\"wtssps\", pd.Series([1.0]*len(cs_envir)))  # default 1.0 if missing\n",
        "\n",
        "# --- Panel ---\n",
        "pl = pd.read_csv(PANEL_CSV)\n",
        "pl[\"att\"] = pl[\"abortion_att4\"].astype(str).str.strip()\n",
        "pl_abortion = pl[pl[\"att\"].isin(ABORT4)].copy()\n",
        "pl_trust = pl[pl[\"trust\"].astype(str).str.strip().isin(TRUST3)].copy()\n",
        "pl_envir = pl[pl[\"natenvir\"].astype(str).str.strip().isin(ENVIR3)].copy()\n",
        "\n",
        "# Define grouping keys\n",
        "GROUP_COLS_4 = [\"generation\",\"gender\",\"race\",\"edu_level\"]\n",
        "GROUP_COLS_3 = [\"generation\",\"gender\",\"race\"]\n",
        "GROUP_COLS_1 = [\"generation\"]\n",
        "\n",
        "for df in (cs, pl):\n",
        "    for c in GROUP_COLS_4:\n",
        "        df[c] = df[c].astype(str).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "18d3111f",
      "metadata": {
        "id": "18d3111f"
      },
      "outputs": [],
      "source": [
        "# Build cross-section margins p_cs[g,y] (weighted)\n",
        "def group_key(row):\n",
        "    return (row[\"generation\"], row[\"gender\"], row[\"race\"], row['edu_level'])\n",
        "\n",
        "def weighted_probs(vals, wts, cats=ABORT4):\n",
        "    # vals: list of category strings; wts: weights\n",
        "    counts = {c:0.0 for c in cats}\n",
        "    for v, w in zip(vals, wts):\n",
        "        counts[v] += float(w)\n",
        "    vec = np.array([counts[c] for c in cats], dtype=float)\n",
        "    s = vec.sum()\n",
        "    if s <= 0: return None\n",
        "    return vec / s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2dd77d",
      "metadata": {
        "id": "aa2dd77d"
      },
      "source": [
        "## Abortion, Group 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "16dcb7d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16dcb7d7",
        "outputId": "ad60f124-0e5b-470a-f2cf-332e179fcdae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Task A rows: 1552\n",
            "Wrote Task B rows: 524\n"
          ]
        }
      ],
      "source": [
        "# p_cs[(g,y)] -> np.array[K]\n",
        "p_cs = {}\n",
        "effN_cs = {}  # effective N for weighting samples in Task B\n",
        "for y in YEARS_CS:\n",
        "    sub = cs_abortion[cs_abortion[\"year\"]==y]\n",
        "    if sub.empty:\n",
        "        continue\n",
        "    for g_vals, df_g in sub.groupby(GROUP_COLS_4):\n",
        "        p = weighted_probs(df_g[\"att\"].tolist(), df_g[\"wt\"].tolist(), ABORT4)\n",
        "        if p is None:\n",
        "            continue\n",
        "        p_cs[(g_vals, y)] = p\n",
        "        effN_cs[(g_vals, y)] = float(df_g[\"wt\"].sum())\n",
        "\n",
        "# Build panel transitions C[(g,t,Δ)] from consecutive waves per yearid\n",
        "# Build transitions per (g, t, Δ)\n",
        "# C[(g,t,Δ)] -> KxK counts; Nfrom[(g,t,Δ)] -> row totals length K\n",
        "from collections import defaultdict\n",
        "\n",
        "def canon_index(cat):\n",
        "    return ABORT2ID.get(cat, None)\n",
        "\n",
        "C = defaultdict(lambda: np.zeros((K,K), dtype=float))\n",
        "Nfrom = defaultdict(lambda: np.zeros((K,), dtype=float))\n",
        "\n",
        "# Weight=1.0\n",
        "pl_abortion[\"w\"] = 1.0\n",
        "\n",
        "# Consecutive transitions per id\n",
        "for (pid), df_id in pl_abortion.groupby(\"yearid\"):\n",
        "    df_id = df_id.sort_values(\"year\")\n",
        "    # collapse duplicates per year if any\n",
        "    df_id = df_id.drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
        "    years = df_id[\"year\"].values.tolist()\n",
        "    atts  = df_id[\"att\"].values.tolist()\n",
        "    wgts  = df_id[\"w\"].values.astype(float).tolist()\n",
        "    gens  = df_id[\"generation\"].values.tolist()\n",
        "    gend  = df_id[\"gender\"].values.tolist()\n",
        "    race  = df_id[\"race\"].values.tolist()\n",
        "    edu = df_id[\"edu_level\"].values.tolist()\n",
        "\n",
        "    # require consistent group labels across waves for this id (common in panels)\n",
        "    if not (len(set(gens))==1 and len(set(gend))==1 and len(set(race))==1 and len(set(edu))==1):\n",
        "        # if you prefer, skip inconsistent cases\n",
        "        continue\n",
        "\n",
        "    g = (gens[0], gend[0], race[0], edu[0])\n",
        "    for i in range(len(years)-1):\n",
        "        t, t1 = int(years[i]), int(years[i+1])\n",
        "        Δ = t1 - t\n",
        "        if Δ not in (2,4):  # we only have 2-yr and 4-yr gaps here\n",
        "            continue\n",
        "        ai = canon_index(atts[i])\n",
        "        aj = canon_index(atts[i+1])\n",
        "        if ai is None or aj is None:\n",
        "            continue\n",
        "        w = float(wgts[i])  # or min(wgts[i], wgts[i+1])\n",
        "        C[(g,t,Δ)][ai, aj] += w\n",
        "        Nfrom[(g,t,Δ)][ai] += w\n",
        "\n",
        "\n",
        "# JSONL builders: Task A (panel rows) and Task B (margins)\n",
        "def smooth_row(row_counts, alpha):\n",
        "    rc = np.array(row_counts, dtype=float) + alpha\n",
        "    s = rc.sum()\n",
        "    if s <= 0:\n",
        "        return np.ones_like(rc)/len(rc)\n",
        "    return rc / s\n",
        "\n",
        "def build_taskA_rows(C, Nfrom, p_cs, out_jsonl, alpha_small=0.05, alpha_big=0.25, n_thresh=20, cap_weight=100.0):\n",
        "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
        "    n_rows = 0\n",
        "    for (g,t,Δ), mat in C.items():\n",
        "        nrow = Nfrom[(g,t,Δ)]\n",
        "        for i in range(K):\n",
        "            alpha = alpha_small if nrow[i] >= n_thresh else alpha_big\n",
        "            tgt = smooth_row(mat[i,:], alpha=alpha).tolist()\n",
        "            # Build prompt\n",
        "            prompt = (\n",
        "                \"[Task: Predict transition row]\\n\"\n",
        "                f\"From: <Y{t}> → To: <Y{t+Δ}> <DT{Δ}>\\n\"\n",
        "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}; edu_level={g[3]}\\n\"\n",
        "                f\"From option: {ABORT4[i]}\\n\"\n",
        "                \"Answer:\\n\"\n",
        "            )\n",
        "            rec = {\n",
        "                \"task\": \"row\",\n",
        "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3]},\n",
        "                \"year_t\": t,\n",
        "                \"year_t1\": t+Δ,\n",
        "                \"dt\": Δ,\n",
        "                \"from_bin\": ABORT4[i],\n",
        "                \"prompt_text\": prompt,\n",
        "                \"to_dist\": tgt,\n",
        "                \"weight\": float(min(nrow[i], cap_weight))\n",
        "            }\n",
        "            # Attach margins if both available (for consistency loss later)\n",
        "            if ((g,t) in p_cs) and ((g,t+Δ) in p_cs):\n",
        "                rec[\"p_curr\"] = p_cs[(g,t)].tolist()\n",
        "                rec[\"p_next\"] = p_cs[(g,t+Δ)].tolist()\n",
        "            out.write(json.dumps(rec) + \"\\n\")\n",
        "            n_rows += 1\n",
        "    out.close()\n",
        "    return n_rows\n",
        "\n",
        "def build_taskB_rows(p_cs, effN_cs, out_jsonl, lags=(2,4), cap_weight=500.0):\n",
        "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
        "    n_rows = 0\n",
        "    # group by g\n",
        "    by_g = {}\n",
        "    for (g,y) in p_cs.keys():\n",
        "        by_g.setdefault(g, []).append(y)\n",
        "    for g, years in by_g.items():\n",
        "        ys = sorted(years)\n",
        "        for y in ys:\n",
        "            # context from previous lags\n",
        "            ctx = []\n",
        "            for L in lags:\n",
        "                yprev = y - L\n",
        "                if (g, yprev) in p_cs:\n",
        "                    ctx.append((yprev, p_cs[(g, yprev)]))\n",
        "            if len(ctx) == 0:\n",
        "                continue\n",
        "            # Build prompt\n",
        "            ctx_parts = \" \".join([f\"<Y{yy}>[{','.join(f'{x:.4f}' for x in p)}]\" for (yy,p) in ctx])\n",
        "            prompt = (\n",
        "                \"[Task: Forecast next-wave margin]\\n\"\n",
        "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}; edu_level={g[3]}\\n\"\n",
        "                f\"Context: {ctx_parts}\\n\"\n",
        "                f\"Predict: <Y{y}>\\n\"\n",
        "                \"Answer:\\n\"\n",
        "            )\n",
        "            w = float(min(effN_cs.get((g,y), 1.0), cap_weight))\n",
        "            rec = {\n",
        "                \"task\": \"margin\",\n",
        "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3]},\n",
        "                \"year\": y,\n",
        "                \"prompt_text\": prompt,\n",
        "                \"to_dist\": p_cs[(g,y)].tolist(),\n",
        "                \"weight\": w\n",
        "            }\n",
        "            out.write(json.dumps(rec) + \"\\n\")\n",
        "            n_rows += 1\n",
        "    out.close()\n",
        "    return n_rows\n",
        "\n",
        "TASKA_JSONL = os.path.join(OUT_DIR, \"taskA_panel_rows_abortion_g4.jsonl\")\n",
        "TASKB_JSONL = os.path.join(OUT_DIR, \"taskB_margins_abortion_g4.jsonl\")\n",
        "\n",
        "na = build_taskA_rows(C, Nfrom, p_cs, TASKA_JSONL)\n",
        "nb = build_taskB_rows(p_cs, effN_cs, TASKB_JSONL)\n",
        "print(f\"Wrote Task A rows: {na}\")\n",
        "print(f\"Wrote Task B rows: {nb}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ef8f7270",
      "metadata": {
        "id": "ef8f7270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "ce67622833e8464da402399263383961",
            "e5666b26ba9b46ac9d7f8ac3f3c1f540",
            "17f4ccd590d74d6594265cac71c2a277",
            "de8f3ca7031d40ec88e33e7b3aff3187",
            "a18a0b9a50714d6d974082b2f8e368a4",
            "2bd89daa8daa48fb9ebf125fa645dd7e",
            "9dfc5a5d4c254e2c926f8fa91cfb2b37",
            "c98a525868ce47d68d9d03599f14359e",
            "5029f6a50c8b4dca9905285f804ed47f",
            "e687903c518043248f10ad7a263bb2cb",
            "c9f8ac84ca7e447abc1837579f9eeb83"
          ]
        },
        "outputId": "c946051f-18bc-4df9-b40a-b687b1584561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce67622833e8464da402399263383961"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Datasets and Collator (multitask)\n",
        "class MTJsonlDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, jsonl_paths: List[str], tokenizer, max_len=768):\n",
        "        self.rows = []\n",
        "        for p in jsonl_paths:\n",
        "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "                self.rows.extend([json.loads(x) for x in f])\n",
        "        random.shuffle(self.rows)\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        enc = self.tok(r[\"prompt_text\"], return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
        "        out = {\n",
        "            \"task_type\": r[\"task\"],\n",
        "            \"input_ids\": enc[\"input_ids\"][0],\n",
        "            \"attention_mask\": enc[\"attention_mask\"][0],\n",
        "            \"to_dist\": torch.tensor(r[\"to_dist\"], dtype=torch.float),\n",
        "            \"weight\": torch.tensor(float(r.get(\"weight\", 1.0)), dtype=torch.float),\n",
        "        }\n",
        "        # add optional consistency fields\n",
        "        if r[\"task\"]==\"row\" and (\"p_curr\" in r) and (\"p_next\" in r):\n",
        "            out[\"p_curr\"] = torch.tensor(r[\"p_curr\"], dtype=torch.float)\n",
        "            out[\"p_next\"] = torch.tensor(r[\"p_next\"], dtype=torch.float)\n",
        "            out[\"has_consistency\"] = torch.tensor(1, dtype=torch.long)\n",
        "        else:\n",
        "            out[\"has_consistency\"] = torch.tensor(0, dtype=torch.long)\n",
        "        return out\n",
        "\n",
        "# Model: LLM backbone + two small heads + last-K pooling\n",
        "# Tokenizer & backbone\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def mt_collate(batch):\n",
        "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "    maxlen = max(x[\"input_ids\"].shape[0] for x in batch)\n",
        "\n",
        "    def pad(seq, pad_val, target_len):\n",
        "        pad_n = target_len - seq.shape[0]\n",
        "        if pad_n <= 0: return seq\n",
        "        return torch.cat([seq, torch.full((pad_n,), pad_val, dtype=seq.dtype)])\n",
        "\n",
        "    input_ids      = torch.stack([pad(x[\"input_ids\"], pad_id, maxlen) for x in batch])\n",
        "    attention_mask = torch.stack([pad(x[\"attention_mask\"], 0, maxlen) for x in batch])\n",
        "    to_dist        = torch.stack([x[\"to_dist\"] for x in batch])\n",
        "    weight         = torch.stack([x[\"weight\"] for x in batch])\n",
        "    has_cons       = torch.stack([x[\"has_consistency\"] for x in batch])\n",
        "    # p_curr/p_next if present; else zeros\n",
        "    p_curr = torch.zeros(len(batch), K, dtype=torch.float)\n",
        "    p_next = torch.zeros(len(batch), K, dtype=torch.float)\n",
        "    for i, x in enumerate(batch):\n",
        "        if x[\"has_consistency\"]==1:\n",
        "            p_curr[i] = x[\"p_curr\"]\n",
        "            p_next[i] = x[\"p_next\"]\n",
        "\n",
        "    task_types = [x[\"task_type\"] for x in batch]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"to_dist\": to_dist,\n",
        "        \"weight\": weight,\n",
        "        \"has_consistency\": has_cons,\n",
        "        \"p_curr\": p_curr,\n",
        "        \"p_next\": p_next,\n",
        "        \"task_types\": task_types,\n",
        "    }\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    output_hidden_states=True\n",
        ")\n",
        "\n",
        "# Add LoRA to attention/MLP projections\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\"]\n",
        ")\n",
        "model = get_peft_model(base, lora_cfg)\n",
        "\n",
        "# Two task heads\n",
        "class TwoHead(nn.Module):\n",
        "    def __init__(self, hidden_size, K):\n",
        "        super().__init__()\n",
        "        self.head_row    = nn.Linear(hidden_size, K)   # Task A\n",
        "        self.head_margin = nn.Linear(hidden_size, K)   # Task B\n",
        "\n",
        "    def forward(self, feats):\n",
        "        return self.head_row(feats), self.head_margin(feats)\n",
        "\n",
        "hidden_size = base.config.hidden_size\n",
        "two_head = TwoHead(hidden_size, K).to(model.device)\n",
        "\n",
        "# Simple pooled features: mean of last K tokens (tail window)\n",
        "def pooled_features(outputs, attention_mask, tail=96):\n",
        "    hs = outputs.hidden_states[-1]     # [B,T,H]\n",
        "    B, T, H = hs.shape\n",
        "    valid_lens = attention_mask.sum(dim=1)  # [B]\n",
        "    feats = []\n",
        "    for b in range(B):\n",
        "        L = int(valid_lens[b].item())\n",
        "        s = max(0, L - tail); e = L\n",
        "        if e <= s: s, e = max(0, L-32), L\n",
        "        feats.append(hs[b, s:e, :].mean(dim=0))\n",
        "    feats = torch.stack(feats, dim=0)\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b2c75c96",
      "metadata": {
        "id": "b2c75c96"
      },
      "outputs": [],
      "source": [
        "# Trainer with multitask losses (KL + optional consistency)\n",
        "class MTTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.two_head = kwargs.pop(\"two_head\")\n",
        "        self.lambda_A = kwargs.pop(\"lambda_A\", 1.0)\n",
        "        self.lambda_B = kwargs.pop(\"lambda_B\", 1.0)\n",
        "        self.lambda_C = kwargs.pop(\"lambda_C\", 0.5)\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        model,\n",
        "        inputs,\n",
        "        return_outputs: bool = False,\n",
        "        num_items_in_batch: Optional[int] = None,  # <-- accept the kwarg\n",
        "    ):\n",
        "        input_ids      = inputs[\"input_ids\"].to(model.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "        to_dist        = inputs[\"to_dist\"].to(model.device)     # [B,K]\n",
        "        weight         = inputs[\"weight\"].to(model.device)      # [B]\n",
        "        has_cons       = inputs[\"has_consistency\"].to(model.device)  # [B]\n",
        "        p_curr         = inputs[\"p_curr\"].to(model.device)      # [B,K]\n",
        "        p_next         = inputs[\"p_next\"].to(model.device)      # [B,K]\n",
        "        task_types     = inputs[\"task_types\"]                   # list[str]\n",
        "\n",
        "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "            out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        feats = pooled_features(out, attention_mask, tail=96).to(model.device)  # [B,H]\n",
        "\n",
        "        logits_row, logits_margin = self.two_head(feats)   # [B,K], [B,K]\n",
        "        p_row_hat    = F.softmax(logits_row, dim=1)\n",
        "        p_margin_hat = F.softmax(logits_margin, dim=1)\n",
        "\n",
        "        # Build masks by task\n",
        "        is_row    = torch.tensor([t == \"row\"    for t in task_types], device=model.device, dtype=torch.bool)\n",
        "        is_margin = torch.tensor([t == \"margin\" for t in task_types], device=model.device, dtype=torch.bool)\n",
        "\n",
        "        eps = 1e-8\n",
        "        def fwd_kl(p, q):\n",
        "            p = p.clamp_min(eps); q = q.clamp_min(eps)\n",
        "            return (p * (p.log() - q.log())).sum(dim=1)\n",
        "\n",
        "        loss = torch.tensor(0.0, device=model.device)\n",
        "\n",
        "        # Task A: panel rows\n",
        "        if is_row.any():\n",
        "            L_A = fwd_kl(to_dist[is_row], p_row_hat[is_row])\n",
        "            loss = loss + self.lambda_A * (weight[is_row] * L_A).mean()\n",
        "\n",
        "        # Task B: margins\n",
        "        if is_margin.any():\n",
        "            L_B = fwd_kl(to_dist[is_margin], p_margin_hat[is_margin])\n",
        "            loss = loss + self.lambda_B * (weight[is_margin] * L_B).mean()\n",
        "\n",
        "        # Consistency (disabled unless you wire full-row assembly):\n",
        "        # cons_mask = is_row & (has_cons==1)\n",
        "        # if cons_mask.any() and self.lambda_C > 0:\n",
        "        #     ...\n",
        "\n",
        "        if return_outputs:\n",
        "            return loss, {\"logits_row\": logits_row, \"logits_margin\": logits_margin, \"labels\": to_dist}\n",
        "        else:\n",
        "            return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4WWUJ0d7UoRg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "4WWUJ0d7UoRg",
        "outputId": "2049ca19-7797-4323-93f7-086bbc5d55a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1006' max='4680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1006/4680 05:29 < 20:07, 3.04 it/s, Epoch 4.29/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.864300</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.904200</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.171500</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.508300</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.786000</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        }
      ],
      "source": [
        "# Training setup\n",
        "tokenizer.padding_side = \"left\"  # (helps with some backbones; optional)\n",
        "\n",
        "train_ds = MTJsonlDataset([TASKA_JSONL, TASKB_JSONL], tokenizer, max_len=768)\n",
        "# For a quick start you can split train/val here:\n",
        "val_frac = 0.1\n",
        "n_val = int(len(train_ds) * val_frac)\n",
        "indices = list(range(len(train_ds)))\n",
        "random.seed(0); random.shuffle(indices)\n",
        "val_idx  = set(indices[:n_val])\n",
        "train_idx= set(indices[n_val:])\n",
        "\n",
        "class SubsetDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, base, keep_idx):\n",
        "        self.base = base\n",
        "        self.keep = sorted(list(keep_idx))\n",
        "    def __len__(self): return len(self.keep)\n",
        "    def __getitem__(self, i): return self.base[self.keep[i]]\n",
        "\n",
        "ds_train = SubsetDS(train_ds, train_idx)\n",
        "ds_val   = SubsetDS(train_ds, val_idx)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=os.path.join(OUT_DIR, \"ckpt\"),\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=20,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    warmup_steps=300,\n",
        "    weight_decay=0.0,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,  # keep our custom fields\n",
        ")\n",
        "\n",
        "trainer = MTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_val,\n",
        "    data_collator=mt_collate,\n",
        "    two_head=two_head,\n",
        "    lambda_A=1.0,\n",
        "    lambda_B=1.0,\n",
        "    lambda_C=0.0,   # keep 0.0 for now; we’ll add strict consistency later\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save adapter + heads\n",
        "save_dir = os.path.join(OUT_DIR, \"final_multitask_abortion_g4\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "torch.save(two_head.state_dict(), os.path.join(save_dir, \"two_head_abortion_g4.pt\"))\n",
        "print(\"Saved to:\", save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abortion, Group 3"
      ],
      "metadata": {
        "id": "G2ed4JNrrZH3"
      },
      "id": "G2ed4JNrrZH3"
    },
    {
      "cell_type": "code",
      "source": [
        "# p_cs[(g,y)] -> np.array[K]\n",
        "p_cs = {}\n",
        "effN_cs = {}  # effective N for weighting samples in Task B\n",
        "for y in YEARS_CS:\n",
        "    sub = cs_abortion[cs_abortion[\"year\"]==y]\n",
        "    if sub.empty:\n",
        "        continue\n",
        "    for g_vals, df_g in sub.groupby(GROUP_COLS_3):\n",
        "        p = weighted_probs(df_g[\"att\"].tolist(), df_g[\"wt\"].tolist(), ABORT4)\n",
        "        if p is None:\n",
        "            continue\n",
        "        p_cs[(g_vals, y)] = p\n",
        "        effN_cs[(g_vals, y)] = float(df_g[\"wt\"].sum())\n",
        "\n",
        "# Build panel transitions C[(g,t,Δ)] from consecutive waves per yearid\n",
        "# Build transitions per (g, t, Δ)\n",
        "# C[(g,t,Δ)] -> KxK counts; Nfrom[(g,t,Δ)] -> row totals length K\n",
        "from collections import defaultdict\n",
        "\n",
        "def canon_index(cat):\n",
        "    return ABORT2ID.get(cat, None)\n",
        "\n",
        "C = defaultdict(lambda: np.zeros((K,K), dtype=float))\n",
        "Nfrom = defaultdict(lambda: np.zeros((K,), dtype=float))\n",
        "\n",
        "# Weight=1.0\n",
        "pl_abortion[\"w\"] = 1.0\n",
        "\n",
        "# Consecutive transitions per id\n",
        "for (pid), df_id in pl_abortion.groupby(\"yearid\"):\n",
        "    df_id = df_id.sort_values(\"year\")\n",
        "    # collapse duplicates per year if any\n",
        "    df_id = df_id.drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
        "    years = df_id[\"year\"].values.tolist()\n",
        "    atts  = df_id[\"att\"].values.tolist()\n",
        "    wgts  = df_id[\"w\"].values.astype(float).tolist()\n",
        "    gens  = df_id[\"generation\"].values.tolist()\n",
        "    gend  = df_id[\"gender\"].values.tolist()\n",
        "    race  = df_id[\"race\"].values.tolist()\n",
        "    #edu = df_id[\"edu_level\"].values.tolist()\n",
        "\n",
        "    # require consistent group labels across waves for this id (common in panels)\n",
        "    if not (len(set(gens))==1 and len(set(gend))==1 and len(set(race))==1):\n",
        "        # if you prefer, skip inconsistent cases\n",
        "        continue\n",
        "\n",
        "    g = (gens[0], gend[0], race[0])\n",
        "    for i in range(len(years)-1):\n",
        "        t, t1 = int(years[i]), int(years[i+1])\n",
        "        Δ = t1 - t\n",
        "        if Δ not in (2,4):  # we only have 2-yr and 4-yr gaps here\n",
        "            continue\n",
        "        ai = canon_index(atts[i])\n",
        "        aj = canon_index(atts[i+1])\n",
        "        if ai is None or aj is None:\n",
        "            continue\n",
        "        w = float(wgts[i])  # or min(wgts[i], wgts[i+1])\n",
        "        C[(g,t,Δ)][ai, aj] += w\n",
        "        Nfrom[(g,t,Δ)][ai] += w\n",
        "\n",
        "\n",
        "# JSONL builders: Task A (panel rows) and Task B (margins)\n",
        "def smooth_row(row_counts, alpha):\n",
        "    rc = np.array(row_counts, dtype=float) + alpha\n",
        "    s = rc.sum()\n",
        "    if s <= 0:\n",
        "        return np.ones_like(rc)/len(rc)\n",
        "    return rc / s\n",
        "\n",
        "def build_taskA_rows(C, Nfrom, p_cs, out_jsonl, alpha_small=0.05, alpha_big=0.25, n_thresh=20, cap_weight=100.0):\n",
        "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
        "    n_rows = 0\n",
        "    for (g,t,Δ), mat in C.items():\n",
        "        nrow = Nfrom[(g,t,Δ)]\n",
        "        for i in range(K):\n",
        "            alpha = alpha_small if nrow[i] >= n_thresh else alpha_big\n",
        "            tgt = smooth_row(mat[i,:], alpha=alpha).tolist()\n",
        "            # Build prompt\n",
        "            prompt = (\n",
        "                \"[Task: Predict transition row]\\n\"\n",
        "                f\"From: <Y{t}> → To: <Y{t+Δ}> <DT{Δ}>\\n\"\n",
        "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}\\n\"\n",
        "                f\"From option: {ABORT4[i]}\\n\"\n",
        "                \"Answer:\\n\"\n",
        "            )\n",
        "            rec = {\n",
        "                \"task\": \"row\",\n",
        "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2]},\n",
        "                \"year_t\": t,\n",
        "                \"year_t1\": t+Δ,\n",
        "                \"dt\": Δ,\n",
        "                \"from_bin\": ABORT4[i],\n",
        "                \"prompt_text\": prompt,\n",
        "                \"to_dist\": tgt,\n",
        "                \"weight\": float(min(nrow[i], cap_weight))\n",
        "            }\n",
        "            # Attach margins if both available (for consistency loss later)\n",
        "            if ((g,t) in p_cs) and ((g,t+Δ) in p_cs):\n",
        "                rec[\"p_curr\"] = p_cs[(g,t)].tolist()\n",
        "                rec[\"p_next\"] = p_cs[(g,t+Δ)].tolist()\n",
        "            out.write(json.dumps(rec) + \"\\n\")\n",
        "            n_rows += 1\n",
        "    out.close()\n",
        "    return n_rows\n",
        "\n",
        "def build_taskB_rows(p_cs, effN_cs, out_jsonl, lags=(2,4), cap_weight=500.0):\n",
        "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
        "    n_rows = 0\n",
        "    # group by g\n",
        "    by_g = {}\n",
        "    for (g,y) in p_cs.keys():\n",
        "        by_g.setdefault(g, []).append(y)\n",
        "    for g, years in by_g.items():\n",
        "        ys = sorted(years)\n",
        "        for y in ys:\n",
        "            # context from previous lags\n",
        "            ctx = []\n",
        "            for L in lags:\n",
        "                yprev = y - L\n",
        "                if (g, yprev) in p_cs:\n",
        "                    ctx.append((yprev, p_cs[(g, yprev)]))\n",
        "            if len(ctx) == 0:\n",
        "                continue\n",
        "            # Build prompt\n",
        "            ctx_parts = \" \".join([f\"<Y{yy}>[{','.join(f'{x:.4f}' for x in p)}]\" for (yy,p) in ctx])\n",
        "            prompt = (\n",
        "                \"[Task: Forecast next-wave margin]\\n\"\n",
        "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}\\n\"\n",
        "                f\"Context: {ctx_parts}\\n\"\n",
        "                f\"Predict: <Y{y}>\\n\"\n",
        "                \"Answer:\\n\"\n",
        "            )\n",
        "            w = float(min(effN_cs.get((g,y), 1.0), cap_weight))\n",
        "            rec = {\n",
        "                \"task\": \"margin\",\n",
        "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2]},\n",
        "                \"year\": y,\n",
        "                \"prompt_text\": prompt,\n",
        "                \"to_dist\": p_cs[(g,y)].tolist(),\n",
        "                \"weight\": w\n",
        "            }\n",
        "            out.write(json.dumps(rec) + \"\\n\")\n",
        "            n_rows += 1\n",
        "    out.close()\n",
        "    return n_rows\n",
        "\n",
        "TASKA_JSONL = os.path.join(OUT_DIR, \"taskA_panel_rows_abortion_g3.jsonl\")\n",
        "TASKB_JSONL = os.path.join(OUT_DIR, \"taskB_margins_abortion_g3.jsonl\")\n",
        "\n",
        "na = build_taskA_rows(C, Nfrom, p_cs, TASKA_JSONL)\n",
        "nb = build_taskB_rows(p_cs, effN_cs, TASKB_JSONL)\n",
        "print(f\"Wrote Task A rows: {na}\")\n",
        "print(f\"Wrote Task B rows: {nb}\")"
      ],
      "metadata": {
        "id": "MSJvnAUGrZEB",
        "outputId": "dcc35be4-9f13-4d46-b487-8c572e894745",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MSJvnAUGrZEB",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote Task A rows: 664\n",
            "Wrote Task B rows: 207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasets and Collator (multitask)\n",
        "class MTJsonlDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, jsonl_paths: List[str], tokenizer, max_len=768):\n",
        "        self.rows = []\n",
        "        for p in jsonl_paths:\n",
        "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "                self.rows.extend([json.loads(x) for x in f])\n",
        "        random.shuffle(self.rows)\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        enc = self.tok(r[\"prompt_text\"], return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
        "        out = {\n",
        "            \"task_type\": r[\"task\"],\n",
        "            \"input_ids\": enc[\"input_ids\"][0],\n",
        "            \"attention_mask\": enc[\"attention_mask\"][0],\n",
        "            \"to_dist\": torch.tensor(r[\"to_dist\"], dtype=torch.float),\n",
        "            \"weight\": torch.tensor(float(r.get(\"weight\", 1.0)), dtype=torch.float),\n",
        "        }\n",
        "        # add optional consistency fields\n",
        "        if r[\"task\"]==\"row\" and (\"p_curr\" in r) and (\"p_next\" in r):\n",
        "            out[\"p_curr\"] = torch.tensor(r[\"p_curr\"], dtype=torch.float)\n",
        "            out[\"p_next\"] = torch.tensor(r[\"p_next\"], dtype=torch.float)\n",
        "            out[\"has_consistency\"] = torch.tensor(1, dtype=torch.long)\n",
        "        else:\n",
        "            out[\"has_consistency\"] = torch.tensor(0, dtype=torch.long)\n",
        "        return out\n",
        "\n",
        "# Model: LLM backbone + two small heads + last-K pooling\n",
        "# Tokenizer & backbone\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def mt_collate(batch):\n",
        "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "    maxlen = max(x[\"input_ids\"].shape[0] for x in batch)\n",
        "\n",
        "    def pad(seq, pad_val, target_len):\n",
        "        pad_n = target_len - seq.shape[0]\n",
        "        if pad_n <= 0: return seq\n",
        "        return torch.cat([seq, torch.full((pad_n,), pad_val, dtype=seq.dtype)])\n",
        "\n",
        "    input_ids      = torch.stack([pad(x[\"input_ids\"], pad_id, maxlen) for x in batch])\n",
        "    attention_mask = torch.stack([pad(x[\"attention_mask\"], 0, maxlen) for x in batch])\n",
        "    to_dist        = torch.stack([x[\"to_dist\"] for x in batch])\n",
        "    weight         = torch.stack([x[\"weight\"] for x in batch])\n",
        "    has_cons       = torch.stack([x[\"has_consistency\"] for x in batch])\n",
        "    # p_curr/p_next if present; else zeros\n",
        "    p_curr = torch.zeros(len(batch), K, dtype=torch.float)\n",
        "    p_next = torch.zeros(len(batch), K, dtype=torch.float)\n",
        "    for i, x in enumerate(batch):\n",
        "        if x[\"has_consistency\"]==1:\n",
        "            p_curr[i] = x[\"p_curr\"]\n",
        "            p_next[i] = x[\"p_next\"]\n",
        "\n",
        "    task_types = [x[\"task_type\"] for x in batch]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"to_dist\": to_dist,\n",
        "        \"weight\": weight,\n",
        "        \"has_consistency\": has_cons,\n",
        "        \"p_curr\": p_curr,\n",
        "        \"p_next\": p_next,\n",
        "        \"task_types\": task_types,\n",
        "    }\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    output_hidden_states=True\n",
        ")\n",
        "\n",
        "# Add LoRA to attention/MLP projections\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\"]\n",
        ")\n",
        "model = get_peft_model(base, lora_cfg)\n",
        "\n",
        "# Two task heads\n",
        "class TwoHead(nn.Module):\n",
        "    def __init__(self, hidden_size, K):\n",
        "        super().__init__()\n",
        "        self.head_row    = nn.Linear(hidden_size, K)   # Task A\n",
        "        self.head_margin = nn.Linear(hidden_size, K)   # Task B\n",
        "\n",
        "    def forward(self, feats):\n",
        "        return self.head_row(feats), self.head_margin(feats)\n",
        "\n",
        "hidden_size = base.config.hidden_size\n",
        "two_head = TwoHead(hidden_size, K).to(model.device)\n",
        "\n",
        "# Simple pooled features: mean of last K tokens (tail window)\n",
        "def pooled_features(outputs, attention_mask, tail=96):\n",
        "    hs = outputs.hidden_states[-1]     # [B,T,H]\n",
        "    B, T, H = hs.shape\n",
        "    valid_lens = attention_mask.sum(dim=1)  # [B]\n",
        "    feats = []\n",
        "    for b in range(B):\n",
        "        L = int(valid_lens[b].item())\n",
        "        s = max(0, L - tail); e = L\n",
        "        if e <= s: s, e = max(0, L-32), L\n",
        "        feats.append(hs[b, s:e, :].mean(dim=0))\n",
        "    feats = torch.stack(feats, dim=0)\n",
        "    return feats"
      ],
      "metadata": {
        "id": "pACabDyArZBF",
        "outputId": "55fbef47-d5cd-4455-f9af-e1bc06a5a208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "c88a444935be46d3880a733cd439649c",
            "d65e44172fef408081f26c38eb10f2fa",
            "623c4059eba84c558f909ccfc7334a89",
            "adb23c01510d48e1acf2b0bb5e900be8",
            "786c9924367b475fa21711636bb799cd",
            "b748a94c1c1c4befb8a03931240e0b14",
            "fb9e3d39a69b45cca57f1b65c4a121a7",
            "9a621a511007401b8eb1f3bcd8ab2b03",
            "549e2f69c5ca4c99b79d6d3330e4ea16",
            "8fc202fd044c4e0aaa36099f891f7312",
            "5e1aabae6fd1421ab679fa16345a2050"
          ]
        }
      },
      "id": "pACabDyArZBF",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c88a444935be46d3880a733cd439649c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer with multitask losses (KL + optional consistency)\n",
        "class MTTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.two_head = kwargs.pop(\"two_head\")\n",
        "        self.lambda_A = kwargs.pop(\"lambda_A\", 1.0)\n",
        "        self.lambda_B = kwargs.pop(\"lambda_B\", 1.0)\n",
        "        self.lambda_C = kwargs.pop(\"lambda_C\", 0.5)\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        model,\n",
        "        inputs,\n",
        "        return_outputs: bool = False,\n",
        "        num_items_in_batch: Optional[int] = None,  # <-- accept the kwarg\n",
        "    ):\n",
        "        input_ids      = inputs[\"input_ids\"].to(model.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "        to_dist        = inputs[\"to_dist\"].to(model.device)     # [B,K]\n",
        "        weight         = inputs[\"weight\"].to(model.device)      # [B]\n",
        "        has_cons       = inputs[\"has_consistency\"].to(model.device)  # [B]\n",
        "        p_curr         = inputs[\"p_curr\"].to(model.device)      # [B,K]\n",
        "        p_next         = inputs[\"p_next\"].to(model.device)      # [B,K]\n",
        "        task_types     = inputs[\"task_types\"]                   # list[str]\n",
        "\n",
        "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "            out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        feats = pooled_features(out, attention_mask, tail=96).to(model.device)  # [B,H]\n",
        "\n",
        "        logits_row, logits_margin = self.two_head(feats)   # [B,K], [B,K]\n",
        "        p_row_hat    = F.softmax(logits_row, dim=1)\n",
        "        p_margin_hat = F.softmax(logits_margin, dim=1)\n",
        "\n",
        "        # Build masks by task\n",
        "        is_row    = torch.tensor([t == \"row\"    for t in task_types], device=model.device, dtype=torch.bool)\n",
        "        is_margin = torch.tensor([t == \"margin\" for t in task_types], device=model.device, dtype=torch.bool)\n",
        "\n",
        "        eps = 1e-8\n",
        "        def fwd_kl(p, q):\n",
        "            p = p.clamp_min(eps); q = q.clamp_min(eps)\n",
        "            return (p * (p.log() - q.log())).sum(dim=1)\n",
        "\n",
        "        loss = torch.tensor(0.0, device=model.device)\n",
        "\n",
        "        # Task A: panel rows\n",
        "        if is_row.any():\n",
        "            L_A = fwd_kl(to_dist[is_row], p_row_hat[is_row])\n",
        "            loss = loss + self.lambda_A * (weight[is_row] * L_A).mean()\n",
        "\n",
        "        # Task B: margins\n",
        "        if is_margin.any():\n",
        "            L_B = fwd_kl(to_dist[is_margin], p_margin_hat[is_margin])\n",
        "            loss = loss + self.lambda_B * (weight[is_margin] * L_B).mean()\n",
        "\n",
        "        # Consistency (disabled unless you wire full-row assembly):\n",
        "        # cons_mask = is_row & (has_cons==1)\n",
        "        # if cons_mask.any() and self.lambda_C > 0:\n",
        "        #     ...\n",
        "\n",
        "        if return_outputs:\n",
        "            return loss, {\"logits_row\": logits_row, \"logits_margin\": logits_margin, \"labels\": to_dist}\n",
        "        else:\n",
        "            return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "regj-48SrY9u"
      },
      "id": "regj-48SrY9u",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training setup\n",
        "tokenizer.padding_side = \"left\"  # (helps with some backbones; optional)\n",
        "\n",
        "train_ds = MTJsonlDataset([TASKA_JSONL, TASKB_JSONL], tokenizer, max_len=768)\n",
        "# For a quick start you can split train/val here:\n",
        "val_frac = 0.1\n",
        "n_val = int(len(train_ds) * val_frac)\n",
        "indices = list(range(len(train_ds)))\n",
        "random.seed(0); random.shuffle(indices)\n",
        "val_idx  = set(indices[:n_val])\n",
        "train_idx= set(indices[n_val:])\n",
        "\n",
        "class SubsetDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, base, keep_idx):\n",
        "        self.base = base\n",
        "        self.keep = sorted(list(keep_idx))\n",
        "    def __len__(self): return len(self.keep)\n",
        "    def __getitem__(self, i): return self.base[self.keep[i]]\n",
        "\n",
        "ds_train = SubsetDS(train_ds, train_idx)\n",
        "ds_val   = SubsetDS(train_ds, val_idx)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=os.path.join(OUT_DIR, \"ckpt\"),\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=20,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    warmup_steps=300,\n",
        "    weight_decay=0.0,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,  # keep our custom fields\n",
        ")\n",
        "\n",
        "trainer = MTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_val,\n",
        "    data_collator=mt_collate,\n",
        "    two_head=two_head,\n",
        "    lambda_A=1.0,\n",
        "    lambda_B=1.0,\n",
        "    lambda_C=0.0,   # keep 0.0 for now; we’ll add strict consistency later\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save adapter + heads\n",
        "save_dir = os.path.join(OUT_DIR, \"final_multitask_abortion_g3\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "torch.save(two_head.state_dict(), os.path.join(save_dir, \"two_head_abortion_g3.pt\"))\n",
        "print(\"Saved to:\", save_dir)"
      ],
      "metadata": {
        "id": "-rX4b2darY6N",
        "outputId": "a4a86614-fba3-4a04-8fcd-a31b4be72cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "id": "-rX4b2darY6N",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1960' max='1960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1960/1960 10:30, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>6.596100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>4.760200</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.344400</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>3.044900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.203400</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.580100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.098000</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.725100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.567900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to: /content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_p_trial/final_multitask_abortion_g3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X5OrPpoX3aPg",
      "metadata": {
        "id": "X5OrPpoX3aPg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce67622833e8464da402399263383961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5666b26ba9b46ac9d7f8ac3f3c1f540",
              "IPY_MODEL_17f4ccd590d74d6594265cac71c2a277",
              "IPY_MODEL_de8f3ca7031d40ec88e33e7b3aff3187"
            ],
            "layout": "IPY_MODEL_a18a0b9a50714d6d974082b2f8e368a4"
          }
        },
        "e5666b26ba9b46ac9d7f8ac3f3c1f540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bd89daa8daa48fb9ebf125fa645dd7e",
            "placeholder": "​",
            "style": "IPY_MODEL_9dfc5a5d4c254e2c926f8fa91cfb2b37",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "17f4ccd590d74d6594265cac71c2a277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c98a525868ce47d68d9d03599f14359e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5029f6a50c8b4dca9905285f804ed47f",
            "value": 4
          }
        },
        "de8f3ca7031d40ec88e33e7b3aff3187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e687903c518043248f10ad7a263bb2cb",
            "placeholder": "​",
            "style": "IPY_MODEL_c9f8ac84ca7e447abc1837579f9eeb83",
            "value": " 4/4 [00:04&lt;00:00,  1.03s/it]"
          }
        },
        "a18a0b9a50714d6d974082b2f8e368a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd89daa8daa48fb9ebf125fa645dd7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dfc5a5d4c254e2c926f8fa91cfb2b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c98a525868ce47d68d9d03599f14359e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5029f6a50c8b4dca9905285f804ed47f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e687903c518043248f10ad7a263bb2cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9f8ac84ca7e447abc1837579f9eeb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c88a444935be46d3880a733cd439649c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d65e44172fef408081f26c38eb10f2fa",
              "IPY_MODEL_623c4059eba84c558f909ccfc7334a89",
              "IPY_MODEL_adb23c01510d48e1acf2b0bb5e900be8"
            ],
            "layout": "IPY_MODEL_786c9924367b475fa21711636bb799cd"
          }
        },
        "d65e44172fef408081f26c38eb10f2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b748a94c1c1c4befb8a03931240e0b14",
            "placeholder": "​",
            "style": "IPY_MODEL_fb9e3d39a69b45cca57f1b65c4a121a7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "623c4059eba84c558f909ccfc7334a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a621a511007401b8eb1f3bcd8ab2b03",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_549e2f69c5ca4c99b79d6d3330e4ea16",
            "value": 4
          }
        },
        "adb23c01510d48e1acf2b0bb5e900be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fc202fd044c4e0aaa36099f891f7312",
            "placeholder": "​",
            "style": "IPY_MODEL_5e1aabae6fd1421ab679fa16345a2050",
            "value": " 4/4 [00:04&lt;00:00,  1.04s/it]"
          }
        },
        "786c9924367b475fa21711636bb799cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b748a94c1c1c4befb8a03931240e0b14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb9e3d39a69b45cca57f1b65c4a121a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a621a511007401b8eb1f3bcd8ab2b03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "549e2f69c5ca4c99b79d6d3330e4ea16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fc202fd044c4e0aaa36099f891f7312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e1aabae6fd1421ab679fa16345a2050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}