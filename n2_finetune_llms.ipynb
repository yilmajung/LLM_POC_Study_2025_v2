{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers accelerate peft sentencepiece pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eecf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86667df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "CS_CSV    = \"data/gss_abt_cs_full.csv\"       # cross-sectional long\n",
    "PANEL_CSV = \"data/gss_abt_panel_full.csv\"    # panel long\n",
    "OUT_DIR   = \"outputs_gss_multitask\"          # where I write JSONLs & checkpoints\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# LLM choice\n",
    "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"  # or \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "# Canonical bins (K=4)\n",
    "CANON4 = [\"strong_anti\", \"anti\", \"pro\", \"strong_pro\"]\n",
    "CAT2ID = {c:i for i,c in enumerate(CANON4)}\n",
    "K = len(CANON4)\n",
    "YEARS_CS = list(range(2006, 2025, 2))  # 2006..2024 every 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and harmonize the data\n",
    "# --- Cross-sectional ---\n",
    "cs = pd.read_csv(CS_CSV)\n",
    "# Expect: yearid, year, abortion_att4, generation, race, gender, edu_level, wtssps\n",
    "# Map the attitude to canonical\n",
    "cs[\"att\"] = cs[\"abortion_att4\"].astype(str).str.strip()\n",
    "\n",
    "# keep only canon categories, drop NAs\n",
    "cs = cs[cs[\"att\"].isin(CANON4)].copy()\n",
    "cs[\"wt\"] = cs.get(\"wtssps\", pd.Series([1.0]*len(cs)))  # default 1.0 if missing\n",
    "\n",
    "# --- Panel ---\n",
    "pl = pd.read_csv(PANEL_CSV)\n",
    "# Expect: id, year, abortion_att4, generation, race, gender, edu_level\n",
    "pl[\"att\"] = pl[\"abortion_att4\"].astype(str).str.strip()\n",
    "pl = pl[pl[\"att\"].isin(CANON4)].copy()\n",
    "\n",
    "# Define grouping keys\n",
    "GROUP_COLS = [\"generation\",\"gender\",\"race\"]\n",
    "for df in (cs, pl):\n",
    "    for c in GROUP_COLS:\n",
    "        df[c] = df[c].astype(str).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cross-section margins p_cs[g,y] (weighted)\n",
    "def group_key(row):\n",
    "    return (row[\"generation\"], row[\"gender\"], row[\"race\"])\n",
    "\n",
    "def weighted_probs(vals, wts, cats=CANON4):\n",
    "    # vals: list of category strings; wts: weights\n",
    "    counts = {c:0.0 for c in cats}\n",
    "    for v, w in zip(vals, wts):\n",
    "        counts[v] += float(w)\n",
    "    vec = np.array([counts[c] for c in cats], dtype=float)\n",
    "    s = vec.sum()\n",
    "    if s <= 0: return None\n",
    "    return vec / s\n",
    "\n",
    "# p_cs[(g,y)] -> np.array[K]\n",
    "p_cs = {}\n",
    "effN_cs = {}  # effective N for weighting samples in Task B\n",
    "for y in YEARS_CS:\n",
    "    sub = cs[cs[\"year\"]==y]\n",
    "    if sub.empty: \n",
    "        continue\n",
    "    for g_vals, df_g in sub.groupby(GROUP_COLS):\n",
    "        p = weighted_probs(df_g[\"att\"].tolist(), df_g[\"wt\"].tolist(), CANON4)\n",
    "        if p is None: \n",
    "            continue\n",
    "        p_cs[(g_vals, y)] = p\n",
    "        effN_cs[(g_vals, y)] = float(df_g[\"wt\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71dd6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build panel transitions C[(g,t,Δ)] from consecutive waves per yearid\n",
    "# Build transitions per (g, t, Δ)\n",
    "# C[(g,t,Δ)] -> KxK counts; Nfrom[(g,t,Δ)] -> row totals length K\n",
    "from collections import defaultdict\n",
    "\n",
    "def canon_index(cat):\n",
    "    return CAT2ID.get(cat, None)\n",
    "\n",
    "C = defaultdict(lambda: np.zeros((K,K), dtype=float))\n",
    "Nfrom = defaultdict(lambda: np.zeros((K,), dtype=float))\n",
    "\n",
    "# Weight=1.0\n",
    "pl[\"w\"] = 1.0\n",
    "\n",
    "# Consecutive transitions per id\n",
    "for (pid), df_id in pl.groupby(\"yearid\"):\n",
    "    df_id = df_id.sort_values(\"year\")\n",
    "    # collapse duplicates per year if any\n",
    "    df_id = df_id.drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
    "    years = df_id[\"year\"].values.tolist()\n",
    "    atts  = df_id[\"att\"].values.tolist()\n",
    "    wgts  = df_id[\"w\"].values.astype(float).tolist()\n",
    "    gens  = df_id[\"generation\"].values.tolist()\n",
    "    gend  = df_id[\"gender\"].values.tolist()\n",
    "    race  = df_id[\"race\"].values.tolist()\n",
    "\n",
    "    # require consistent group labels across waves for this id (common in panels)\n",
    "    if not (len(set(gens))==1 and len(set(gend))==1 and len(set(race))==1):\n",
    "        # if you prefer, skip inconsistent cases\n",
    "        continue\n",
    "\n",
    "    g = (gens[0], gend[0], race[0])\n",
    "    for i in range(len(years)-1):\n",
    "        t, t1 = int(years[i]), int(years[i+1])\n",
    "        Δ = t1 - t\n",
    "        if Δ not in (2,4):  # we only have 2-yr and 4-yr gaps here\n",
    "            continue\n",
    "        ai = canon_index(atts[i])\n",
    "        aj = canon_index(atts[i+1])\n",
    "        if ai is None or aj is None:\n",
    "            continue\n",
    "        w = float(wgts[i])  # or min(wgts[i], wgts[i+1])\n",
    "        C[(g,t,Δ)][ai, aj] += w\n",
    "        Nfrom[(g,t,Δ)][ai] += w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL builders: Task A (panel rows) and Task B (margins)\n",
    "def smooth_row(row_counts, alpha):\n",
    "    rc = np.array(row_counts, dtype=float) + alpha\n",
    "    s = rc.sum()\n",
    "    if s <= 0: \n",
    "        return np.ones_like(rc)/len(rc)\n",
    "    return rc / s\n",
    "\n",
    "def build_taskA_rows(C, Nfrom, p_cs, out_jsonl, alpha_small=0.05, alpha_big=0.25, n_thresh=20, cap_weight=100.0):\n",
    "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
    "    n_rows = 0\n",
    "    for (g,t,Δ), mat in C.items():\n",
    "        nrow = Nfrom[(g,t,Δ)]\n",
    "        for i in range(K):\n",
    "            alpha = alpha_small if nrow[i] >= n_thresh else alpha_big\n",
    "            tgt = smooth_row(mat[i,:], alpha=alpha).tolist()\n",
    "            # Build prompt\n",
    "            prompt = (\n",
    "                \"[Task: Predict transition row]\\n\"\n",
    "                f\"From: <Y{t}> → To: <Y{t+Δ}> <DT{Δ}>\\n\"\n",
    "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}\\n\"\n",
    "                f\"From option: {CANON4[i]}\\n\"\n",
    "                \"Answer:\\n\"\n",
    "            )\n",
    "            rec = {\n",
    "                \"task\": \"row\",\n",
    "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2]},\n",
    "                \"year_t\": t,\n",
    "                \"year_t1\": t+Δ,\n",
    "                \"dt\": Δ,\n",
    "                \"from_bin\": CANON4[i],\n",
    "                \"prompt_text\": prompt,\n",
    "                \"to_dist\": tgt,\n",
    "                \"weight\": float(min(nrow[i], cap_weight))\n",
    "            }\n",
    "            # Attach margins if both available (for consistency loss later)\n",
    "            if ((g,t) in p_cs) and ((g,t+Δ) in p_cs):\n",
    "                rec[\"p_curr\"] = p_cs[(g,t)].tolist()\n",
    "                rec[\"p_next\"] = p_cs[(g,t+Δ)].tolist()\n",
    "            out.write(json.dumps(rec) + \"\\n\")\n",
    "            n_rows += 1\n",
    "    out.close()\n",
    "    return n_rows\n",
    "\n",
    "def build_taskB_rows(p_cs, effN_cs, out_jsonl, lags=(2,4), cap_weight=500.0):\n",
    "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
    "    n_rows = 0\n",
    "    # group by g\n",
    "    by_g = {}\n",
    "    for (g,y) in p_cs.keys():\n",
    "        by_g.setdefault(g, []).append(y)\n",
    "    for g, years in by_g.items():\n",
    "        ys = sorted(years)\n",
    "        for y in ys:\n",
    "            # context from previous lags\n",
    "            ctx = []\n",
    "            for L in lags:\n",
    "                yprev = y - L\n",
    "                if (g, yprev) in p_cs:\n",
    "                    ctx.append((yprev, p_cs[(g, yprev)]))\n",
    "            if len(ctx) == 0:\n",
    "                continue\n",
    "            # Build prompt\n",
    "            ctx_parts = \" \".join([f\"<Y{yy}>[{','.join(f'{x:.4f}' for x in p)}]\" for (yy,p) in ctx])\n",
    "            prompt = (\n",
    "                \"[Task: Forecast next-wave margin]\\n\"\n",
    "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}\\n\"\n",
    "                f\"Context: {ctx_parts}\\n\"\n",
    "                f\"Predict: <Y{y}>\\n\"\n",
    "                \"Answer:\\n\"\n",
    "            )\n",
    "            w = float(min(effN_cs.get((g,y), 1.0), cap_weight))\n",
    "            rec = {\n",
    "                \"task\": \"margin\",\n",
    "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2]},\n",
    "                \"year\": y,\n",
    "                \"prompt_text\": prompt,\n",
    "                \"to_dist\": p_cs[(g,y)].tolist(),\n",
    "                \"weight\": w\n",
    "            }\n",
    "            out.write(json.dumps(rec) + \"\\n\")\n",
    "            n_rows += 1\n",
    "    out.close()\n",
    "    return n_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c51ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKA_JSONL = os.path.join(OUT_DIR, \"taskA_panel_rows.jsonl\")\n",
    "TASKB_JSONL = os.path.join(OUT_DIR, \"taskB_margins.jsonl\")\n",
    "\n",
    "na = build_taskA_rows(C, Nfrom, p_cs, TASKA_JSONL)\n",
    "nb = build_taskB_rows(p_cs, effN_cs, TASKB_JSONL)\n",
    "print(f\"Wrote Task A rows: {na}\")\n",
    "print(f\"Wrote Task B rows: {nb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and Collator (multitask)\n",
    "class MTJsonlDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, jsonl_paths: List[str], tokenizer, max_len=768):\n",
    "        self.rows = []\n",
    "        for p in jsonl_paths:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.rows.extend([json.loads(x) for x in f])\n",
    "        random.shuffle(self.rows)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        r = self.rows[i]\n",
    "        enc = self.tok(r[\"prompt_text\"], return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
    "        out = {\n",
    "            \"task_type\": r[\"task\"],\n",
    "            \"input_ids\": enc[\"input_ids\"][0],\n",
    "            \"attention_mask\": enc[\"attention_mask\"][0],\n",
    "            \"to_dist\": torch.tensor(r[\"to_dist\"], dtype=torch.float),\n",
    "            \"weight\": torch.tensor(float(r.get(\"weight\", 1.0)), dtype=torch.float),\n",
    "        }\n",
    "        # add optional consistency fields\n",
    "        if r[\"task\"]==\"row\" and (\"p_curr\" in r) and (\"p_next\" in r):\n",
    "            out[\"p_curr\"] = torch.tensor(r[\"p_curr\"], dtype=torch.float)\n",
    "            out[\"p_next\"] = torch.tensor(r[\"p_next\"], dtype=torch.float)\n",
    "            out[\"has_consistency\"] = torch.tensor(1, dtype=torch.long)\n",
    "        else:\n",
    "            out[\"has_consistency\"] = torch.tensor(0, dtype=torch.long)\n",
    "        return out\n",
    "\n",
    "def mt_collate(batch):\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    maxlen = max(x[\"input_ids\"].shape[0] for x in batch)\n",
    "\n",
    "    def pad(seq, pad_val, target_len):\n",
    "        pad_n = target_len - seq.shape[0]\n",
    "        if pad_n <= 0: return seq\n",
    "        return torch.cat([seq, torch.full((pad_n,), pad_val, dtype=seq.dtype)])\n",
    "\n",
    "    input_ids      = torch.stack([pad(x[\"input_ids\"], pad_id, maxlen) for x in batch])\n",
    "    attention_mask = torch.stack([pad(x[\"attention_mask\"], 0, maxlen) for x in batch])\n",
    "    to_dist        = torch.stack([x[\"to_dist\"] for x in batch])\n",
    "    weight         = torch.stack([x[\"weight\"] for x in batch])\n",
    "    has_cons       = torch.stack([x[\"has_consistency\"] for x in batch])\n",
    "    # p_curr/p_next if present; else zeros\n",
    "    p_curr = torch.zeros(len(batch), K, dtype=torch.float)\n",
    "    p_next = torch.zeros(len(batch), K, dtype=torch.float)\n",
    "    for i, x in enumerate(batch):\n",
    "        if x[\"has_consistency\"]==1:\n",
    "            p_curr[i] = x[\"p_curr\"]\n",
    "            p_next[i] = x[\"p_next\"]\n",
    "\n",
    "    task_types = [x[\"task_type\"] for x in batch]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"to_dist\": to_dist,\n",
    "        \"weight\": weight,\n",
    "        \"has_consistency\": has_cons,\n",
    "        \"p_curr\": p_curr,\n",
    "        \"p_next\": p_next,\n",
    "        \"task_types\": task_types,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ba38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: LLM backbone + two small heads + last-K pooling\n",
    "# Tokenizer & backbone\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "# Add LoRA to attention/MLP projections\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "# Two small heads\n",
    "class TwoHead(nn.Module):\n",
    "    def __init__(self, hidden_size, K):\n",
    "        super().__init__()\n",
    "        self.head_row    = nn.Linear(hidden_size, K)   # Task A\n",
    "        self.head_margin = nn.Linear(hidden_size, K)   # Task B\n",
    "\n",
    "    def forward(self, feats):\n",
    "        return self.head_row(feats), self.head_margin(feats)\n",
    "\n",
    "hidden_size = base.config.hidden_size\n",
    "two_head = TwoHead(hidden_size, K).to(model.device)\n",
    "\n",
    "# Simple pooled features: mean of last K tokens (tail window)\n",
    "def pooled_features(outputs, attention_mask, tail=96):\n",
    "    hs = outputs.hidden_states[-1]     # [B,T,H]\n",
    "    B, T, H = hs.shape\n",
    "    valid_lens = attention_mask.sum(dim=1)  # [B]\n",
    "    feats = []\n",
    "    for b in range(B):\n",
    "        L = int(valid_lens[b].item())\n",
    "        s = max(0, L - tail); e = L\n",
    "        if e <= s: s, e = max(0, L-32), L\n",
    "        feats.append(hs[b, s:e, :].mean(dim=0))\n",
    "    feats = torch.stack(feats, dim=0)\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c75c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer with multitask losses (KL + optional consistency)\n",
    "class MTTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.two_head = kwargs.pop(\"two_head\")\n",
    "        self.lambda_A = kwargs.pop(\"lambda_A\", 1.0)\n",
    "        self.lambda_B = kwargs.pop(\"lambda_B\", 1.0)\n",
    "        self.lambda_C = kwargs.pop(\"lambda_C\", 0.5)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids      = inputs[\"input_ids\"].to(model.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "        to_dist        = inputs[\"to_dist\"].to(model.device)     # [B,K]\n",
    "        weight         = inputs[\"weight\"].to(model.device)      # [B]\n",
    "        has_cons       = inputs[\"has_consistency\"].to(model.device)  # [B]\n",
    "        p_curr         = inputs[\"p_curr\"].to(model.device)      # [B,K]\n",
    "        p_next         = inputs[\"p_next\"].to(model.device)      # [B,K]\n",
    "        task_types     = inputs[\"task_types\"]                   # list[str]\n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        feats = pooled_features(out, attention_mask, tail=96).to(model.device)  # [B,H]\n",
    "\n",
    "        logits_row, logits_margin = self.two_head(feats)   # [B,K], [B,K]\n",
    "        p_row_hat    = F.softmax(logits_row, dim=1)\n",
    "        p_margin_hat = F.softmax(logits_margin, dim=1)\n",
    "\n",
    "        # Build masks by task\n",
    "        is_row    = torch.tensor([t==\"row\"    for t in task_types], device=model.device, dtype=torch.bool)\n",
    "        is_margin = torch.tensor([t==\"margin\" for t in task_types], device=model.device, dtype=torch.bool)\n",
    "\n",
    "        eps = 1e-8\n",
    "        def fwd_kl(p, q):\n",
    "            p = p.clamp_min(eps); q = q.clamp_min(eps)\n",
    "            return (p * (p.log() - q.log())).sum(dim=1)\n",
    "\n",
    "        loss = torch.tensor(0.0, device=model.device)\n",
    "\n",
    "        # Task A: panel rows\n",
    "        if is_row.any():\n",
    "            L_A = fwd_kl(to_dist[is_row], p_row_hat[is_row])\n",
    "            loss = loss + self.lambda_A * (weight[is_row] * L_A).mean()\n",
    "\n",
    "        # Task B: margins\n",
    "        if is_margin.any():\n",
    "            L_B = fwd_kl(to_dist[is_margin], p_margin_hat[is_margin])\n",
    "            loss = loss + self.lambda_B * (weight[is_margin] * L_B).mean()\n",
    "\n",
    "        # Consistency: only where we have panel row AND margins for that item\n",
    "        cons_mask = is_row & (has_cons==1)\n",
    "        if cons_mask.any() and self.lambda_C > 0:\n",
    "            # Here each item is ONE row. For a strict consistency you’d assemble all 4 rows per (g,t,Δ).\n",
    "            # As a simple, stable proxy, we push the single-row prediction to move p_curr toward p_next\n",
    "            # by encouraging alignment on the target category mass direction.\n",
    "            # A stronger version would batch rows by (g,t,Δ) and assemble a full T̂.\n",
    "            p_curr_cons = p_curr[cons_mask]     # [B1,K]\n",
    "            p_next_cons = p_next[cons_mask]     # [B1,K]\n",
    "            # push in the direction implied by this row; we approximate by mixing:\n",
    "            # p_next_trans ≈ p_curr with mass from the 'from_bin' redistributed as p_row_hat.\n",
    "            # But we don't have 'from_bin' index in this collate—so we keep L_C off by default\n",
    "            # OR set lambda_C=0.0 now and later implement grouped assembly.\n",
    "            # We'll leave lambda_C=0.0 below to avoid confusion.\n",
    "            pass\n",
    "\n",
    "        return (loss, {\"logits_row\": logits_row, \"logits_margin\": logits_margin}) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b03be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "tokenizer.padding_side = \"left\"  # (helps with some backbones; optional)\n",
    "\n",
    "train_ds = MTJsonlDataset([TASKA_JSONL, TASKB_JSONL], tokenizer, max_len=768)\n",
    "# For a quick start you can split train/val here:\n",
    "val_frac = 0.1\n",
    "n_val = int(len(train_ds) * val_frac)\n",
    "indices = list(range(len(train_ds)))\n",
    "random.seed(0); random.shuffle(indices)\n",
    "val_idx  = set(indices[:n_val])\n",
    "train_idx= set(indices[n_val:])\n",
    "\n",
    "class SubsetDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, base, keep_idx):\n",
    "        self.base = base\n",
    "        self.keep = sorted(list(keep_idx))\n",
    "    def __len__(self): return len(self.keep)\n",
    "    def __getitem__(self, i): return self.base[self.keep[i]]\n",
    "\n",
    "ds_train = SubsetDS(train_ds, train_idx)\n",
    "ds_val   = SubsetDS(train_ds, val_idx)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUT_DIR, \"ckpt\"),\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=6,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=400,\n",
    "    save_steps=400,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.0,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,  # keep our custom fields\n",
    ")\n",
    "\n",
    "trainer = MTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    data_collator=mt_collate,\n",
    "    two_head=two_head,\n",
    "    lambda_A=1.0,\n",
    "    lambda_B=1.0,\n",
    "    lambda_C=0.0,   # keep 0.0 for now; we’ll add strict consistency later\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter + heads\n",
    "save_dir = os.path.join(OUT_DIR, \"final_multitask\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "torch.save(two_head.state_dict(), os.path.join(save_dir, \"two_head.pt\"))\n",
    "print(\"Saved to:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d344019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc26ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778271c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
