{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2ed569a4",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yilmajung/LLM_POC_Study_2025_v2/blob/main/n2_finetune_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d3ab407b",
      "metadata": {
        "id": "d3ab407b"
      },
      "outputs": [],
      "source": [
        "!pip -q install transformers accelerate peft sentencepiece pandas pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "hBmUogRwruoZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBmUogRwruoZ",
        "outputId": "5174974f-48bd-408d-99af-a14796087b21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3eecf706",
      "metadata": {
        "id": "3eecf706"
      },
      "outputs": [],
      "source": [
        "import os, json, math, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "86667df6",
      "metadata": {
        "id": "86667df6"
      },
      "outputs": [],
      "source": [
        "# Set up paths\n",
        "CS_CSV    = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_cs_full.csv\"       # cross-sectional long\n",
        "PANEL_CSV = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_panel_full.csv\"    # panel long\n",
        "OUT_DIR   = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_multitask\"          # where I write JSONLs & checkpoints\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# LLM choice\n",
        "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"  # or \"mistralai/Mistral-7B-v0.3\"\n",
        "\n",
        "# Canonical bins (K=4)\n",
        "CANON4 = [\"strong_anti\", \"anti\", \"pro\", \"strong_pro\"]\n",
        "CAT2ID = {c:i for i,c in enumerate(CANON4)}\n",
        "K = len(CANON4)\n",
        "YEARS_CS = list(range(2006, 2025, 2))  # 2006..2024 every 2 years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2f93df",
      "metadata": {
        "id": "cd2f93df"
      },
      "outputs": [],
      "source": [
        "# Load and harmonize the data\n",
        "# --- Cross-sectional ---\n",
        "cs = pd.read_csv(CS_CSV)\n",
        "# Expect: yearid, year, abortion_att4, generation, race, gender, edu_level, wtssps\n",
        "# Map the attitude to canonical\n",
        "cs[\"att\"] = cs[\"abortion_att4\"].astype(str).str.strip()\n",
        "\n",
        "# keep only canon categories, drop NAs\n",
        "cs = cs[cs[\"att\"].isin(CANON4)].copy()\n",
        "cs[\"wt\"] = cs.get(\"wtssps\", pd.Series([1.0]*len(cs)))  # default 1.0 if missing\n",
        "\n",
        "# --- Panel ---\n",
        "pl = pd.read_csv(PANEL_CSV)\n",
        "# Expect: id, year, abortion_att4, generation, race, gender, edu_level\n",
        "pl[\"att\"] = pl[\"abortion_att4\"].astype(str).str.strip()\n",
        "pl = pl[pl[\"att\"].isin(CANON4)].copy()\n",
        "\n",
        "# Define grouping keys\n",
        "GROUP_COLS = [\"generation\",\"gender\",\"race\",\"edu_level\"]\n",
        "for df in (cs, pl):\n",
        "    for c in GROUP_COLS:\n",
        "        df[c] = df[c].astype(str).str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d3111f",
      "metadata": {
        "id": "18d3111f"
      },
      "outputs": [],
      "source": [
        "# Build cross-section margins p_cs[g,y] (weighted)\n",
        "def group_key(row):\n",
        "    return (row[\"generation\"], row[\"gender\"], row[\"race\"], row['edu_level'])\n",
        "\n",
        "def weighted_probs(vals, wts, cats=CANON4):\n",
        "    # vals: list of category strings; wts: weights\n",
        "    counts = {c:0.0 for c in cats}\n",
        "    for v, w in zip(vals, wts):\n",
        "        counts[v] += float(w)\n",
        "    vec = np.array([counts[c] for c in cats], dtype=float)\n",
        "    s = vec.sum()\n",
        "    if s <= 0: return None\n",
        "    return vec / s\n",
        "\n",
        "# p_cs[(g,y)] -> np.array[K]\n",
        "p_cs = {}\n",
        "effN_cs = {}  # effective N for weighting samples in Task B\n",
        "for y in YEARS_CS:\n",
        "    sub = cs[cs[\"year\"]==y]\n",
        "    if sub.empty:\n",
        "        continue\n",
        "    for g_vals, df_g in sub.groupby(GROUP_COLS):\n",
        "        p = weighted_probs(df_g[\"att\"].tolist(), df_g[\"wt\"].tolist(), CANON4)\n",
        "        if p is None:\n",
        "            continue\n",
        "        p_cs[(g_vals, y)] = p\n",
        "        effN_cs[(g_vals, y)] = float(df_g[\"wt\"].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e71dd6ec",
      "metadata": {
        "id": "e71dd6ec"
      },
      "outputs": [],
      "source": [
        "# Build panel transitions C[(g,t,Δ)] from consecutive waves per yearid\n",
        "# Build transitions per (g, t, Δ)\n",
        "# C[(g,t,Δ)] -> KxK counts; Nfrom[(g,t,Δ)] -> row totals length K\n",
        "from collections import defaultdict\n",
        "\n",
        "def canon_index(cat):\n",
        "    return CAT2ID.get(cat, None)\n",
        "\n",
        "C = defaultdict(lambda: np.zeros((K,K), dtype=float))\n",
        "Nfrom = defaultdict(lambda: np.zeros((K,), dtype=float))\n",
        "\n",
        "# Weight=1.0\n",
        "pl[\"w\"] = 1.0\n",
        "\n",
        "# Consecutive transitions per id\n",
        "for (pid), df_id in pl.groupby(\"yearid\"):\n",
        "    df_id = df_id.sort_values(\"year\")\n",
        "    # collapse duplicates per year if any\n",
        "    df_id = df_id.drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
        "    years = df_id[\"year\"].values.tolist()\n",
        "    atts  = df_id[\"att\"].values.tolist()\n",
        "    wgts  = df_id[\"w\"].values.astype(float).tolist()\n",
        "    gens  = df_id[\"generation\"].values.tolist()\n",
        "    gend  = df_id[\"gender\"].values.tolist()\n",
        "    race  = df_id[\"race\"].values.tolist()\n",
        "    edu = df_id[\"edu_level\"].values.tolist()\n",
        "\n",
        "    # require consistent group labels across waves for this id (common in panels)\n",
        "    if not (len(set(gens))==1 and len(set(gend))==1 and len(set(race))==1 and len(set(edu))==1):\n",
        "        # if you prefer, skip inconsistent cases\n",
        "        continue\n",
        "\n",
        "    g = (gens[0], gend[0], race[0], edu[0])\n",
        "    for i in range(len(years)-1):\n",
        "        t, t1 = int(years[i]), int(years[i+1])\n",
        "        Δ = t1 - t\n",
        "        if Δ not in (2,4):  # we only have 2-yr and 4-yr gaps here\n",
        "            continue\n",
        "        ai = canon_index(atts[i])\n",
        "        aj = canon_index(atts[i+1])\n",
        "        if ai is None or aj is None:\n",
        "            continue\n",
        "        w = float(wgts[i])  # or min(wgts[i], wgts[i+1])\n",
        "        C[(g,t,Δ)][ai, aj] += w\n",
        "        Nfrom[(g,t,Δ)][ai] += w\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5195d7a2",
      "metadata": {
        "id": "5195d7a2"
      },
      "outputs": [],
      "source": [
        "# JSONL builders: Task A (panel rows) and Task B (margins)\n",
        "def smooth_row(row_counts, alpha):\n",
        "    rc = np.array(row_counts, dtype=float) + alpha\n",
        "    s = rc.sum()\n",
        "    if s <= 0:\n",
        "        return np.ones_like(rc)/len(rc)\n",
        "    return rc / s\n",
        "\n",
        "def build_taskA_rows(C, Nfrom, p_cs, out_jsonl, alpha_small=0.05, alpha_big=0.25, n_thresh=20, cap_weight=100.0):\n",
        "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
        "    n_rows = 0\n",
        "    for (g,t,Δ), mat in C.items():\n",
        "        nrow = Nfrom[(g,t,Δ)]\n",
        "        for i in range(K):\n",
        "            alpha = alpha_small if nrow[i] >= n_thresh else alpha_big\n",
        "            tgt = smooth_row(mat[i,:], alpha=alpha).tolist()\n",
        "            # Build prompt\n",
        "            prompt = (\n",
        "                \"[Task: Predict transition row]\\n\"\n",
        "                f\"From: <Y{t}> → To: <Y{t+Δ}> <DT{Δ}>\\n\"\n",
        "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}; edu_level={g[3]}\\n\"\n",
        "                f\"From option: {CANON4[i]}\\n\"\n",
        "                \"Answer:\\n\"\n",
        "            )\n",
        "            rec = {\n",
        "                \"task\": \"row\",\n",
        "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3]},\n",
        "                \"year_t\": t,\n",
        "                \"year_t1\": t+Δ,\n",
        "                \"dt\": Δ,\n",
        "                \"from_bin\": CANON4[i],\n",
        "                \"prompt_text\": prompt,\n",
        "                \"to_dist\": tgt,\n",
        "                \"weight\": float(min(nrow[i], cap_weight))\n",
        "            }\n",
        "            # Attach margins if both available (for consistency loss later)\n",
        "            if ((g,t) in p_cs) and ((g,t+Δ) in p_cs):\n",
        "                rec[\"p_curr\"] = p_cs[(g,t)].tolist()\n",
        "                rec[\"p_next\"] = p_cs[(g,t+Δ)].tolist()\n",
        "            out.write(json.dumps(rec) + \"\\n\")\n",
        "            n_rows += 1\n",
        "    out.close()\n",
        "    return n_rows\n",
        "\n",
        "def build_taskB_rows(p_cs, effN_cs, out_jsonl, lags=(2,4), cap_weight=500.0):\n",
        "    out = open(out_jsonl, \"w\", encoding=\"utf-8\")\n",
        "    n_rows = 0\n",
        "    # group by g\n",
        "    by_g = {}\n",
        "    for (g,y) in p_cs.keys():\n",
        "        by_g.setdefault(g, []).append(y)\n",
        "    for g, years in by_g.items():\n",
        "        ys = sorted(years)\n",
        "        for y in ys:\n",
        "            # context from previous lags\n",
        "            ctx = []\n",
        "            for L in lags:\n",
        "                yprev = y - L\n",
        "                if (g, yprev) in p_cs:\n",
        "                    ctx.append((yprev, p_cs[(g, yprev)]))\n",
        "            if len(ctx) == 0:\n",
        "                continue\n",
        "            # Build prompt\n",
        "            ctx_parts = \" \".join([f\"<Y{yy}>[{','.join(f'{x:.4f}' for x in p)}]\" for (yy,p) in ctx])\n",
        "            prompt = (\n",
        "                \"[Task: Forecast next-wave margin]\\n\"\n",
        "                f\"Group: generation={g[0]}; gender={g[1]}; race={g[2]}; edu_level={g[3]}\\n\"\n",
        "                f\"Context: {ctx_parts}\\n\"\n",
        "                f\"Predict: <Y{y}>\\n\"\n",
        "                \"Answer:\\n\"\n",
        "            )\n",
        "            w = float(min(effN_cs.get((g,y), 1.0), cap_weight))\n",
        "            rec = {\n",
        "                \"task\": \"margin\",\n",
        "                \"group\": {\"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3]},\n",
        "                \"year\": y,\n",
        "                \"prompt_text\": prompt,\n",
        "                \"to_dist\": p_cs[(g,y)].tolist(),\n",
        "                \"weight\": w\n",
        "            }\n",
        "            out.write(json.dumps(rec) + \"\\n\")\n",
        "            n_rows += 1\n",
        "    out.close()\n",
        "    return n_rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "d1c51ee0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1c51ee0",
        "outputId": "42c0cacd-cd9a-4f2e-cd14-e1422ebd80e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote Task A rows: 712\n",
            "Wrote Task B rows: 228\n"
          ]
        }
      ],
      "source": [
        "TASKA_JSONL = os.path.join(OUT_DIR, \"taskA_panel_rows.jsonl\")\n",
        "TASKB_JSONL = os.path.join(OUT_DIR, \"taskB_margins.jsonl\")\n",
        "\n",
        "na = build_taskA_rows(C, Nfrom, p_cs, TASKA_JSONL)\n",
        "nb = build_taskB_rows(p_cs, effN_cs, TASKB_JSONL)\n",
        "print(f\"Wrote Task A rows: {na}\")\n",
        "print(f\"Wrote Task B rows: {nb}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ef8f7270",
      "metadata": {
        "id": "ef8f7270"
      },
      "outputs": [],
      "source": [
        "# Datasets and Collator (multitask)\n",
        "class MTJsonlDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, jsonl_paths: List[str], tokenizer, max_len=768):\n",
        "        self.rows = []\n",
        "        for p in jsonl_paths:\n",
        "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "                self.rows.extend([json.loads(x) for x in f])\n",
        "        random.shuffle(self.rows)\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        enc = self.tok(r[\"prompt_text\"], return_tensors=\"pt\", truncation=True, max_length=self.max_len)\n",
        "        out = {\n",
        "            \"task_type\": r[\"task\"],\n",
        "            \"input_ids\": enc[\"input_ids\"][0],\n",
        "            \"attention_mask\": enc[\"attention_mask\"][0],\n",
        "            \"to_dist\": torch.tensor(r[\"to_dist\"], dtype=torch.float),\n",
        "            \"weight\": torch.tensor(float(r.get(\"weight\", 1.0)), dtype=torch.float),\n",
        "        }\n",
        "        # add optional consistency fields\n",
        "        if r[\"task\"]==\"row\" and (\"p_curr\" in r) and (\"p_next\" in r):\n",
        "            out[\"p_curr\"] = torch.tensor(r[\"p_curr\"], dtype=torch.float)\n",
        "            out[\"p_next\"] = torch.tensor(r[\"p_next\"], dtype=torch.float)\n",
        "            out[\"has_consistency\"] = torch.tensor(1, dtype=torch.long)\n",
        "        else:\n",
        "            out[\"has_consistency\"] = torch.tensor(0, dtype=torch.long)\n",
        "        return out\n",
        "\n",
        "def mt_collate(batch):\n",
        "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "    maxlen = max(x[\"input_ids\"].shape[0] for x in batch)\n",
        "\n",
        "    def pad(seq, pad_val, target_len):\n",
        "        pad_n = target_len - seq.shape[0]\n",
        "        if pad_n <= 0: return seq\n",
        "        return torch.cat([seq, torch.full((pad_n,), pad_val, dtype=seq.dtype)])\n",
        "\n",
        "    input_ids      = torch.stack([pad(x[\"input_ids\"], pad_id, maxlen) for x in batch])\n",
        "    attention_mask = torch.stack([pad(x[\"attention_mask\"], 0, maxlen) for x in batch])\n",
        "    to_dist        = torch.stack([x[\"to_dist\"] for x in batch])\n",
        "    weight         = torch.stack([x[\"weight\"] for x in batch])\n",
        "    has_cons       = torch.stack([x[\"has_consistency\"] for x in batch])\n",
        "    # p_curr/p_next if present; else zeros\n",
        "    p_curr = torch.zeros(len(batch), K, dtype=torch.float)\n",
        "    p_next = torch.zeros(len(batch), K, dtype=torch.float)\n",
        "    for i, x in enumerate(batch):\n",
        "        if x[\"has_consistency\"]==1:\n",
        "            p_curr[i] = x[\"p_curr\"]\n",
        "            p_next[i] = x[\"p_next\"]\n",
        "\n",
        "    task_types = [x[\"task_type\"] for x in batch]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"to_dist\": to_dist,\n",
        "        \"weight\": weight,\n",
        "        \"has_consistency\": has_cons,\n",
        "        \"p_curr\": p_curr,\n",
        "        \"p_next\": p_next,\n",
        "        \"task_types\": task_types,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c71ba38d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8266c08f84714cb4bc681b53d23725ea",
            "6904567760e24df8a5c6aa1df3f3871f",
            "e8a9159e2b174e3d913c5043810daa67",
            "a7b79c1cee474266b6826ff70809c2c0",
            "2e96edd059fe48bd95c7251952d07c72",
            "794d9d5e82974d61b0bf445607db5c1b",
            "8ddef760247e4004ac9891a8596e20e5",
            "756113d24fc34d1da84a7c0bad225ebf",
            "95b9222cc1dd4b0087e7b46e9256d366",
            "bc18e3aa01904b008e671d01197ee3e2",
            "7010eb27daff495caf0e8b34dd31d718"
          ]
        },
        "id": "c71ba38d",
        "outputId": "afb067c5-1bf8-4b58-c8ef-0e4d528f2d98"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8266c08f84714cb4bc681b53d23725ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Model: LLM backbone + two small heads + last-K pooling\n",
        "# Tokenizer & backbone\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    output_hidden_states=True\n",
        ")\n",
        "\n",
        "# Add LoRA to attention/MLP projections\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\"]\n",
        ")\n",
        "model = get_peft_model(base, lora_cfg)\n",
        "\n",
        "# Two task heads\n",
        "class TwoHead(nn.Module):\n",
        "    def __init__(self, hidden_size, K):\n",
        "        super().__init__()\n",
        "        self.head_row    = nn.Linear(hidden_size, K)   # Task A\n",
        "        self.head_margin = nn.Linear(hidden_size, K)   # Task B\n",
        "\n",
        "    def forward(self, feats):\n",
        "        return self.head_row(feats), self.head_margin(feats)\n",
        "\n",
        "hidden_size = base.config.hidden_size\n",
        "two_head = TwoHead(hidden_size, K).to(model.device)\n",
        "\n",
        "# Simple pooled features: mean of last K tokens (tail window)\n",
        "def pooled_features(outputs, attention_mask, tail=96):\n",
        "    hs = outputs.hidden_states[-1]     # [B,T,H]\n",
        "    B, T, H = hs.shape\n",
        "    valid_lens = attention_mask.sum(dim=1)  # [B]\n",
        "    feats = []\n",
        "    for b in range(B):\n",
        "        L = int(valid_lens[b].item())\n",
        "        s = max(0, L - tail); e = L\n",
        "        if e <= s: s, e = max(0, L-32), L\n",
        "        feats.append(hs[b, s:e, :].mean(dim=0))\n",
        "    feats = torch.stack(feats, dim=0)\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b2c75c96",
      "metadata": {
        "id": "b2c75c96"
      },
      "outputs": [],
      "source": [
        "# Trainer with multitask losses (KL + optional consistency)\n",
        "class MTTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.two_head = kwargs.pop(\"two_head\")\n",
        "        self.lambda_A = kwargs.pop(\"lambda_A\", 1.0)\n",
        "        self.lambda_B = kwargs.pop(\"lambda_B\", 1.0)\n",
        "        self.lambda_C = kwargs.pop(\"lambda_C\", 0.5)\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        model,\n",
        "        inputs,\n",
        "        return_outputs: bool = False,\n",
        "        num_items_in_batch: Optional[int] = None,  # <-- accept the kwarg\n",
        "    ):\n",
        "        input_ids      = inputs[\"input_ids\"].to(model.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
        "        to_dist        = inputs[\"to_dist\"].to(model.device)     # [B,K]\n",
        "        weight         = inputs[\"weight\"].to(model.device)      # [B]\n",
        "        has_cons       = inputs[\"has_consistency\"].to(model.device)  # [B]\n",
        "        p_curr         = inputs[\"p_curr\"].to(model.device)      # [B,K]\n",
        "        p_next         = inputs[\"p_next\"].to(model.device)      # [B,K]\n",
        "        task_types     = inputs[\"task_types\"]                   # list[str]\n",
        "\n",
        "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "            out = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        feats = pooled_features(out, attention_mask, tail=96).to(model.device)  # [B,H]\n",
        "\n",
        "        logits_row, logits_margin = self.two_head(feats)   # [B,K], [B,K]\n",
        "        p_row_hat    = F.softmax(logits_row, dim=1)\n",
        "        p_margin_hat = F.softmax(logits_margin, dim=1)\n",
        "\n",
        "        # Build masks by task\n",
        "        is_row    = torch.tensor([t == \"row\"    for t in task_types], device=model.device, dtype=torch.bool)\n",
        "        is_margin = torch.tensor([t == \"margin\" for t in task_types], device=model.device, dtype=torch.bool)\n",
        "\n",
        "        eps = 1e-8\n",
        "        def fwd_kl(p, q):\n",
        "            p = p.clamp_min(eps); q = q.clamp_min(eps)\n",
        "            return (p * (p.log() - q.log())).sum(dim=1)\n",
        "\n",
        "        loss = torch.tensor(0.0, device=model.device)\n",
        "\n",
        "        # Task A: panel rows\n",
        "        if is_row.any():\n",
        "            L_A = fwd_kl(to_dist[is_row], p_row_hat[is_row])\n",
        "            loss = loss + self.lambda_A * (weight[is_row] * L_A).mean()\n",
        "\n",
        "        # Task B: margins\n",
        "        if is_margin.any():\n",
        "            L_B = fwd_kl(to_dist[is_margin], p_margin_hat[is_margin])\n",
        "            loss = loss + self.lambda_B * (weight[is_margin] * L_B).mean()\n",
        "\n",
        "        # Consistency (disabled unless you wire full-row assembly):\n",
        "        # cons_mask = is_row & (has_cons==1)\n",
        "        # if cons_mask.any() and self.lambda_C > 0:\n",
        "        #     ...\n",
        "\n",
        "        if return_outputs:\n",
        "            return loss, {\"logits_row\": logits_row, \"logits_margin\": logits_margin, \"labels\": to_dist}\n",
        "        else:\n",
        "            return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "4WWUJ0d7UoRg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "4WWUJ0d7UoRg",
        "outputId": "e0dfc0c2-b9ef-44d7-add2-73e8bda9a314"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2120' max='2120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2120/2120 11:27, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.185000</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.879200</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.672900</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.296100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.127800</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.079000</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.764700</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.484100</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.314800</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.258700</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-4224191036.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to: /content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_multitask/final_multitask\n"
          ]
        }
      ],
      "source": [
        "# Training setup\n",
        "tokenizer.padding_side = \"left\"  # (helps with some backbones; optional)\n",
        "\n",
        "train_ds = MTJsonlDataset([TASKA_JSONL, TASKB_JSONL], tokenizer, max_len=768)\n",
        "# For a quick start you can split train/val here:\n",
        "val_frac = 0.1\n",
        "n_val = int(len(train_ds) * val_frac)\n",
        "indices = list(range(len(train_ds)))\n",
        "random.seed(0); random.shuffle(indices)\n",
        "val_idx  = set(indices[:n_val])\n",
        "train_idx= set(indices[n_val:])\n",
        "\n",
        "class SubsetDS(torch.utils.data.Dataset):\n",
        "    def __init__(self, base, keep_idx):\n",
        "        self.base = base\n",
        "        self.keep = sorted(list(keep_idx))\n",
        "    def __len__(self): return len(self.keep)\n",
        "    def __getitem__(self, i): return self.base[self.keep[i]]\n",
        "\n",
        "ds_train = SubsetDS(train_ds, train_idx)\n",
        "ds_val   = SubsetDS(train_ds, val_idx)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=os.path.join(OUT_DIR, \"ckpt\"),\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_train_epochs=20,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    bf16=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    warmup_steps=300,\n",
        "    weight_decay=0.0,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,  # keep our custom fields\n",
        ")\n",
        "\n",
        "trainer = MTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_val,\n",
        "    data_collator=mt_collate,\n",
        "    two_head=two_head,\n",
        "    lambda_A=1.0,\n",
        "    lambda_B=1.0,\n",
        "    lambda_C=0.0,   # keep 0.0 for now; we’ll add strict consistency later\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save adapter + heads\n",
        "save_dir = os.path.join(OUT_DIR, \"final_multitask\")\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "torch.save(two_head.state_dict(), os.path.join(save_dir, \"two_head.pt\"))\n",
        "print(\"Saved to:\", save_dir)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2e96edd059fe48bd95c7251952d07c72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6904567760e24df8a5c6aa1df3f3871f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_794d9d5e82974d61b0bf445607db5c1b",
            "placeholder": "​",
            "style": "IPY_MODEL_8ddef760247e4004ac9891a8596e20e5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7010eb27daff495caf0e8b34dd31d718": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "756113d24fc34d1da84a7c0bad225ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794d9d5e82974d61b0bf445607db5c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8266c08f84714cb4bc681b53d23725ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6904567760e24df8a5c6aa1df3f3871f",
              "IPY_MODEL_e8a9159e2b174e3d913c5043810daa67",
              "IPY_MODEL_a7b79c1cee474266b6826ff70809c2c0"
            ],
            "layout": "IPY_MODEL_2e96edd059fe48bd95c7251952d07c72"
          }
        },
        "8ddef760247e4004ac9891a8596e20e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95b9222cc1dd4b0087e7b46e9256d366": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7b79c1cee474266b6826ff70809c2c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc18e3aa01904b008e671d01197ee3e2",
            "placeholder": "​",
            "style": "IPY_MODEL_7010eb27daff495caf0e8b34dd31d718",
            "value": " 4/4 [00:05&lt;00:00,  1.14s/it]"
          }
        },
        "bc18e3aa01904b008e671d01197ee3e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8a9159e2b174e3d913c5043810daa67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_756113d24fc34d1da84a7c0bad225ebf",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95b9222cc1dd4b0087e7b46e9256d366",
            "value": 4
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
