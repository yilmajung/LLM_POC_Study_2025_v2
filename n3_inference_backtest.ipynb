{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yilmajung/LLM_POC_Study_2025_v2/blob/main/n3_inference_backtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9esWPo7KehI-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9esWPo7KehI-",
        "outputId": "5d01a35b-9439-43f8-f303-e2440906965b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ce5230e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8b268baf348541a5b7bf786e4cdcfd5a",
            "0efa59068c57467ab92901687f89975c",
            "6b58c83f36834f2a898f563a27db3a9b",
            "e8dfef70209c456bbc4b5f2455903dcb",
            "3ba2f628afa145328f7616360674085d",
            "41e9a22633d848eb8c6aabfa33b9a816",
            "4047b574f31b4df69b409e5f3fa0ab86",
            "f537ea94db6b4aeb842f04919caba061",
            "ec098c851b3943eda8ae232c54817cf5",
            "b8b74d895cf3432daaf9c92f5cde6391",
            "f2d9b706d1364fbdb757a90241757cb3"
          ]
        },
        "id": "ce5230e7",
        "outputId": "fe82a91a-58eb-411e-8cab-f5524393d997"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b268baf348541a5b7bf786e4cdcfd5a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os, json, numpy as np, pandas as pd, torch, torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import math\n",
        "\n",
        "# Set up paths\n",
        "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"    # or mistral\n",
        "SAVE_DIR   = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_multitask/final_multitask\"  # from training step\n",
        "HEAD_PATH  = os.path.join(SAVE_DIR, \"two_head.pt\")\n",
        "\n",
        "# Canon (K=4)\n",
        "CANON4 = [\"strong_anti\",\"anti\",\"pro\",\"strong_pro\"]\n",
        "K = len(CANON4)\n",
        "\n",
        "# Load tokenizer + base + LoRA\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    output_hidden_states=True\n",
        ")\n",
        "model = PeftModel.from_pretrained(base, SAVE_DIR)\n",
        "model.eval()\n",
        "\n",
        "# Two-head module (must match training)\n",
        "import torch.nn as nn\n",
        "class TwoHead(nn.Module):\n",
        "    def __init__(self, hidden_size, K):\n",
        "        super().__init__()\n",
        "        self.head_row    = nn.Linear(hidden_size, K)   # Task A\n",
        "        self.head_margin = nn.Linear(hidden_size, K)   # Task B\n",
        "    def forward(self, feats):\n",
        "        return self.head_row(feats), self.head_margin(feats)\n",
        "\n",
        "hidden_size = base.config.hidden_size\n",
        "two_head = TwoHead(hidden_size, K).to(model.device)\n",
        "two_head.load_state_dict(torch.load(HEAD_PATH, map_location=model.device))\n",
        "two_head.eval()\n",
        "\n",
        "def pooled_features(outputs, attention_mask, tail=96):\n",
        "    hs = outputs.hidden_states[-1]   # [B,T,H]\n",
        "    valid = attention_mask.sum(dim=1)\n",
        "    feats = []\n",
        "    for b in range(hs.size(0)):\n",
        "        L = int(valid[b].item())\n",
        "        s = max(0, L - tail); e = L\n",
        "        if e <= s: s, e = max(0, L-32), L\n",
        "        feats.append(hs[b, s:e, :].mean(dim=0))\n",
        "    return torch.stack(feats, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1671d50b",
      "metadata": {
        "id": "1671d50b"
      },
      "outputs": [],
      "source": [
        "# Helpers: predict a row/margin/full matrix\n",
        "def _to_head_dtype(x):\n",
        "    return x.to(two_head.head_row.weight.dtype)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_row_distribution(group, year_t, year_t1, from_bin, dt=None, max_len=768):\n",
        "    \"\"\"Task A: one row (from_bin) of the transition matrix.\"\"\"\n",
        "    if dt is None: dt = int(year_t1) - int(year_t)\n",
        "    prompt = (\n",
        "        \"[Task: Predict transition row]\\n\"\n",
        "        f\"From: <Y{year_t}> → To: <Y{year_t1}> <DT{dt}>\\n\"\n",
        "        f\"Group: generation={group['generation']}; gender={group['gender']}; race={group['race']}; edu_level={group['edu_level']}\\n\"\n",
        "        f\"From option: {from_bin}\\n\"\n",
        "        \"Answer:\\n\"\n",
        "    )\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
        "    ids = enc[\"input_ids\"].to(model.device); attn = enc[\"attention_mask\"].to(model.device)\n",
        "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "        out = model(input_ids=ids, attention_mask=attn, output_hidden_states=True)\n",
        "    feats = pooled_features(out, attn, tail=96).to(model.device)\n",
        "    feats = _to_head_dtype(feats)\n",
        "    logits_row, _ = two_head(feats)\n",
        "    p = F.softmax(logits_row, dim=1).float().cpu().numpy()[0]\n",
        "    return p  # shape [K]\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_full_transition(group, year_t, year_t1, max_len=768):\n",
        "    \"\"\"Assemble 4×4 by calling row head for each from-bin.\"\"\"\n",
        "    T = np.zeros((K,K), dtype=np.float32)\n",
        "    dt = int(year_t1) - int(year_t)\n",
        "    for i, from_bin in enumerate(CANON4):\n",
        "        T[i,:] = predict_row_distribution(group, year_t, year_t1, from_bin, dt=dt, max_len=max_len)\n",
        "    # force row-stochastic\n",
        "    T = np.clip(T, 1e-12, 1)\n",
        "    T = T / T.sum(axis=1, keepdims=True)\n",
        "    return T\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_next_margin(group, context, target_year, max_len=768):\n",
        "    \"\"\"\n",
        "    Task B: context = list of (year, prob_vector length K) tuples.\n",
        "    \"\"\"\n",
        "    ctx_parts = \" \".join([f\"<Y{yy}>[{','.join(f'{x:.4f}' for x in p)}]\" for (yy,p) in context])\n",
        "    prompt = (\n",
        "        \"[Task: Forecast next-wave margin]\\n\"\n",
        "        f\"Group: generation={group['generation']}; gender={group['gender']}; race={group['race']}; edu_level={group['edu_level']}\\n\"\n",
        "        f\"Context: {ctx_parts}\\n\"\n",
        "        f\"Predict: <Y{target_year}>\\n\"\n",
        "        \"Answer:\\n\"\n",
        "    )\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
        "    ids = enc[\"input_ids\"].to(model.device); attn = enc[\"attention_mask\"].to(model.device)\n",
        "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "        out = model(input_ids=ids, attention_mask=attn, output_hidden_states=True)\n",
        "    feats = pooled_features(out, attn, tail=96).to(model.device)\n",
        "    feats = _to_head_dtype(feats)\n",
        "    _, logits_margin = two_head(feats)\n",
        "    p = F.softmax(logits_margin, dim=1).float().cpu().numpy()[0]\n",
        "    return p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fe667f67",
      "metadata": {
        "id": "fe667f67"
      },
      "outputs": [],
      "source": [
        "# Using real margins and forecasting\n",
        "# Load cross-sectional CSV and build p_cs[(g,y)]\n",
        "cs = pd.read_csv(\"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_cs_full.csv\")\n",
        "# Map to 4 bins if numeric:\n",
        "def map_att(v):\n",
        "    if isinstance(v, (int, np.integer, float)) and not pd.isna(v):\n",
        "        return {1:\"strong_anti\",2:\"anti\",3:\"pro\",4:\"strong_pro\"}.get(int(v), None)\n",
        "    return str(v).strip()\n",
        "cs[\"att\"] = cs[\"abortion_att4\"].apply(map_att)\n",
        "cs = cs[cs[\"att\"].isin(CANON4)].copy()\n",
        "cs[\"wt\"] = cs.get(\"wtssps\", pd.Series([1.0]*len(cs)))\n",
        "\n",
        "GROUP_COLS = [\"generation\",\"gender\",\"race\",\"edu_level\"]\n",
        "for c in GROUP_COLS: cs[c] = cs[c].astype(str).str.strip()\n",
        "\n",
        "def weighted_probs(vals, wts):\n",
        "    d = {c:0.0 for c in CANON4}\n",
        "    for v,w in zip(vals,wts): d[v]+=float(w)\n",
        "    vec = np.array([d[c] for c in CANON4],dtype=float)\n",
        "    s = vec.sum();\n",
        "    return vec/s if s>0 else None\n",
        "\n",
        "p_cs = {}\n",
        "for (gvals, df_g) in cs.groupby(GROUP_COLS):\n",
        "    for y, df_y in df_g.groupby(\"year\"):\n",
        "        p = weighted_probs(df_y[\"att\"].tolist(), df_y[\"wt\"].tolist())\n",
        "        if p is not None:\n",
        "            p_cs[(gvals, int(y))] = p\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "-yh6Zv3NeeQS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "-yh6Zv3NeeQS",
        "outputId": "18231f57-93dd-4ec4-9717-5d90feb42988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-92730498.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-92730498.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/LLM_POC_Study_2025_v2/forecast_2024_from_2022_by_group.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Forecast 2024 from 2022 (per subgroup)\n",
        "# choose subgroups present in 2022\n",
        "groups_2022 = sorted({g for (g,y) in p_cs.keys() if y==2022})\n",
        "\n",
        "rows = []\n",
        "alpha = 0.5  # ensemble weight between transition-based and margin-head forecast\n",
        "\n",
        "for g in groups_2022:\n",
        "    group = {\"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3]}\n",
        "    # context margins e.g., 2018, 2020, 2022 if available\n",
        "    ctx = []\n",
        "    for yy in [2018, 2020, 2022]:\n",
        "        if (g, yy) in p_cs:\n",
        "            ctx.append((yy, p_cs[(g,yy)]))\n",
        "    if len(ctx)==0 or (g,2022) not in p_cs:\n",
        "        continue\n",
        "\n",
        "    p_2022 = p_cs[(g,2022)]\n",
        "    # Task A: transition matrix 2022->2024\n",
        "    T_22_24 = predict_full_transition(group, 2022, 2024)\n",
        "    p_2024_trans = p_2022 @ T_22_24\n",
        "\n",
        "    # Task B: direct next margin\n",
        "    p_2024_margin = predict_next_margin(group, ctx, 2024)\n",
        "\n",
        "    # Ensemble (optional)\n",
        "    p_2024_hat = alpha * p_2024_trans + (1 - alpha) * p_2024_margin\n",
        "    p_2024_hat = p_2024_hat / p_2024_hat.sum()\n",
        "\n",
        "    # Observed 2024 for backtest if available\n",
        "    p_2024_obs = p_cs.get((g,2024), None)\n",
        "\n",
        "    rec = {\n",
        "        \"generation\": g[0], \"gender\": g[1], \"race\": g[2],\n",
        "        **{f\"p2022_{c}\": float(p_2022[i]) for i,c in enumerate(CANON4)},\n",
        "        **{f\"p2024_trans_{c}\": float(p_2024_trans[i]) for i,c in enumerate(CANON4)},\n",
        "        **{f\"p2024_margin_{c}\": float(p_2024_margin[i]) for i,c in enumerate(CANON4)},\n",
        "        **{f\"p2024_hat_{c}\": float(p_2024_hat[i]) for i,c in enumerate(CANON4)},\n",
        "    }\n",
        "    if p_2024_obs is not None:\n",
        "        rec.update({f\"p2024_obs_{c}\": float(p_2024_obs[i]) for i,c in enumerate(CANON4)})\n",
        "\n",
        "        # quick metrics\n",
        "        def jsd(p,q,eps=1e-9):\n",
        "            p = np.clip(p,eps,1); q = np.clip(q,eps,1)\n",
        "            p/=p.sum(); q/=q.sum(); m=0.5*(p+q)\n",
        "            return 0.5*np.sum(p*np.log(p/m)) + 0.5*np.sum(q*np.log(q/m))\n",
        "        def rmse(p,q): return float(np.sqrt(np.mean((p-q)**2)))\n",
        "        rec[\"JSD_hat_vs_obs\"] = float(jsd(p_2024_hat, p_2024_obs))\n",
        "        rec[\"RMSE_hat_vs_obs\"] = rmse(p_2024_hat, p_2024_obs)\n",
        "        rec[\"JSD_trans_vs_obs\"] = float(jsd(p_2024_trans, p_2024_obs))\n",
        "        rec[\"JSD_margin_vs_obs\"]= float(jsd(p_2024_margin, p_2024_obs))\n",
        "\n",
        "    rows.append(rec)\n",
        "\n",
        "df_forecast = pd.DataFrame(rows)\n",
        "out_csv = os.path.join(SAVE_DIR, \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/forecast_2024_from_2022_by_group.csv\")\n",
        "df_forecast.to_csv(out_csv, index=False)\n",
        "out_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1vDDFk6tOQUV",
      "metadata": {
        "id": "1vDDFk6tOQUV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "CANON4 = [\"strong_anti\",\"anti\",\"pro\",\"strong_pro\"]\n",
        "\n",
        "def forecast_every_year_from_previous(\n",
        "    p_cs,                       # dict: {(gen,gender,race, edu_level), year} -> np.array length 5 (observed)\n",
        "    alpha=0.5,                  # ensemble weight: alpha*transition + (1-alpha)*margin\n",
        "    save_csv_path=None,         # optional: path to save tidy CSV\n",
        "    context_lags=(4, 2, 0),     # try up to these lags relative to y_prev (0 means include y_prev itself)\n",
        "    max_len=768,                # tokenizer max length\n",
        "):\n",
        "    \"\"\"\n",
        "    Produce forecasts for every (group, year) using the immediately previous observed year as the anchor.\n",
        "    Returns:\n",
        "        p_hat: dict keyed by ((gen,gender,race), year) -> np.array([4]) forecast for that year\n",
        "        df_out: tidy DataFrame with observed (if available) and all forecast variants\n",
        "    \"\"\"\n",
        "    # collect groups and years present in observed margins\n",
        "    all_pairs = list(p_cs.keys())\n",
        "    groups = sorted({g for (g, y) in all_pairs})\n",
        "    years_by_g = {g: sorted([y for (gg, y) in all_pairs if gg == g]) for g in groups}\n",
        "\n",
        "    rows = []\n",
        "    p_hat = {}\n",
        "\n",
        "    for g in groups:\n",
        "        ys = years_by_g[g]\n",
        "        if not ys:\n",
        "            continue\n",
        "        for idx in range(1, len(ys)):  # start from the second observed year\n",
        "            y_prev = ys[idx - 1]\n",
        "            y = ys[idx]\n",
        "\n",
        "            # observed previous margin\n",
        "            p_prev = p_cs.get((g, y_prev), None)\n",
        "            if p_prev is None:\n",
        "                continue  # cannot forecast without previous anchor\n",
        "\n",
        "            # build context relative to y_prev (e.g., y_prev-4, y_prev-2, y_prev)\n",
        "            ctx = []\n",
        "            for L in context_lags:\n",
        "                y_ctx = y_prev - L\n",
        "                if (g, y_ctx) in p_cs:\n",
        "                    ctx.append((y_ctx, p_cs[(g, y_ctx)]))\n",
        "            # ensure the last element of context is the anchor y_prev if available\n",
        "            if len(ctx) == 0 and (g, y_prev) in p_cs:\n",
        "                ctx.append((y_prev, p_cs[(g, y_prev)]))\n",
        "\n",
        "            # group dict for prompt\n",
        "            group = {\"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3]}\n",
        "\n",
        "            # predict transition matrix for the exact gap (usually 2; could be 4)\n",
        "            T = predict_full_transition(group, y_prev, y, max_len=max_len)  # shape [4,4]\n",
        "            p_trans = (p_prev @ T)  # transition-propagated forecast\n",
        "\n",
        "            # predict margin directly (if context exists)\n",
        "            if len(ctx) > 0:\n",
        "                p_margin = predict_next_margin(group, ctx, y, max_len=max_len)\n",
        "            else:\n",
        "                p_margin = p_trans.copy()  # fallback\n",
        "\n",
        "            # ensemble\n",
        "            p_mix = alpha * p_trans + (1 - alpha) * p_margin\n",
        "            p_mix = np.clip(p_mix, 1e-12, 1.0)\n",
        "            p_mix = p_mix / p_mix.sum()\n",
        "\n",
        "            p_hat[(g, y)] = p_mix\n",
        "\n",
        "            # observed for target (if available)\n",
        "            p_obs = p_cs.get((g, y), None)\n",
        "\n",
        "            # metrics (if observed exists)\n",
        "            def jsd(p, q, eps=1e-9):\n",
        "                p = np.clip(p, eps, 1); q = np.clip(q, eps, 1)\n",
        "                p /= p.sum(); q /= q.sum(); m = 0.5*(p+q)\n",
        "                return 0.5*np.sum(p*np.log(p/m)) + 0.5*np.sum(q*np.log(q/m))\n",
        "            def rmse(p, q): return float(np.sqrt(np.mean((p-q)**2)))\n",
        "\n",
        "            rec = {\n",
        "                \"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3],\n",
        "                \"year_prev\": y_prev, \"year\": y,\n",
        "                **{f\"p_prev_{c}\": float(p_prev[i]) for i, c in enumerate(CANON4)},\n",
        "                **{f\"p_trans_{c}\": float(p_trans[i]) for i, c in enumerate(CANON4)},\n",
        "                **{f\"p_margin_{c}\": float(p_margin[i]) for i, c in enumerate(CANON4)},\n",
        "                **{f\"p_hat_{c}\": float(p_mix[i]) for i, c in enumerate(CANON4)},\n",
        "            }\n",
        "            if p_obs is not None:\n",
        "                rec.update({f\"p_obs_{c}\": float(p_obs[i]) for i, c in enumerate(CANON4)})\n",
        "                rec[\"JSD_hat_vs_obs\"] = float(jsd(p_mix, p_obs))\n",
        "                rec[\"RMSE_hat_vs_obs\"] = rmse(p_mix, p_obs)\n",
        "                rec[\"JSD_trans_vs_obs\"] = float(jsd(p_trans, p_obs))\n",
        "                rec[\"JSD_margin_vs_obs\"] = float(jsd(p_margin, p_obs))\n",
        "            rows.append(rec)\n",
        "\n",
        "    df_out = pd.DataFrame(rows)\n",
        "    if save_csv_path:\n",
        "        df_out.to_csv(save_csv_path, index=False)\n",
        "    return p_hat, df_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "kTSXq9pBOYzI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTSXq9pBOYzI",
        "outputId": "a5ede6db-49d1-46ab-d4d5-db83d06103fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-92730498.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-92730498.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        }
      ],
      "source": [
        "# Example run (save a long tidy CSV you can reuse for plots)\n",
        "p_hat, df_prev = forecast_every_year_from_previous(\n",
        "    p_cs,\n",
        "    alpha=0.5,\n",
        "    save_csv_path=\"/content/drive/MyDrive/LLM_POC_Study_2025_v2/forecasts_from_previous_year.csv\"\n",
        ")\n",
        "\n",
        "# Now your plotting code can read p_hat directly:\n",
        "GEN, GENDER, RACE, EDU_LEVEL = \"Millennial\", \"Female\", \"White\", \"Associate or Bachelor's Degree\"\n",
        "years = sorted({y for ((g,y),_) in p_hat.items() if g==(GEN,GENDER,RACE,EDU_LEVEL)})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def backtest_interval(year_t, year_t1, alpha=0.5):\n",
        "    results = []\n",
        "    groups = sorted({g for (g,y) in p_cs.keys() if y==year_t})\n",
        "    for g in groups:\n",
        "        if (g,year_t) not in p_cs or (g,year_t1) not in p_cs:\n",
        "            continue\n",
        "        group = {\"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3]}\n",
        "        p_t = p_cs[(g,year_t)]\n",
        "        # context: use up to two previous margins if present\n",
        "        ctx_years = [year_t-4, year_t-2, year_t]\n",
        "        ctx = [(yy, p_cs[(g,yy)]) for yy in ctx_years if (g,yy) in p_cs]\n",
        "        T = predict_full_transition(group, year_t, year_t1)\n",
        "        p_next_trans = p_t @ T\n",
        "        p_next_margin = predict_next_margin(group, ctx, year_t1) if len(ctx)>0 else p_next_trans\n",
        "        p_hat = alpha*p_next_trans + (1-alpha)*p_next_margin\n",
        "        p_obs = p_cs[(g,year_t1)]\n",
        "        def jsd(p,q,eps=1e-9):\n",
        "            p = np.clip(p,eps,1); q = np.clip(q,eps,1)\n",
        "            p/=p.sum(); q/=q.sum(); m=0.5*(p+q)\n",
        "            return 0.5*np.sum(p*np.log(p/m)) + 0.5*np.sum(q*np.log(q/m))\n",
        "        def rmse(p,q): return float(np.sqrt(np.mean((p-q)**2)))\n",
        "        results.append({\n",
        "            \"year_t\": year_t, \"year_t1\": year_t1,\n",
        "            \"generation\": g[0], \"gender\": g[1], \"race\": g[2], \"edu_level\": g[3],\n",
        "            \"JSD_hat\": float(jsd(p_hat, p_obs)),\n",
        "            \"JSD_trans\": float(jsd(p_next_trans, p_obs)),\n",
        "            \"JSD_margin\": float(jsd(p_next_margin, p_obs)),\n",
        "            \"RMSE_hat\": float(rmse(p_hat, p_obs)),\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "bt_all = []\n",
        "for y0 in range(2008, 2024, 2):  # 2008→2010 … 2022→2024\n",
        "    bt_all.append(backtest_interval(y0, y0+2, alpha=0.5))\n",
        "bt = pd.concat(bt_all, ignore_index=True)\n",
        "bt_out = os.path.join(SAVE_DIR, \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/backtest_by_interval.csv\")\n",
        "bt.to_csv(bt_out, index=False)\n",
        "bt_out, bt.groupby([\"year_t\",\"year_t1\"]).JSD_hat.mean()"
      ],
      "metadata": {
        "id": "GhrYZWVk__C0",
        "outputId": "f15d2665-028a-4acb-eae0-0ab5545145b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GhrYZWVk__C0",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-92730498.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-92730498.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-92730498.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
            "/tmp/ipython-input-92730498.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/LLM_POC_Study_2025_v2/backtest_by_interval.csv',\n",
              " year_t  year_t1\n",
              " 2008    2010       0.119875\n",
              " 2010    2012       0.109060\n",
              " 2012    2014       0.110817\n",
              " 2014    2016       0.087119\n",
              " 2016    2018       0.117687\n",
              " 2022    2024       0.092643\n",
              " Name: JSD_hat, dtype: float64)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vSRFe2SlOxi3",
      "metadata": {
        "id": "vSRFe2SlOxi3"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GHUjARggOxbJ",
      "metadata": {
        "id": "GHUjARggOxbJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DBdds8wzOw3_",
      "metadata": {
        "id": "DBdds8wzOw3_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B9GdTOYoOwPP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9GdTOYoOwPP",
        "outputId": "76c69917-337c-4999-aa33-0a889fdd7a62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0.])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "cats = [\"strong_anti\",\"anti\",\"pro\",\"strong_pro\"]\n",
        "counts = {c:0.0 for c in cats}\n",
        "np.array([counts[c] for c in cats], dtype=float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m7xQGcUmt0gl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7xQGcUmt0gl",
        "outputId": "034595ed-f466-4cac-9271-66af083bec65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "defaultdict(<function <lambda> at 0x7c66b5fc32e0>, {})\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "k = defaultdict(lambda: np.zeros((K,K), dtype=float))\n",
        "print(k)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8b268baf348541a5b7bf786e4cdcfd5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0efa59068c57467ab92901687f89975c",
              "IPY_MODEL_6b58c83f36834f2a898f563a27db3a9b",
              "IPY_MODEL_e8dfef70209c456bbc4b5f2455903dcb"
            ],
            "layout": "IPY_MODEL_3ba2f628afa145328f7616360674085d"
          }
        },
        "0efa59068c57467ab92901687f89975c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41e9a22633d848eb8c6aabfa33b9a816",
            "placeholder": "​",
            "style": "IPY_MODEL_4047b574f31b4df69b409e5f3fa0ab86",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6b58c83f36834f2a898f563a27db3a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f537ea94db6b4aeb842f04919caba061",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec098c851b3943eda8ae232c54817cf5",
            "value": 4
          }
        },
        "e8dfef70209c456bbc4b5f2455903dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8b74d895cf3432daaf9c92f5cde6391",
            "placeholder": "​",
            "style": "IPY_MODEL_f2d9b706d1364fbdb757a90241757cb3",
            "value": " 4/4 [00:05&lt;00:00,  1.13s/it]"
          }
        },
        "3ba2f628afa145328f7616360674085d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41e9a22633d848eb8c6aabfa33b9a816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4047b574f31b4df69b409e5f3fa0ab86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f537ea94db6b4aeb842f04919caba061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec098c851b3943eda8ae232c54817cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8b74d895cf3432daaf9c92f5cde6391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d9b706d1364fbdb757a90241757cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}