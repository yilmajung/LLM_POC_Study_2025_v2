{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5230e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, numpy as np, pandas as pd, torch, torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import math\n",
    "\n",
    "# Set up paths\n",
    "BASE_MODEL_NAME = \"meta-llama/llama-3.1-8b\"    # or mistral\n",
    "SAVE_DIR   = \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/outputs_gss_multitask/final_multitask\"  # from training step\n",
    "HEAD_PATH  = os.path.join(SAVE_DIR, \"two_head.pt\")\n",
    "\n",
    "# Canon (K=4)\n",
    "CANON4 = [\"strong_anti\",\"anti\",\"pro\",\"strong_pro\"]\n",
    "K = len(CANON4)\n",
    "\n",
    "# Load tokenizer + base + LoRA\n",
    "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, SAVE_DIR)\n",
    "model.eval()\n",
    "\n",
    "# Two-head module (must match training)\n",
    "import torch.nn as nn\n",
    "class TwoHead(nn.Module):\n",
    "    def __init__(self, hidden_size, K):\n",
    "        super().__init__()\n",
    "        self.head_row    = nn.Linear(hidden_size, K)   # Task A\n",
    "        self.head_margin = nn.Linear(hidden_size, K)   # Task B\n",
    "    def forward(self, feats):\n",
    "        return self.head_row(feats), self.head_margin(feats)\n",
    "\n",
    "hidden_size = base.config.hidden_size\n",
    "two_head = TwoHead(hidden_size, K).to(model.device)\n",
    "two_head.load_state_dict(torch.load(HEAD_PATH, map_location=model.device))\n",
    "two_head.eval()\n",
    "\n",
    "def pooled_features(outputs, attention_mask, tail=96):\n",
    "    hs = outputs.hidden_states[-1]   # [B,T,H]\n",
    "    valid = attention_mask.sum(dim=1)\n",
    "    feats = []\n",
    "    for b in range(hs.size(0)):\n",
    "        L = int(valid[b].item())\n",
    "        s = max(0, L - tail); e = L\n",
    "        if e <= s: s, e = max(0, L-32), L\n",
    "        feats.append(hs[b, s:e, :].mean(dim=0))\n",
    "    return torch.stack(feats, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1671d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: predict a row/margin/full matrix\n",
    "@torch.no_grad()\n",
    "def predict_row_distribution(group, year_t, year_t1, from_bin, dt=None, max_len=768):\n",
    "    \"\"\"Task A: one row (from_bin) of the transition matrix.\"\"\"\n",
    "    if dt is None: dt = int(year_t1) - int(year_t)\n",
    "    prompt = (\n",
    "        \"[Task: Predict transition row]\\n\"\n",
    "        f\"From: <Y{year_t}> → To: <Y{year_t1}> <DT{dt}>\\n\"\n",
    "        f\"Group: generation={group['generation']}; gender={group['gender']}; race={group['race']}\\n\"\n",
    "        f\"From option: {from_bin}\\n\"\n",
    "        \"Answer:\\n\"\n",
    "    )\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    ids = enc[\"input_ids\"].to(model.device); attn = enc[\"attention_mask\"].to(model.device)\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        out = model(input_ids=ids, attention_mask=attn, output_hidden_states=True)\n",
    "    feats = pooled_features(out, attn, tail=96).to(model.device)\n",
    "    logits_row, _ = two_head(feats)\n",
    "    p = F.softmax(logits_row, dim=1).float().cpu().numpy()[0]\n",
    "    return p  # shape [K]\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_full_transition(group, year_t, year_t1, max_len=768):\n",
    "    \"\"\"Assemble 4×4 by calling row head for each from-bin.\"\"\"\n",
    "    T = np.zeros((K,K), dtype=np.float32)\n",
    "    dt = int(year_t1) - int(year_t)\n",
    "    for i, from_bin in enumerate(CANON4):\n",
    "        T[i,:] = predict_row_distribution(group, year_t, year_t1, from_bin, dt=dt, max_len=max_len)\n",
    "    # force row-stochastic\n",
    "    T = np.clip(T, 1e-12, 1)\n",
    "    T = T / T.sum(axis=1, keepdims=True)\n",
    "    return T\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_next_margin(group, context, target_year, max_len=768):\n",
    "    \"\"\"\n",
    "    Task B: context = list of (year, prob_vector length K) tuples.\n",
    "    \"\"\"\n",
    "    ctx_parts = \" \".join([f\"<Y{yy}>[{','.join(f'{x:.4f}' for x in p)}]\" for (yy,p) in context])\n",
    "    prompt = (\n",
    "        \"[Task: Forecast next-wave margin]\\n\"\n",
    "        f\"Group: generation={group['generation']}; gender={group['gender']}; race={group['race']}\\n\"\n",
    "        f\"Context: {ctx_parts}\\n\"\n",
    "        f\"Predict: <Y{target_year}>\\n\"\n",
    "        \"Answer:\\n\"\n",
    "    )\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    ids = enc[\"input_ids\"].to(model.device); attn = enc[\"attention_mask\"].to(model.device)\n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        out = model(input_ids=ids, attention_mask=attn, output_hidden_states=True)\n",
    "    feats = pooled_features(out, attn, tail=96).to(model.device)\n",
    "    _, logits_margin = two_head(feats)\n",
    "    p = F.softmax(logits_margin, dim=1).float().cpu().numpy()[0]\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe667f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using real margins and forecasting\n",
    "# Load cross-sectional CSV and build p_cs[(g,y)]\n",
    "cs = pd.read_csv(\"/content/drive/MyDrive/LLM_POC_Study_2025_v2/gss_abt_cs_full.csv\")\n",
    "# Map to 4 bins if numeric:\n",
    "def map_att(v):\n",
    "    if isinstance(v, (int, np.integer, float)) and not pd.isna(v):\n",
    "        return {1:\"strong_anti\",2:\"anti\",3:\"pro\",4:\"strong_pro\"}.get(int(v), None)\n",
    "    return str(v).strip()\n",
    "cs[\"att\"] = cs[\"abortion_att4\"].apply(map_att)\n",
    "cs = cs[cs[\"att\"].isin(CANON4)].copy()\n",
    "cs[\"wt\"] = cs.get(\"wtssps\", pd.Series([1.0]*len(cs)))\n",
    "\n",
    "GROUP_COLS = [\"generation\",\"gender\",\"race\"]\n",
    "for c in GROUP_COLS: cs[c] = cs[c].astype(str).str.strip()\n",
    "\n",
    "def weighted_probs(vals, wts):\n",
    "    d = {c:0.0 for c in CANON4}\n",
    "    for v,w in zip(vals,wts): d[v]+=float(w)\n",
    "    vec = np.array([d[c] for c in CANON4],dtype=float)\n",
    "    s = vec.sum(); \n",
    "    return vec/s if s>0 else None\n",
    "\n",
    "p_cs = {}\n",
    "for (gvals, df_g) in cs.groupby(GROUP_COLS):\n",
    "    for y, df_y in df_g.groupby(\"year\"):\n",
    "        p = weighted_probs(df_y[\"att\"].tolist(), df_y[\"wt\"].tolist())\n",
    "        if p is not None:\n",
    "            p_cs[(gvals, int(y))] = p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast 2024 from 2022 (per subgroup)\n",
    "# choose subgroups present in 2022\n",
    "groups_2022 = sorted({g for (g,y) in p_cs.keys() if y==2022})\n",
    "\n",
    "rows = []\n",
    "alpha = 0.5  # ensemble weight between transition-based and margin-head forecast\n",
    "\n",
    "for g in groups_2022:\n",
    "    group = {\"generation\": g[0], \"gender\": g[1], \"race\": g[2]}\n",
    "    # context margins e.g., 2018, 2020, 2022 if available\n",
    "    ctx = []\n",
    "    for yy in [2018, 2020, 2022]:\n",
    "        if (g, yy) in p_cs:\n",
    "            ctx.append((yy, p_cs[(g,yy)]))\n",
    "    if len(ctx)==0 or (g,2022) not in p_cs:\n",
    "        continue\n",
    "\n",
    "    p_2022 = p_cs[(g,2022)]\n",
    "    # Task A: transition matrix 2022->2024\n",
    "    T_22_24 = predict_full_transition(group, 2022, 2024)\n",
    "    p_2024_trans = p_2022 @ T_22_24\n",
    "\n",
    "    # Task B: direct next margin\n",
    "    p_2024_margin = predict_next_margin(group, ctx, 2024)\n",
    "\n",
    "    # Ensemble (optional)\n",
    "    p_2024_hat = alpha * p_2024_trans + (1 - alpha) * p_2024_margin\n",
    "    p_2024_hat = p_2024_hat / p_2024_hat.sum()\n",
    "\n",
    "    # Observed 2024 for backtest if available\n",
    "    p_2024_obs = p_cs.get((g,2024), None)\n",
    "\n",
    "    rec = {\n",
    "        \"generation\": g[0], \"gender\": g[1], \"race\": g[2],\n",
    "        **{f\"p2022_{c}\": float(p_2022[i]) for i,c in enumerate(CANON4)},\n",
    "        **{f\"p2024_trans_{c}\": float(p_2024_trans[i]) for i,c in enumerate(CANON4)},\n",
    "        **{f\"p2024_margin_{c}\": float(p_2024_margin[i]) for i,c in enumerate(CANON4)},\n",
    "        **{f\"p2024_hat_{c}\": float(p_2024_hat[i]) for i,c in enumerate(CANON4)},\n",
    "    }\n",
    "    if p_2024_obs is not None:\n",
    "        rec.update({f\"p2024_obs_{c}\": float(p_2024_obs[i]) for i,c in enumerate(CANON4)})\n",
    "\n",
    "        # quick metrics\n",
    "        def jsd(p,q,eps=1e-9):\n",
    "            p = np.clip(p,eps,1); q = np.clip(q,eps,1)\n",
    "            p/=p.sum(); q/=q.sum(); m=0.5*(p+q)\n",
    "            return 0.5*np.sum(p*np.log(p/m)) + 0.5*np.sum(q*np.log(q/m))\n",
    "        def rmse(p,q): return float(np.sqrt(np.mean((p-q)**2)))\n",
    "        rec[\"JSD_hat_vs_obs\"] = float(jsd(p_2024_hat, p_2024_obs))\n",
    "        rec[\"RMSE_hat_vs_obs\"] = rmse(p_2024_hat, p_2024_obs)\n",
    "        rec[\"JSD_trans_vs_obs\"] = float(jsd(p_2024_trans, p_2024_obs))\n",
    "        rec[\"JSD_margin_vs_obs\"]= float(jsd(p_2024_margin, p_2024_obs))\n",
    "\n",
    "    rows.append(rec)\n",
    "\n",
    "df_forecast = pd.DataFrame(rows)\n",
    "out_csv = os.path.join(SAVE_DIR, \"/content/drive/MyDrive/LLM_POC_Study_2025_v2/forecast_2024_from_2022_by_group.csv\")\n",
    "df_forecast.to_csv(out_csv, index=False)\n",
    "out_csv\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
